[{"text": "Data analysis is the process of inspecting, cleansing, transforming, and modeling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "The data is necessary as inputs to the analysis, which is specified based upon the requirements of those directing the analytics (or customers, who will use the finished product of the analysis). The general type of entity upon which the data will be collected is referred to as an experimental unit (e.g., a person or population of people). Specific variables regarding a population (e.g., age and income) may be specified and obtained.  Data may be numerical or categorical (i.e., a text label for numbers).\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Mathematical formulas or models (also known as algorithms), may be applied to the data in order to identify relationships among the variables; for example, using correlation or causation. In general terms, models may be developed to evaluate a specific variable based on other variable(s) contained within the dataset, with some residual error depending on the implemented model's accuracy (e.g., Data = Model + Error).\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "A data product is a computer application that takes data inputs and generates outputs, feeding them back into the environment. It may be based on a model or algorithm. For instance, an application that analyzes data about customer purchase history, and uses the results to recommend other purchases the customer might enjoy.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "When determining how to communicate the results, the analyst may consider implementing a variety of data visualization techniques to help communicate the message more clearly and efficiently to the audience. Data visualization uses information displays (graphics such as, tables and charts) to help communicate key messages contained in the data. Tables are a valuable tool by enabling the ability of a user to query and focus on specific numbers; while charts (e.g., bar charts or line charts), may help explain the quantitative messages contained in the data.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Stephen Few described eight types of quantitative messages that users may attempt to understand or communicate from a set of data and the associated graphs used to help communicate the message. Customers specifying requirements and analysts performing the data analysis may consider these messages during the course of the process.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Analysts may use robust statistical measurements to solve certain analytical problems. Hypothesis testing is used when a particular hypothesis about the true state of affairs is made by the analyst and data is gathered to determine whether that state of affairs is true or false. For example, the hypothesis might be that \"Unemployment has no effect on inflation\", which relates to an economics concept called the Phillips Curve. Hypothesis testing involves considering the likelihood of Type I and type II errors, which relate to whether the data supports accepting or rejecting the hypothesis.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Regression analysis may be used when the analyst is trying to determine the extent to which independent variable X affects dependent variable Y (e.g., \"To what extent do changes in the unemployment rate (X) affect the inflation rate (Y)?\"). This is an attempt to model or fit an equation line or curve to the data, such that Y is a function of X.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Users may have particular data points of interest within a data set, as opposed to the general messaging outlined above. Such low-level user analytic activities are presented in the following table. The taxonomy can also be organized by three poles of activities: retrieving values, finding data points, and arranging data points.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "As another example, the auditor of a public company must arrive at a formal opinion on whether financial statements of publicly traded corporations are \"fairly stated, in all material respects\". This requires extensive analysis of factual data and evidence to support their opinion. When making the leap from facts to opinions, there is always the possibility that the opinion is erroneous.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Analysts may be trained specifically to be aware of these biases and how to overcome them. In his book Psychology of Intelligence Analysis, retired CIA analyst Richards Heuer wrote that analysts should clearly delineate their assumptions and chains of inference and specify the degree and source of the uncertainty involved in the conclusions. He emphasized procedures to help surface and debate alternative points of view.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Analysts may also analyze data under different assumptions or scenario. For example, when analysts perform financial statement analysis, they will often recast the financial statements under different assumptions to help arrive at an estimate of future cash flow, which they then discount to present value based on some interest rate, to determine the valuation of the company or its stock. Similarly, the CBO analyzes the effects of various policy options on the government's revenue, outlays and deficits, creating alternative future scenarios for key measures.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "A data analytics approach can be used in order to predict energy consumption in buildings. The different steps of the data analysis process are carried out in order to realise smart buildings, where the building management and control operations including heating, ventilation, air conditioning, lighting and security are realised automatically by miming the needs of the building users and optimising resources like energy and time.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Analytics is the \"extensive use of data, statistical and quantitative analysis, explanatory and predictive models, and fact-based management to drive decisions and actions.\" It is a subset of business intelligence, which is a set of technologies and processes that uses data to understand and analyze business performance to drive decision-making .\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "In education, most educators have access to a data system for the purpose of analyzing student data. These data systems present data to educators in an over-the-counter data format (embedding labels, supplemental documentation, and a help system and making key package/display and content decisions) to improve the accuracy of educators' data analyses.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "One should check the success of the randomization procedure, for instance by checking whether background and substantive variables are equally distributed within and across groups. If the study did not need or use a randomization procedure, one should check the success of the non-random sampling, for instance by checking whether all subgroups of the population of interest are represented in sample.Other possible data distortions that should be checked are:\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "In any report or article, the structure of the sample must be accurately described. It is especially important to exactly determine the structure of the sample (and specifically the size of the subgroups) when subgroup analyses will be performed during the main analysis phase.The characteristics of the data sample can be assessed by looking at:\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "During the final stage, the findings of the initial data analysis are documented, and necessary, preferable, and possible corrective actions are taken.Also, the original plan for the main data analyses can and should be specified in more detail or rewritten. In order to do this, several decisions about the main data analyses can and should be made:\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "In the main analysis phase, either an exploratory or confirmatory approach can be adopted. Usually the approach is decided before data is collected. In an exploratory analysis no clear hypothesis is stated before analysing the data, and the data is searched for models that describe the data well. In a confirmatory analysis clear hypotheses about the data are tested.\n", "link": "https://en.wikipedia.org/wiki/Data_analysis"}, {"text": "Statistics (from German: Statistik, orig. \"description of a state, a country\") is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as \"all people living in a country\" or \"every atom composing a crystal\". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Statistical measurement processes are also prone to error in regards to the data that they generate. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Statistics is the discipline that deals with data, facts and figures with which meaningful information is inferred. Data may represent a numerical value, in form of quantitative data, or a label, as with qualitative data. Data may be collected, presented and summarised, in one of two methods called descriptive statistics. Two elementary summaries of data, singularly called a statistic, are the mean and dispersion. Whereas inferential statistics interprets data from a population sample to induce statements and predictions about a population.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Some consider statistics to be a distinct mathematical science rather than a branch of mathematics. While many scientific investigations make use of data, statistics is generally concerned with the use of data in the context of uncertainty and decision-making in the face of uncertainty. Statistics is indexed at 62, a subclass of probability theory and stochastic processes, in the Mathematics Subject Classification. Mathematical statistics is covered in the range 276-280 of subclass QA (science > mathematics) in the Library of Congress Classification.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "To use a sample as a guide to an entire population, it is important that it truly represents the overall population. Representative sampling assures that inferences and conclusions can safely extend from the sample to the population as a whole. A major problem lies in determining the extent that the sample chosen is actually representative. Statistics offers methods to estimate and correct for any bias within the sample and data collection procedures. There are also methods of experimental design that can lessen these issues at the outset of a study, strengthening its capability to discern truths about the population.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. \"The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer.\"\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features of a collection of information, while descriptive statistics in the mass noun sense is the process of using and analyzing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics), in that descriptive statistics aims to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Statistical inference is the process of using data analysis to deduce properties of an underlying probability distribution. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population. Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Consider independent identically distributed (IID) random variables with a given probability distribution: standard statistical inference and estimation theory defines a random sample as the random vector given by the column vector of these IID variables. The population being examined is described by a probability distribution that may have unknown parameters.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "A statistic is a random variable that is a function of the random sample, but not a function of unknown parameters. The probability distribution of the statistic, though, may have unknown parameters. Consider now a function of the unknown parameter: an estimator is a statistic used to estimate such function. Commonly used estimators include sample mean, unbiased sample variance and sample covariance.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Between two estimators of a given parameter, the one with lower mean squared error is said to be more efficient. Furthermore, an estimator is said to be unbiased if its expected value is equal to the true value of the unknown parameter being estimated, and asymptotically unbiased if its expected value converges at the limit to the true value of such parameter.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Measurement processes that generate statistical data are also subject to error.  Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also be important. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "In principle confidence intervals can be symmetrical or asymmetrical. An interval can be asymmetrical because it works as lower or upper bound for a parameter (left-sided interval or right sided interval), but it can also be asymmetrical because the two sided interval is built violating symmetry around the estimate. Sometimes the bounds for a confidence interval are reached asymptotically and these are used to approximate the true bounds.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Referring to statistical significance does not necessarily mean that the overall result is significant in real world terms. For example, in a large study of a drug it may be shown that the drug has a statistically significant but very small beneficial effect, such that the drug is unlikely to help the patient noticeably.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Although in principle the acceptable level of statistical significance may be subject to debate, the significance level is the largest p-value that allows the test to reject the null hypothesis. This test is logically equivalent to saying that the p-value is the probability, assuming the null hypothesis is true, of observing a result at least as extreme as the test statistic. Therefore, the smaller the significance level, the lower the probability of committing type I error.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Mathematical statistics is the application of mathematics to statistics. Mathematical techniques used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure-theoretic probability theory. All statistical analyses make use of at least some mathematics, and mathematical statistics can therefore be regarded as a fundamental component of general statistics.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Formal discussions on inference date back to the mathematicians and cryptographers of the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717\u2013786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels. Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187\u20131268) later made an important contribution on the use of sample size in frequency analysis.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Applied statistics, sometimes referred to as Statistical science, comprises descriptive statistics and the application of inferential statistics. Theoretical statistics concerns the logical arguments underlying justification of approaches to statistical inference, as well as encompassing mathematical statistics. Mathematical statistics includes not only the manipulation of probability distributions necessary for deriving results related to methods of estimation and inference, but also various aspects of computational statistics and the design of experiments.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Statistics is applicable to a wide variety of academic disciplines, including natural and social sciences, government, and business. Business statistics applies statistical methods in econometrics, auditing and production and operations, including services improvement and marketing research. A study of two journals in tropical biology found that the 12 most frequent statistical tests are: analysis of variance (ANOVA), chi-squared test, Student's t-test, linear regression, Pearson's correlation coefficient, Mann-Whitney U test, Kruskal-Wallis test, Shannon's diversity index, Tukey's range test, cluster analysis, Spearman's rank correlation coefficient and principal component analysis.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "The rapid and sustained increases in computing power starting from the second half of the 20th century have had a substantial impact on the practice of statistical science. Early statistical models were almost always from the class of linear models, but powerful computers, coupled with suitable numerical algorithms, caused an increased interest in nonlinear models (such as neural networks) as well as the creation of new types, such as generalized linear models and multilevel models.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Increased computing power has also led to the growing popularity of computationally intensive methods based on resampling, such as permutation tests and the bootstrap, while techniques such as Gibbs sampling have made use of Bayesian models more feasible. The computer revolution has implications for the future of statistics with a new emphasis on \"experimental\" and \"empirical\" statistics. A large number of both general and special purpose statistical software are now available. Examples of available software capable of complex statistical computation include programs such as Mathematica, SAS, SPSS, and R.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "In business, \"statistics\" is a widely used management- and decision support tool. It is particularly applied in financial management, marketing management, and production, services and operations management. Statistics is also heavily used in management accounting and auditing. The discipline of Management Science formalizes the use of statistics, and other mathematics, in business. (Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships.)\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "A typical \"Business Statistics\" course is intended for business majors, and covers descriptive statistics (collection, description, analysis, and summary of data), probability (typically the binomial and normal distributions), test of hypotheses and confidence intervals, linear regression, and correlation; (follow-on) courses may include forecasting, time series, decision trees, multiple linear regression, and other topics from business analytics more generally. Professional certification programs, such as the CFA, often include topics in statistics.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Traditionally, statistics was concerned with drawing inferences using a semi-standardized methodology that was \"required learning\" in most sciences. This tradition has changed with the use of statistics in non-inferential contexts. What was once considered a dry subject, taken in many fields as a degree-requirement, is now viewed enthusiastically. Initially derided by some mathematical purists, it is now considered essential methodology in certain areas.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Misuse of statistics can produce subtle but serious errors in description and interpretation\u2014subtle in the sense that even experienced professionals make such errors, and serious in the sense that they can lead to devastating decision errors. For instance, social policy, medical practice, and the reliability of structures like bridges all rely on the proper use of statistics.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Even when statistical techniques are correctly applied, the results can be difficult to interpret for those lacking expertise. The statistical significance of a trend in the data\u2014which measures the extent to which a trend could be caused by random variation in the sample\u2014may or may not agree with an intuitive sense of its significance. The set of basic statistical skills (and skepticism) that people need to deal with information in their everyday lives properly is referred to as statistical literacy.\n", "link": "https://en.wikipedia.org/wiki/Statistics"}, {"text": "Data visualization is concerned with visually presenting sets of primarily quantitative raw data in a schematic form. The visual formats used in data visualization include tables, charts and graphs (e.g. pie charts, bar charts, line charts, area charts, cone charts, pyramid charts, donut charts, histograms, spectrograms, cohort charts, waterfall charts, funnel charts, bullet graphs, etc.), diagrams, plots (e.g. scatter plots, distribution plots, box-and-whisker plots), geospatial maps (such as proportional symbol maps, choropleth maps, isopleth maps and heat maps), figures, correlation matrices, percentage gauges, etc., which sometimes can be combined in a dashboard. \n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Information visualization, on the other hand, deals with multiple, large-scale and complicated datasets which contain quantitative (numerical) data as well as qualitative (non-numerical, i.e. verbal or graphical) and primarily abstract information and its goal is to add value to raw data, improve the viewers' comprehension, reinforce their cognition and help them derive insights and make decisions as they navigate and interact with the computer-supported graphical display. Visual tools used in information visualization include maps (such as tree maps), animations, infographics, Sankey diagrams, flow charts, network diagrams, semantic networks, entity-relationship diagrams, venn diagrams, timelines, mind maps, etc.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Emerging technologies like virtual, augmented and mixed reality have the potential to make information visualization more immersive, intuitive, interactive and easily manipulable and thus enhance the user's visual perception and cognition. In data and information visualization, the goal is to graphically present and explore abstract, non-physical and non-spatial data collected from databases, information systems, file systems, documents, business data, etc. (presentational and exploratory visualization) which is different from the field of scientific visualization, where the goal is to render realistic images based on physical and spatial scientific data to confirm or reject hypotheses (confirmatory visualization).\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Research into how people read and misread various types of visualizations is helping to determine what types and features of visualizations are most understandable and effective in conveying information. On the other hand, unintentionally poor or intentionally misleading and deceptive visualizations (misinformative visualization) can function as powerful tools which disseminate misinformation, manipulate public perception and divert public opinion toward a certain agenda. Thus data visualization literacy has become an important component of data and information literacy in the information age akin to the roles played by textual, mathematical and visual literacy in the past.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Data and information visualization presumes that \"visual representations and interaction techniques take advantage of the human eye's broad bandwidth pathway into the mind to allow users to see, explore, and understand large amounts of information at once. Information visualization focused on the creation of approaches for conveying abstract information in intuitive ways.\"\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Edward Tufte has explained that users of information displays are executing particular analytical tasks  such as making comparisons. The design principle of the information graphic should support the analytical task. As William Cleveland and Robert McGill show, different graphical elements accomplish this more or less effectively. For example, dot plots and bar charts outperform pie charts.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Not applying these principles may result in misleading graphs, distorting the message, or supporting an erroneous conclusion. According to Tufte, chartjunk refers to the extraneous interior decoration of the graphic that does not enhance the message or gratuitous three-dimensional or perspective effects. Needlessly separating the explanatory key from the image itself, requiring the eye to travel back and forth from the image to the key, is a form of \"administrative debris.\" The ratio of \"data to ink\" should be maximized, erasing non-data ink where feasible.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "A human can distinguish differences in line length, shape, orientation, distances, and color (hue) readily without significant processing effort; these are referred to as \"pre-attentive attributes\".  For example, it may require significant time and effort (\"attentive processing\") to identify the number of times the digit \"5\" appears in a series of numbers; but if that digit is different in size, orientation, or color, instances of the digit can be noted quickly through pre-attentive processing.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Compelling graphics take advantage of pre-attentive processing and attributes and the relative strength of these attributes. For example, since humans can more easily process differences in line length than surface area, it may be more effective to use a bar chart (which takes advantage of line length to show comparison) rather than pie charts (which use surface area to show comparison).\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "By the 16th century, techniques and instruments for precise observation and measurement of physical quantities, and geographic and celestial position were well-developed (for example, a \"wall quadrant\" constructed by Tycho Brahe [1546\u20131601], covering an entire wall in his observatory). Particularly important were the development of triangulation and other methods to determine mapping locations accurately. Very early, the measure of time led scholars to develop innovative way of visualizing the data (e.g. Lorenz Codomann in 1596, Johannes Temporarius in 1596).\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "\nFrench philosopher and mathematician Ren\u00e9 Descartes and Pierre de Fermat developed analytic geometry and two-dimensional coordinate system which heavily influenced the practical methods of displaying and calculating values. Fermat and Blaise Pascal's work on statistics and probability theory laid the groundwork for what we now conceptualize as data. According to the Interaction Design Foundation, these developments allowed and helped William Playfair, who saw potential for graphical communication of quantitative data, to generate and develop graphical methods of statistics. ", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "John Tukey and Edward Tufte pushed the bounds of data visualization; Tukey with his new statistical approach of exploratory data analysis and Tufte with his book \"The Visual Display of Quantitative Information\" paved the way for refining data visualization techniques for more than statisticians. With the progression of technology came the progression of data visualization; starting with hand-drawn visualizations and evolving into more technical applications \u2013 including interactive designs leading to software visualization.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Programs like SAS, SOFA, R, Minitab, Cornerstone and more allow for data visualization in the field of statistics. Other data visualization applications, more focused and unique to individuals, programming languages such as D3, Python and JavaScript help to make the visualization of quantitative data a possibility.  Private schools have also developed programs to meet the demand for learning data visualization and associated programming libraries, including free programs like The Data Incubator or paid programs like General Assembly.\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Eppler and Lengler have developed the \"Periodic Table of Visualization Methods,\" an interactive chart displaying various data visualization methods. It includes six types of data visualization methods: data, information, concept, strategy, metaphor and compound. In \"Visualization Analysis and Design\" Tamara Munzner writes \"Computer-based visualization systems provide visual representations of datasets designed to help people carry out tasks more effectively.\" Munzner agues that visualization \"is suitable when there is a need to augment human capabilities rather than replace people with computational decision-making methods.\"\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "There are different approaches on the scope of data visualization. One common focus is on information presentation, such as Friedman (2008). Friendly (2008) presumes two main parts of data visualization: statistical graphics, and thematic cartography. In this line the \"Data Visualization: Modern Approaches\" (2007) article gives an overview of seven subjects of data visualization:\n", "link": "https://en.wikipedia.org/wiki/Data_and_information_visualization"}, {"text": "Tukey defined data analysis in 1961 as: \"Procedures for analyzing data, techniques for interpreting the results of such procedures, ways of planning the gathering of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data.\"\n", "link": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"}, {"text": "Tukey's championing of EDA encouraged the development of statistical computing packages, especially S at Bell Labs. The S programming language inspired the systems S-PLUS and R. This family of statistical-computing environments featured vastly improved dynamic visualization capabilities, which allowed statisticians to identify outliers, trends and patterns in data that merited further study.\n", "link": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"}, {"text": "Exploratory data analysis, robust statistics, nonparametric statistics, and the development of statistical programming languages facilitated statisticians' work on scientific and engineering problems. Such problems included the fabrication of semiconductors and the understanding of communications networks, both of which were of interest to Bell Labs. These statistical developments, all championed by Tukey, were designed to complement the analytic theory of testing statistical hypotheses, particularly the Laplacian tradition's emphasis on exponential families.\n", "link": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"}, {"text": "John W. Tukey wrote the book Exploratory Data Analysis in 1977. Tukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test. In particular, he held that confusing the two types of analyses and employing them on the same set of data can lead to systematic bias owing to the issues inherent in testing hypotheses suggested by the data.\n", "link": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"}, {"text": "Findings from EDA are orthogonal to the primary analysis task. To illustrate, consider an example from Cook et al. where the analysis task is to find the variables which best predict the tip that a dining party will give to the waiter. The variables available in the data collected for this task are: the tip amount, total bill, payer gender, smoking/non-smoking section, time of day, day of the week, and size of the party. The primary analysis task is approached by fitting a regression model where the tip rate is the response variable. The fitted model is\n", "link": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"}, {"text": "What is learned from the plots is different from what is illustrated by the regression model, even though the experiment was not designed to investigate any of these other trends. The patterns found by exploring the data suggest hypotheses about tipping that may not have been anticipated in advance, and which could lead to interesting follow-up experiments where the hypotheses are formally stated and tested by collecting new data.\n", "link": "https://en.wikipedia.org/wiki/Exploratory_data_analysis"}, {"text": "There are many similarities between information design and information architecture. The title of information designer is sometimes used by graphic designers who specialize in creating websites. The skillset of the information designer, as the title is applied more globally, is closer to that of the information architect in the U.S. Similar skills for organization and structure are brought to bear in designing web sites and digital media, with additional constraints and functions that earn a designer the title information architect.\n", "link": "https://en.wikipedia.org/wiki/Information_design"}, {"text": "The Minard diagram shows the losses suffered by Napoleon's army in the 1812\u20131813 period. Six variables are plotted: the size of the army, its location on a two-dimensional surface (x and y), time, direction of movement, and temperature. This multivariate display on a two-dimensional surface tells a story that can be grasped immediately while identifying the source data to build credibility. Edward Tufte wrote in 1983 that: \"It may well be the best statistical graphic ever drawn.\"\n", "link": "https://en.wikipedia.org/wiki/Information_design"}, {"text": "Information design can be used for broad audiences (such as signs in airports) or specific audiences (such as personalized telephone bills).  The resulting work often seeks to improve a user's trust of a product (such as medicine packaging inserts, operational instructions for industrial machinery and information for emergencies). The example of signs also highlights a niche category known as wayfinding. \n", "link": "https://en.wikipedia.org/wiki/Information_design"}, {"text": "Governments and regulatory authorities have legislated about a number of information design issues, such as the minimum size of type in financial small print, the labelling of ingredients in processed food, and the testing of medicine labelling. Examples of this are the Truth in Lending Act in the USA, which introduced the Schumer box (a concise summary of charges for people applying for a credit card), and the Guideline on the Readability of the Labelling and Package Leaflet of Medicinal Products for Human Use (European Commission, Revision 1, 12 January 2009).\n", "link": "https://en.wikipedia.org/wiki/Information_design"}, {"text": "Simplicity is a major concern in information design. The aim is clarity and understanding. Simplification of messages may imply quantitative reduction but is not restricted to that. Sometimes more information means more clarity. Also, simplicity is a highly subjective matter and should always be evaluated with the information user in mind. Simplicity can be easy when following five simple steps when it comes to information design: \n", "link": "https://en.wikipedia.org/wiki/Information_design"}, {"text": "Some measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness.\n", "link": "https://en.wikipedia.org/wiki/Descriptive_statistics"}, {"text": "Descriptive statistics provide simple summaries about the sample and about the observations that have been made. Such summaries may be either quantitative, i.e. summary statistics, or visual, i.e. simple-to-understand graphs. These summaries may either form the basis of the initial description of the data as part of a more extensive statistical analysis, or they may be sufficient in and of themselves for a particular investigation.\n", "link": "https://en.wikipedia.org/wiki/Descriptive_statistics"}, {"text": "For example, the shooting percentage in basketball is a descriptive statistic that summarizes the performance of a player or a team. This number is the number of shots made divided by the number of shots taken. For example, a player who shoots 33% is making approximately one shot in every three. The percentage summarizes or describes multiple discrete events. Consider also the grade point average. This single number describes the general performance of a student across the range of their course experiences.\n", "link": "https://en.wikipedia.org/wiki/Descriptive_statistics"}, {"text": "The use of descriptive and summary statistics has an extensive history and, indeed, the simple tabulation of populations and of economic data was the first way the topic of statistics appeared. More recently, a collection of summarisation techniques has been formulated under the heading of exploratory data analysis: an example of such a technique is the box plot.\n", "link": "https://en.wikipedia.org/wiki/Descriptive_statistics"}, {"text": "Univariate analysis involves describing the distribution of a single variable, including its central tendency (including the mean, median, and mode) and dispersion (including the range and quartiles of the data-set, and measures of spread such as the variance and standard deviation). The shape of the distribution may also be described via indices such as skewness and kurtosis. Characteristics of a variable's distribution may also be depicted in graphical or tabular format, including histograms and stem-and-leaf display.\n", "link": "https://en.wikipedia.org/wiki/Descriptive_statistics"}, {"text": "Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean \"make a prediction, by evaluating an already trained model\"; in this context inferring properties of the model is referred to as training or learning (rather than inference), and using a model for prediction is referred to as inference (instead of prediction); see also predictive inference.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Statistical inference makes propositions about a population, using data drawn from the population with some form of sampling. Given a hypothesis about a population, for which we wish to draw inferences, statistical inference consists of (first) selecting a statistical model of the process that generates the data and (second) deducing propositions from the model.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Any statistical inference requires some assumptions. A statistical model is a set of assumptions concerning the generation of the observed data and similar data. Descriptions of statistical models usually emphasize the role of population quantities of interest, about which we wish to draw inference. Descriptive statistics are typically used as a preliminary step before more formal inferences are drawn.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Objective randomization allows properly inductive procedures. Many statisticians prefer randomization-based analysis of data that was generated by well-defined randomization procedures. (However, it is true that in fields of science with developed theoretical knowledge and experimental control, randomized experiments may increase the costs of experimentation without improving the quality of inferences.) Similarly, results from randomized experiments are recommended by leading statistical authorities as allowing inferences with greater reliability than do observational studies of the same phenomena. However, a good observational study may be better than a bad randomized experiment.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "It is standard practice to refer to a statistical model, e.g., a linear or logistic models, when analyzing data from randomized experiments. However, the randomization scheme guides the choice of a statistical model. It is not possible to choose an appropriate model without knowing the randomization scheme. Seriously misleading results can be obtained analyzing data from randomized experiments while ignoring the experimental protocol; common mistakes include forgetting the blocking used in an experiment and confusing repeated measurements on the same experimental unit with independent replicates of the treatment applied to different experimental units.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "This paradigm calibrates the plausibility of propositions by considering (notional) repeated sampling of a population distribution to produce datasets similar to the one at hand. By considering the dataset's characteristics under repeated sampling, the frequentist properties of a statistical proposition can be quantified\u2014although in practice this quantification may be challenging.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Many informal Bayesian inferences are based on \"intuitively reasonable\" summaries of the posterior. For example, the posterior mean, median and mode, highest posterior density intervals, and Bayes Factors can all be motivated in this way. While a user's utility function need not be stated for this sort of inference, these summaries do all depend (to some extent) on stated prior beliefs, and are generally viewed as subjective conclusions. (Methods of prior construction which do not require external input have been proposed but not yet fully developed.)\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "The Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "AIC is founded on information theory: it offers an estimate of the relative information lost when a given model is used to represent the process that generated the data. (In doing so, it deals with the trade-off between the goodness of fit of the model and the simplicity of the model.)\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "The minimum description length (MDL) principle has been developed from ideas in information theory and the theory of Kolmogorov complexity. The (MDL) principle selects statistical models that maximally compress the data; inference proceeds without assuming counterfactual or non-falsifiable \"data-generating mechanisms\" or probability models for the data, as might be done in frequentist or Bayesian approaches.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "However, if a \"data generating mechanism\" does exist in reality, then according to Shannon's source coding theorem it provides the MDL description of the data, on average and asymptotically. In minimizing description length (or descriptive complexity), MDL estimation is similar to maximum likelihood estimation and maximum a posteriori estimation (using maximum-entropy Bayesian priors). However, MDL avoids assuming that the underlying probability model is known; the MDL principle can also be applied without assumptions that e.g. the data arose from independent sampling.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Developing ideas of Fisher and of Pitman from 1938 to 1939, George A. Barnard developed \"structural inference\" or \"pivotal inference\", an approach using invariant probabilities on group families. Barnard reformulated the arguments behind fiducial inference on a restricted class of models on which \"fiducial\" procedures would be well-defined and useful. Donald A. S. Fraser developed a general theory for structural inference based on group theory and applied this to linear models. The theory formulated by Fraser has close links to decision theory and Bayesian statistics and can provide optimal frequentist decision rules if they exist.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Initially, predictive inference was based on observable parameters and it was the main purpose of studying probability, but it fell out of favor in the 20th century due to a new parametric approach pioneered by Bruno de Finetti. The approach modeled phenomena as a physical system observed with error (e.g., celestial mechanics). De Finetti's idea of exchangeability\u2014that future observations should behave like past observations\u2014came to the attention of the English-speaking world with the 1974 translation from French of his 1937 paper, and has since been propounded by such statisticians as Seymour Geisser.\n", "link": "https://en.wikipedia.org/wiki/Statistical_inference"}, {"text": "Exploratory data analysis (EDA) relies heavily on such techniques. They can also provide insight into a data set to help with testing assumptions, model selection and regression model validation, estimator selection, relationship identification, factor effect determination, and outlier detection. In addition, the choice of appropriate statistical graphics can provide a convincing means of communicating the underlying message that is present in the data to others.\n", "link": "https://en.wikipedia.org/wiki/Statistical_graphics"}, {"text": "Infographics (a clipped compound of \"information\" and \"graphics\") are graphic visual representations of information, data, or knowledge intended to present information quickly and clearly.  They can improve cognition by using graphics to enhance the human visual system's ability to see patterns and trends.  Similar pursuits are information visualization, data visualization, statistical graphics, information design, or information architecture. Infographics have evolved in recent years to be for mass communication, and thus are designed with fewer assumptions about the readers' knowledge base than other types of visualizations. Isotypes are an early example of infographics conveying information quickly and easily to the masses.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Infographics have been around for many years and recently the increase of the number of easy-to-use, free tools have made the creation of infographics available to a large segment of the population.  Social media sites such as Facebook and Twitter have also allowed for individual infographics to be spread among many people around the world. Infographics are widely used in the age of short attention span.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "In newspapers, infographics are commonly used to show the weather, as well as maps, site plans, and graphs for summaries of data. Some books are almost entirely made up of information graphics, such as David Macaulay's The Way Things Work.  The Snapshots in USA Today are also an example of simple infographics used to convey news and current events.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Modern maps, especially route maps for transit systems, use infographic techniques to integrate a variety of information, such as the conceptual layout of the transit network, transfer points, and local landmarks.  Public transportation maps, such as those for the Washington Metro and the London Underground map, are well-known infographics.  Public places such as transit terminals usually have some sort of integrated \"signage system\" with standardized icons and stylized maps.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "In 1786, William Playfair, an engineer and political economist, published the first data graphs in his book The Commercial and Political Atlas. To represent the economy of 18th century England, Playfair used statistical graphs, bar charts, line graphs, area charts, and histograms. In his work, Statistical Breviary, he is credited with introducing the first pie chart.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Around 1820, modern geography was established by Carl Ritter.   His maps included shared frames, agreed map legends, scales, repeatability, and fidelity. Such a map can be considered a \"supersign\" which combines sign systems\u2014as defined by Charles Sanders Peirce\u2014consisting of symbols, icons, indexes as representations.  Other examples can be seen in the works of geographers Ritter and Alexander von Humboldt.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "In 1857, English nurse Florence Nightingale used information graphics to persuade Queen Victoria to improve conditions in military hospitals.  The principal one she used was the Coxcomb chart, a combination of stacked bar and pie charts, depicting the number and causes of deaths during each month of the Crimean War.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "1861 saw the release of an influential information graphic on the subject of Napoleon's disastrous march on Moscow. The graphic's creator, Charles Joseph Minard, captured four different changing variables that contributed to Napoleon's downfall in a single two-dimensional image: the army's direction as they traveled, the location the troops passed through, the size of the army as troops died from hunger and wounds, and the freezing temperatures they experienced.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "In 1900, the African-American historian, sociologist, writer, and Black rights activist, W.E.B. Du Bois presented data visualizations at the Exposition Universelle (1900) in Paris, France. In addition to curating 500 photographs of the lives of Black Americans, Du Bois and his Atlanta University team of students and scholars created 60 handmade data visualizations  to document the ways Black Americans were being denied access to education, housing, employment, and household wealth.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "The Cologne Progressives developed an aesthetic approach to art that focused on communicating information. Gerd Arntz, Peter Alma and Augustin Tschinkel, all participants in this movement were recruited by Otto Neurath for the Gesellschafts- und Wirtschaftsmuseum, where they developed the Vienna Method from 1926 to 1934. Here simple images were used to represent data in a structured way. Following the victory of Austrofascism in the Austrian Civil War, the team moved to the Netherlands where they continued their work rebranding it Isotypes (International System of Typographic Picture Education). The method was also applied by IZOSTAT (\u0418\u0417\u041e\u0421\u0422\u0410\u0422) in the Soviet Union.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "In 1958 Stephen Toulmin proposed a graphical argument model, called The Toulmin Model of Argumentation. The diagram contained six interrelated components used for analyzing arguments and was considered Toulmin's most influential work, particularly in the field of rhetoric, communication, and computer science. The Toulmin Model of Argumentation became influential in argumentation theory and its applications.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "In 1972 and 1973, respectively, the Pioneer 10 and Pioneer 11 spacecraft included on their vessels the Pioneer Plaques, a pair of gold-anodized aluminum plaques, each featuring a pictorial message. The pictorial messages included nude male and female figures as well as symbols that were intended to provide information about the origin of the spacecraft. The images were designed by Carl Sagan and Frank Drake and were unique in that their graphical meanings were to be understandable to extraterrestrial beings, who would have no conception of human language.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Likewise, television began to incorporate infographics into the viewers' experiences in the early 2000s. One example of infographics usage in television and in pop culture is the 2002 music video by the Norwegian musicians of R\u00f6yksopp, for their song \"Remind Me.\" The video was composed entirely of animated infographics. Similarly, in 2004, a television commercial for the French nuclear technology company Areva used animated infographics as an advertising tactic. Both of these videos and the attention they received have conveyed to other fields the potential value of using information graphics to describe complex information efficiently.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "The field of journalism has also incorporated and applied information graphics to news stories. For stories that intend to include text, images, and graphics, the system called the maestro concept allows entire newsrooms to collaborate and organize a story to successfully incorporate all components. Across many newsrooms, this teamwork-integrated system is applied to improve time management. The maestro system is designed to improve the presentation of stories for busy readers of media. Many news-based websites have also used interactive information graphics in which the user can extract information on a subject as they explore the graphic.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "With the popularity of social media, infographics have become popular, often as static images or simple web interfaces, covering any number of topics. Such infographics are often shared between users of social networks such as Facebook, Twitter, Pinterest, Google+ and Reddit. The hashtag #infographic was tweeted 56,765 times in March 2012 and at its peak 3,365 times in a span of 24 hours.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "When designing the visual aspect of an infographic, a number of considerations must be made to optimize the effectiveness of the visualization.  The six components of visual encoding are spatial, marks, connection, enclosure, retinal properties, and temporal encoding.  Each of these can be utilized in its own way to represent relationships between different types of data.  However, studies have shown that spatial position is the most effective way to represent numerical data and leads to the fastest and easiest understanding by viewers.  Therefore, the designers often spatially represent the most important relationship being depicted in an infographic.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "However, the appeal and the retention can in practice be put together with the aid of a comprehensible layout design. Recently, as an attempt to study the effect of the layout of an infographic on the comprehension of the viewers, a new Neural Network-based cognitive load estimation method was applied on different types of common layouts for the infographic design. When the varieties of factors listed above are taken into consideration when designing infographics, they can be a highly efficient and effective way to convey large amounts of information in a visual manner.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Data visualizations are often used in infographics and may make up the entire infographic.  There are many types of visualizations that can be used to represent the same set of data. Therefore, it is crucial to identify the appropriate visualization for the data set and infographic by taking into consideration graphical features such as position, size, shape, and color. There are primarily five types of visualization categories \u2013 time-series data, statistical distributions, maps, hierarchies, and networking.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "While all of these visualizations can be effectively used on their own, many modern infographics combine multiple types into one graphic, along with other features, such as illustrations and text.  Some modern infographics do not even contain data visualization, and instead are simply a colorful and succinct ways to present knowledge.  Fifty-three percent of the 30 most-viewed infographics on the infographic sharing site visual.ly did not contain actual data.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Comparison infographics are a type of visual representation that focuses on comparing and contrasting different elements, such as products, services, options, or features. These infographics are designed to help viewers make informed decisions by presenting information in a clear and concise manner. Comparison infographics can be highly effective in simplifying complex data and highlighting key differences between multiple items.\n", "link": "https://en.wikipedia.org/wiki/Infographic"}, {"text": "Data science is \"a concept to unify statistics, data analysis, informatics, and their related methods\" to \"understand and analyze actual phenomena\" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a \"fourth paradigm\" of science (empirical, theoretical, computational, and now data-driven) and asserted that \"everything about science is changing because of the impact of information technology\" and the data deluge.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "Many statisticians, including Nate Silver, have argued that data science is not a new field, but rather another name for statistics. Others argue that data science is distinct from statistics because it focuses on problems and techniques unique to digital data. Vasant Dhar writes that statistics emphasizes quantitative data and description. In contrast, data science deals with quantitative and qualitative data (e.g., from images, text, sensors, transactions, customer information, etc.) and emphasizes prediction and action. Andrew Gelman of Columbia University has described statistics as a non-essential part of data science.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "Stanford professor David Donoho writes that data science is not distinguished from statistics by the size of datasets or use of computing and that many graduate programs misleadingly advertise their analytics and statistics training as the essence of a data-science program. He describes data science as an applied field growing out of traditional statistics.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "In 1962, John Tukey described a field he called \"data analysis\", which resembles modern data science. In 1985, in a lecture given to the Chinese Academy of Sciences in Beijing, C.\u00a0F. Jeff Wu used the term \"data science\" for the first time as an alternative name for statistics. Later, attendees at a 1992 statistics symposium at the University of Montpellier\u00a0 II acknowledged the emergence of a new discipline focused on data of various origins and forms, combining established concepts and principles of statistics and data analysis with computing.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "In 2012, technologists Thomas H. Davenport and DJ Patil declared \"Data Scientist: The Sexiest Job of the 21st Century\", a catchphrase that was picked up even by major-city newspapers like the New York Times and the Boston Globe. A decade later, they reaffirmed it, stating that \"the job is more in demand than ever with employers\".\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "The professional title of \"data scientist\" has been attributed to DJ Patil and Jeff Hammerbacher in 2008. Though it was used by the National Science Board in their 2005 report \"Long-Lived Digital Data Collections: Enabling Research and Education in the 21st Century\", it referred broadly to any key role in managing a digital data collection.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "There is still no consensus on the definition of data science, and it is considered by some to be a buzzword. Big data is a related marketing term. Data scientists are responsible for breaking down big data into usable information and creating software and algorithms that help companies and organizations determine optimal operations.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "Data science and data analysis are both important disciplines in the field of data management and analysis, but they differ in several key ways. While both fields involve working with data, data science is more of an interdisciplinary field that involves the application of statistical, computational, and machine learning methods to extract insights from data and make predictions, while data analysis is more focused on the examination and interpretation of data to identify patterns and trends.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "Data analysis typically involves working with smaller, structured datasets to answer specific questions or solve specific problems. This can involve tasks such as data cleaning, data visualization, and exploratory data analysis to gain insights into the data and develop hypotheses about relationships between variables. Data analysts typically use statistical methods to test these hypotheses and draw conclusions from the data. For example, a data analyst might analyze sales data to identify trends in customer behavior and make recommendations for marketing strategies.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "While data analysis focuses on extracting insights from existing data, data science goes beyond that by incorporating the development and implementation of predictive models to make informed decisions. Data scientists are often responsible for collecting and cleaning data, selecting appropriate analytical techniques, and deploying models in real-world scenarios. They work at the intersection of mathematics, computer science and domain expertise to solve complex problems and uncover hidden patterns in large datasets.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "Despite these differences, data science and data analysis are closely related fields and often require similar skill sets. Both fields require a solid foundation in statistics, programming, and data visualization, as well as the ability to communicate findings effectively to both technical and non-technical audiences. Both fields benefit from critical thinking and domain knowledge, as understanding the context and nuances of the data is essential for accurate analysis and modeling.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "In summary, data analysis and data science are distinct yet interconnected disciplines within the broader field of data management and analysis. Data analysis focuses on extracting insights and drawing conclusions from structured data, while data science involves a more comprehensive approach that combines statistical analysis, computational methods, and machine learning to extract insights, build predictive models, and drive data-driven decision-making. Both fields use data to understand patterns, make informed decisions, and solve complex problems across various domains.\n", "link": "https://en.wikipedia.org/wiki/Data_science"}, {"text": "Tamara Macushla Munzner was born in 1969 in Minneapolis, Minnesota. She is the daughter of abstract painter Aribert Munzner. She graduated from South High School in 1986. She earned a Bachelor of Science degree in computer science from Stanford University in 1991. She returned to Stanford for her graduate studies, completing her Ph.D. in 2000 under the supervision of Pat Hanrahan. Her thesis, Interactive Visualization of Large Graphs and Networks, involved using hyperbolic geometry to visualize large graphs.\n", "link": "https://en.wikipedia.org/wiki/Tamara_Munzner"}, {"text": "Munzner worked as an intern at ETA Systems in the 1980s while still in college. Munzner then worked at The Geometry Center at the University of Minnesota from 1991 to 1995. There, she helped produce two mathematical visualization videos, one about turning spheres inside-out and another about the different topological structures that a three-dimensional universe could have.\n", "link": "https://en.wikipedia.org/wiki/Tamara_Munzner"}, {"text": "Munzner is the author of the book Visualization Analysis and Design (CRC Press, 2014). She was program co-chair of the InfoVis conference in 2003 and 2004, and of EuroVis in 2009 and 2010. She has served as the chair of the steering committee of Institute of Electrical and Electronics Engineers (IEEE) InfoVis and the executive committee of IEEE Visualization.\n", "link": "https://en.wikipedia.org/wiki/Tamara_Munzner"}, {"text": "Ben Shneiderman (born August 21, 1947) is an American computer scientist, a Distinguished University Professor in the University of Maryland Department of Computer Science, which is part of the University of Maryland College of Computer, Mathematical, and Natural Sciences at the University of Maryland, College Park, and the founding director (1983-2000) of the   University of Maryland Human-Computer Interaction Lab. He conducted fundamental research in the field of human\u2013computer interaction, developing new ideas, methods, and tools such as the direct manipulation interface, and his eight rules of design.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "Born in New York, Shneiderman, attended the Bronx High School of Science, and received a BS in Mathematics and Physics from the City College of New York in 1968. He then went on to study at the State University of New York at Stony Brook, where he received an MS in Computer Science in 1972 and graduated with a PhD in 1973.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "In 2002 his book Leonardo's Laptop: Human Needs and the New Computing Technologies was Winner of an IEEE-USA Award for Distinguished Contributions Furthering Public Understanding of the Profession.  His 2016 book, The New ABCs of Research: Achieving Breakthrough Collaborations, encourages applied and basic research to be combined. In 2019, he published Encounters with HCI Pioneers: A Personal History and Photo Journal, and Human-Centered AI in 2022.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "With the advent of structured programming and GOTO-less programming a method is needed to model computation in simply ordered structures, each representing a complete thought possibly defined in terms of other thoughts as yet undefined. A model is needed which prevents unrestricted transfers of control and has a control structure closer to languages amenable to structured programming. We present an attempt at such a model.", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "Flowcharts have been a part of computer programming since the introduction of computers in the 1940s. In 1947 Goldstein and von Neumann [7] presented a system of describing processes using operation, assertion, and alternative boxes. They felt that \"coding begins with the drawing of flow diagram.\" Prior to coding, the algorithm had been identified and understood. The flowchart represented a high level definition of the solution to be implemented on a machine. Although they were working only with numerical algorithms, they proposed a programming methodology which has since become standard practice in the computer programming field.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "Direct manipulation concepts led to touchscreen interfaces for home controls, finger-painting, and the now ubiquitous small touchscreen keyboards. The development of the \"Lift-off strategy\" by University of Maryland Human\u2013Computer Interaction Lab (HCIL) researchers enabled users to touch the screen, getting feedback as to what will be selected, adjust their finger position, and complete the selection by lifting the finger off the screen.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "His major work in recent years has been on information visualization, originating the treemap concept for hierarchical data. Treemaps are implemented in most information visualization tools including Spotfire, Tableau Software, QlikView, SAS, JMP, and Microsoft Excel. Treemaps are included in hard drive exploration tools, stock market data analysis, census systems, election data, gene expression, and data journalism. The artistic side of treemaps are on view in the Treemap Art Project.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "He also developed dynamic queries sliders with multiple coordinated displays that are a key component of Spotfire, which was acquired by TIBCO in 2007.  His work continued on visual analysis tools for time series data, TimeSearcher, high dimensional data, Hierarchical Clustering Explorer, and social network data, SocialAction.  Shneiderman contributed to the widely used social network analysis and visualization tool NodeXL.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "Current work deals with visualization of temporal event sequences, such as found in Electronic Health Records, in systems such as LifeLines2  and EventFlow. These tools visualize the categorical data that make up a single patient history and they present an aggregated view that enables analysts to find patterns in large patient history databases.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "In 2012, Jeffrey Heer and Shneiderman coauthored the article \"Interactive Dynamics for Visual Analysis\" in Association for Computing Machinery Queue vol. 10, no. 2. Included in this article is a taxonomy of interactive dynamics to assist researchers, designers, analysts, educators, and students in evaluating and creating visual analysis tools. The taxonomy consists of 12 task types grouped into three high-level categories, as shown below.\n", "link": "https://en.wikipedia.org/wiki/Ben_Shneiderman"}, {"text": "John Wilder Tukey (/\u02c8tu\u02d0ki/; June 16, 1915 \u2013 July 26, 2000) was an American mathematician and statistician, best known for the development of the fast Fourier Transform (FFT) algorithm and box plot. The Tukey range test, the Tukey lambda distribution, the Tukey test of additivity, and the Teichm\u00fcller\u2013Tukey lemma all bear his name. He is also credited with coining the term bit and the first published use of the word software.\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "Tukey was born in New Bedford, Massachusetts, in 1915, to a Latin teacher father and a private tutor. He was mainly taught by his mother and attended regular classes only for certain subjects like French. Tukey obtained a B.A. in 1936 and M.S. in 1937 in chemistry, from Brown University, before moving to Princeton University, where in 1939 he received a PhD in mathematics after completing a doctoral dissertation titled \"On denumerability in topology\".\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "During World War II, Tukey worked at the Fire Control Research Office and collaborated with Samuel Wilks and William Cochran. He is claimed to have helped design the U-2 spy plane. After the war, he returned to Princeton, dividing his time between the university and AT&T Bell Laboratories. In 1962, Tukey was elected to the American Philosophical Society. He became a full professor at 35 and founding chairman of the Princeton statistics department in 1965.\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "Among many contributions to civil society, Tukey served on a committee of the American Statistical Association that produced a report critiquing the statistical methodology of the Kinsey Report, Statistical Problems of the Kinsey Report on Sexual Behavior in the Human Male, which summarized \"A random selection of three people would have been better than a group of 300 chosen by Mr. Kinsey\".\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "His statistical interests were many and varied. He is particularly remembered for his development with James Cooley of the Cooley\u2013Tukey FFT algorithm. In 1970, he contributed significantly to what is today known as the jackknife\u2014also termed Quenouille\u2013Tukey jackknife. He introduced the box plot in his 1977 book, \"Exploratory Data Analysis\".\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "Making sense of data has a long history and has been addressed by statisticians, mathematicians, scientists, and others for many many years. During the 1960s, Tukey challenged the dominance at the time of what he called \"confirmatory data analysis\", statistical analyses driven by rigid mathematical configurations. Tukey emphasized the importance of having a more flexible attitude towards data analysis and of exploring data carefully to see what structures and information might be contained therein. He called this \"exploratory data analysis\" (EDA). In many ways, EDA was a precursor to data science.\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "Tukey also realized the importance of computer science to EDA. Graphics are an integral part of EDA methodology and, while much of Tukey's work focused on static displays (such as box plots) that could be drawn by hand, he realized that computer graphics would be much more effective for studying multivariate data. PRIM-9, the first program for viewing multivariate data, was conceived by him during the early 1970s.\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "Tukey articulated the important distinction between exploratory data analysis and confirmatory data analysis, believing that much statistical methodology placed too great an emphasis on the latter. Though he believed in the utility of separating the two types of analysis, he pointed out that sometimes, especially in natural science, this was problematic and termed such situations uncomfortable science.\n", "link": "https://en.wikipedia.org/wiki/John_Tukey"}, {"text": "Tufte was hired in 1967 by the Woodrow Wilson School of Princeton University as a lecturer in politics and public affairs, where he steadily moved up to the rank of full Professor in 1972. He taught courses there in political economy and data analysis while publishing three quantitatively inclined political science books.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "In 1975, while at Princeton, Tufte was asked to teach a statistics course to a group of journalists who were visiting the school to study economics. He developed a set of readings and lectures on statistical graphics, which he further developed in joint seminars he taught with renowned statistician John Tukey, a pioneer in the field of information design. These course materials became the foundation for Tufte's first book on information design, The Visual Display of Quantitative Information.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "After negotiations with major publishers failed, Tufte decided to self-publish the book The Visual Display of Quantitative Information in 1982, working closely with graphic designer Howard Gralla. Tufte financed the work by taking out a second mortgage on his home. The book quickly became a commercial success and secured Tufte's transition from political scientist to information expert.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Tufte's writing is important in such fields as information design and visual literacy, which deal with the visual communication of information.  He coined the word chartjunk to refer to useless, non-informative, or information-obscuring elements of quantitative information displays.  Tufte's other key concepts include what he calls the lie factor, the data-ink ratio, and the data density of a graphic.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Tufte uses the term \"data-ink ratio\" to argue against using excessive decoration in visual displays of quantitative information. In Visual Display, Tufte explains, \"Sometimes decoration can help editorialize about the substance of the graphic. But it is wrong to distort the data measures\u2014the ink locating values of numbers\u2014in order to make an editorial comment or fit a decorative scheme.\"\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Tufte encourages the use of data-rich illustrations that present all available data. When such illustrations are examined closely, every data point has a value, but when they are looked at more generally, only trends and patterns can be observed. Tufte suggests these macro/micro readings be presented in the space of an eye-span, in the high resolution format of the printed page, and at the unhurried pace of the viewer's leisure.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Tufte cites the way PowerPoint was used by NASA engineers in the events leading to the Space Shuttle Columbia disaster as an example of PowerPoint's many problems. The software style is designed to persuade rather than to inform people of technical details.  Tufte's analysis of a NASA PowerPoint slide is included in the Columbia Accident Investigation Board\u2019s report -- including an engineering detail buried in small type on a crowded slide with six bullet points, that if presented in a regular engineering white paper, might have been noticed and the disaster prevented.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Instead, Tufte argues that the most effective way of presenting information in a technical setting, such as an academic seminar or a meeting of industry experts, is by distributing a brief written report that can be read by all participants in the first 5 to 10 minutes of the meeting.  Tufte believes that this is the most efficient method of transferring knowledge from the presenter to the audience and then the rest of the meeting is devoted to discussion and debate.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "One method Tufte encourages to allow quick visual comparison of multiple series is the small multiple, a chart with many series shown on a single pair of axes that can often be easier to read when displayed as several separate pairs of axes placed next to each other. He suggests this is particularly helpful when the series are measured on quite different vertical (y-axis) scales, but over the same range on the horizontal x-axis (usually time).\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Beyond his academic endeavors over the years, Tufte has created sculptures, often large outdoor ones made of metal or stone, that were first primarily exhibited on his own rural Connecticut property. In 2009\u201310, some of these artworks were exhibited at the Aldrich Contemporary Art Museum in Ridgefield, Connecticut, in the one-man show Edward Tufte: Seeing Around.\n", "link": "https://en.wikipedia.org/wiki/Edward_Tufte"}, {"text": "Rosling was born in Uppsala, Sweden, on 28 July 1948. From 1967 to 1974, he studied statistics and medicine at Uppsala University, and in 1972 he studied public health at St. John's Medical College, Bangalore, India. He became a licensed physician in 1976 and from 1979 to 1981 he served as District Medical Officer in Nacala in northern Mozambique. In 1981, he began investigating an outbreak of konzo, a paralytic disease first described in the Democratic Republic of the Congo. His investigations earned him a Ph.D. at Uppsala University in 1986. Rosling was dyslexic.\n", "link": "https://en.wikipedia.org/wiki/Hans_Rosling"}, {"text": "Rosling was a sword swallower, as demonstrated in the final moments of his second talk at the TED conference. In 2009 he was listed as one of 100 leading global thinkers by Foreign Policy, and in 2011 as one of 100 most creative people in business by Fast Company. In 2011 he was elected member of the Swedish Academy of Engineering Sciences and in 2012 as member of the Swedish Academy of Sciences. He was included in the Time 100 list of the world's 100 most influential people in 2012.\n", "link": "https://en.wikipedia.org/wiki/Hans_Rosling"}, {"text": "Rosling spent two decades studying outbreaks of konzo, a paralytic disease, in remote rural areas across Africa and supervised more than ten PhD students. His work with Julie Cliff, Johannes M\u00e5rtensson, Per Lundqvist, and Bo S\u00f6rbo found that outbreaks occur among hunger-stricken rural populations in Africa where a diet dominated by insufficiently processed cassava results in simultaneous malnutrition and high dietary cyanide intake.\n", "link": "https://en.wikipedia.org/wiki/Hans_Rosling"}, {"text": "Rosling's son, Ola Rosling, built the Trendalyzer software to animate data compiled by the UN and the World Bank that helped him explain the world with graphics. Rosling co-founded the Gapminder Foundation together with his son Ola and daughter-in-law Anna Rosling R\u00f6nnlund to develop Trendalyzer to convert international statistics into moving, interactive graphics. The provocative presentations that have resulted have made him famous, and his lectures using Gapminder graphics to visualize world development have won awards. The interactive animations are freely available from the Foundation's website.\n", "link": "https://en.wikipedia.org/wiki/Hans_Rosling"}, {"text": "In his later years he advocated on behalf of refugees from Syria and partnered with the United Nations High Commissioner for Refugees (UNHCR) in this effort. In his last book he wrote repeatedly about the tragedy of the war in Syria saying: \"The Syrian conflict will most likely prove to be the deadliest in the world since the Ethiopian-Eritrean war of 1998 to 2000.\"\n", "link": "https://en.wikipedia.org/wiki/Hans_Rosling"}, {"text": "McCandless is the founder of the visual blog Information Is Beautiful. Early explorations into the synergy between data visualisation and his work as a journalist led to the development of Information Is Beautiful and the subsequent publication of his book of the same name (titled A Visual Miscellaneum in the United States).\n", "link": "https://en.wikipedia.org/wiki/David_McCandless"}, {"text": "McCandless began his career writing for cult video game magazines such as Your Sinclair and PC Zone in the late 1980s and 1990s before moving on to work for The Guardian and Wired magazine. Since the publication of Information Is Beautiful in 2009, his information design work has appeared in numerous publications, including The Guardian, Wired, and Die Zeit, and has also been showcased at the Museum of Modern Art in New York, the Wellcome Trust gallery in London, and at the Tate Britain. His second book, Knowledge Is Beautiful, was published in 2014\n", "link": "https://en.wikipedia.org/wiki/David_McCandless"}, {"text": "Kim Albrecht is a German data artist, media artist, information designer, and scholar known for his critical and investigative data visualizations. He is a professor at the Filmuniversit\u00e4t Babelsberg Konrad Wolf, principle of metaLAB (at) Harvard, and co-founder of metaLAB (at) Berlin. Albrecht earned his Ph.D. from the University of Potsdam and is a faculty associate of the Berkman Klein Center for Internet & Society. His project \"Artificial Worldviews\" was featured on the cover of the science magazine Nature's 10 in December 2023.\n", "link": "https://en.wikipedia.org/wiki/Kim_Albrecht"}, {"text": "Albrecht's works have been exhibited worldwide in internationally renowned institutions such as the Harvard Art Museums, the Cooper Hewitt Design Museum New York, the ZKM Center for Art and Media, the Istanbul Contemporary Art Museum, the Kunsthaus Graz, and the Ars Electronica Center. Albrecht has won numerous international awards in the field of information design and his works are part of the permanent collections of the ZKM, the Cooper Hewitt Design Museum und des Ars Electronica Center.\n", "link": "https://en.wikipedia.org/wiki/Kim_Albrecht"}, {"text": "Born 1974 in St. Gallen, Osterwalder obtained his MA in Political Science in 2000 at the University of Lausanne, where in 2004 he also obtained his PhD in Management Information Systems under Yves Pigneur with the thesis, entitled \"The Business Model Ontology - a proposition in a design science approach.\"\n", "link": "https://en.wikipedia.org/wiki/Alexander_Osterwalder"}, {"text": "In 1999 Osterwalder co-founded his first startup Netfinance.ch, which focused on financial literacy. He was a journalist for the Swiss business magazine BILANZ in 2000\u201301, and Senior Research Fellow back at the University of Lausanne from 2000 to 2005 in the time he finished his PhD research. In 2006 he founded BusinessModelDesign.com, and in 2010 he co-founded  the consultancy firm Strategyzer, which has provided over 5 million people with Osterwalder's Business Model Canvas.\n", "link": "https://en.wikipedia.org/wiki/Alexander_Osterwalder"}, {"text": "Edward Hawkins MBE (born 1977) is a British climate scientist who is Professor of climate science at the University of Reading, principal research scientist at the National Centre for Atmospheric Science (NCAS), editor of Climate Lab Book blog and lead scientist for the Weather Rescue citizen science project. He is known for his data visualizations of climate change for the general public such as warming stripes and climate spirals.\n", "link": "https://en.wikipedia.org/wiki/Ed_Hawkins_(climatologist)"}, {"text": "As of 2023 Hawkins is a professor of climate science at the University of Reading, where he serves as academic lead for public engagement and is affiliated with the National Centre for Atmospheric Science (NCAS). He is a lead for Weather Rescue and Rainfall Rescue, citizen science projects in which volunteers transcribe data from historical meteorological and rainfall records for digital analysis.\n", "link": "https://en.wikipedia.org/wiki/Ed_Hawkins_(climatologist)"}, {"text": "On 22 May 2018, Hawkins published his warming stripes data visualization graphic, which has been used by meteorologists in Climate Central's annual #MetsUnite campaign to raise public awareness of global warming during broadcasts on the summer solstice. Hawkins' similar #ShowYourStripes initiative, in which the public could freely download and share graphics customized to specific countries or localities, was launched on 17 June 2019. The warming stripes graphic is used in the logo of the U.S. House Select Committee on the Climate Crisis from 2019 onwards.\n", "link": "https://en.wikipedia.org/wiki/Ed_Hawkins_(climatologist)"}, {"text": "Hadley Alexander Wickham (born 14 October 1979) is a New Zealand statistician known for his work on open-source software for the R statistical programming environment. He is the chief scientist at Posit PBC and an adjunct professor of statistics at the University of Auckland, Stanford University, and Rice University. His work includes the data visualisation system ggplot2 and the tidyverse, a collection of R packages for data science based on the concept of tidy data.\n", "link": "https://en.wikipedia.org/wiki/Hadley_Wickham"}, {"text": "Wickham was born in Hamilton, New Zealand. He received a Bachelors degree in Human Biology and a master's degree in statistics at the University of Auckland in 1999\u20132004 and his PhD at Iowa State University in 2008 supervised by Di Cook and Heike Hofmann. He is the chief scientist at Posit PBC (formerly RStudio PBC) and an adjunct professor of statistics at the University of Auckland, Stanford University, and Rice University.\n", "link": "https://en.wikipedia.org/wiki/Hadley_Wickham"}, {"text": "Wickham is a prominent and active member of the R user community and has developed several notable and widely used packages including ggplot2, plyr, dplyr and reshape2. Wickham's data analysis packages for R are collectively known as the tidyverse. According to Wickham's tidy data approach, each variable should be a column, each observation should be a row, and each type of observational unit should be a table.\n", "link": "https://en.wikipedia.org/wiki/Hadley_Wickham"}, {"text": "In 2006 he was awarded the John Chambers Award for Statistical Computing for his work developing tools for data reshaping and visualisation. Wickham was named a Fellow by the American Statistical Association in 2015 for \"pivotal contributions to statistical practice through innovative and pioneering research in statistical graphics and computing\". Wickham was awarded the international COPSS Presidents' Award in 2019 for \"influential work in statistical computing, visualisation, graphics, and data analysis\" including \"making statistical thinking and computing accessible to a large audience\".\n", "link": "https://en.wikipedia.org/wiki/Hadley_Wickham"}, {"text": "While attending Yale between 1974 and 1976, he served as an instructor of psychology. He became an assistant professor of psychology at University of Illinois at Chicago (UIC) in 1976 and was promoted to associate professor in 1980. In 1991, he became an adjunct professor of statistics at Northwestern University. He remained in that role until 2010. He rejoined UIC in 2007 as an adjunct professor of computer science.\n", "link": "https://en.wikipedia.org/wiki/Leland_Wilkinson"}, {"text": "Wilkinson became a Fellow of the American Statistical Association in 1998 and a Fellow of the American Association for the Advancement of Science in 2009. He was an elected member of the International Statistical Institute in 2006. Wilkinson received the National Institute of Statistical Sciences Distinguished Service Award in 2010.\n", "link": "https://en.wikipedia.org/wiki/Leland_Wilkinson"}, {"text": "For several years, Bostock led data visualization projects at the New York Times, where he developed several notable interactive news articles. For this work, he shared the 2013, 2014, and 2015 Gerald Loeb Awards for Images/Visuals. He left his position at the Times in 2015 to focus on other projects.\n", "link": "https://en.wikipedia.org/wiki/Mike_Bostock"}, {"text": "Bostock has received recognition for his work. In 2013, the influential statistician Edward Tufte predicted that he will become one of the most important people for the future of data visualization, and in 2015, the New York Times' \"Innovation Report\" called him a \"digital superstar\". Bostock was also interviewed by the Data Stories podcast and presented at Eyeo 2014. He advised the data platform provider Trifacta, which was later acquired by the technology company Alteryx.\n", "link": "https://en.wikipedia.org/wiki/Mike_Bostock"}, {"text": "Jeffrey Michael Heer (born June 15, 1979) is an American computer scientist best known for his work on information visualization and interactive data analysis. He is a professor of computer science & engineering at the University of Washington, where he directs the UW Interactive Data Lab. He co-founded Trifacta with Joe Hellerstein and Sean Kandel in 2012.\n", "link": "https://en.wikipedia.org/wiki/Jeffrey_Heer"}, {"text": "Heer was an assistant professor of computer science at Stanford University, from 2009 to 2013. He is also co-founder and chief experience officer of Trifacta. Heer's research focuses on new systems and techniques for data visualization. As a member of the Stanford University faculty, he worked with Mike Bostock on the Protovis and D3.js systems.\n", "link": "https://en.wikipedia.org/wiki/Jeffrey_Heer"}, {"text": "Heer then moved to the University of Washington where he worked with students and collaborators to develop the Vega and Vega-Lite visualisation grammars. Along with Joe Hellerstein and Sean Kandel, Heer has also developed interactive tools for data transformation (including Data Wrangler), leading to the founding of Trifacta. Other research contributions include work on the graphical perception of visualizations, social data analysis, text visualization, and interactive language translation tools.\n", "link": "https://en.wikipedia.org/wiki/Jeffrey_Heer"}, {"text": "Heer's research has been recognized by an ACM Grace Murray Hopper Award, a Gordon and Betty Moore Foundation Data-Driven Discovery Investigator Award, an Alfred P. Sloan Fellowship, and MIT Technology Review's TR35 list. Heer and his students have won best paper awards at human-computer interaction and visualization conferences. His work has also appeared in the popular press.\n", "link": "https://en.wikipedia.org/wiki/Jeffrey_Heer"}, {"text": "Ihab Francis Ilyas (born May 13, 1973) is a computer scientist who works in data science. He is currently a professor of computer science in the David R. Cheriton School of Computer Science at the University of Waterloo. He also led the Knowledge  Platform team at Apple Inc. Ihab is the holder of the Thomson Reuters-NSERC Industrial Research Chair in Data Cleaning at the University of Waterloo.\n", "link": "https://en.wikipedia.org/wiki/Ihab_Ilyas"}, {"text": "Ilyas co-founded Tamr Inc., a start-up focusing on large-scale data integration and cleaning, with Andy Palmer and Michael Stonebraker, a Turing Award winner. Ilyas was the CEO of Inductiv Inc., an artificial intelligence start-up that uses machine learning to automate the task of identifying and correcting errors in data, which he co-founded with Theodoros Rekatsinas at the University of Wisconsin-Madison and Christopher R\u00e9 at Stanford University. Inductiv was acquired by Apple Inc. in May 2020.\n", "link": "https://en.wikipedia.org/wiki/Ihab_Ilyas"}, {"text": "Since 2009, he has focused research on data quality and the technical challenges in building data-cleaning systems. He and his research group introduced novel practical algorithms and system prototypes, which circumvent limitations of previous data-cleaning solutions that either focus narrowly on single types of data errors or ignore real-life considerations that prevent their adoption.\n", "link": "https://en.wikipedia.org/wiki/Ihab_Ilyas"}, {"text": "He was named an ACM Distinguished Scientist in 2014, and an ACM Fellow in 2020 for his contributions to data cleaning and data integration. He was also named IEEE Fellow in 2022 for his contributions in data cleaning, data integration and rank-aware query processing and a Fellow of the Royal Society of Canada in 2024. In 2024, he also received the 2024 C.C. Gotlieb Computer Award from IEEE Canada in recognition of his contributions to building large-scale machine learning systems for data integration, data cleaning, and knowledge construction.\n", "link": "https://en.wikipedia.org/wiki/Ihab_Ilyas"}, {"text": "Such a table representation of data is a great way to display exact values, but it can prevent the discovery and understanding of patterns in the values. In addition, a table display is often erroneously considered to be an objective, neutral collection or storage of the data (and may in that sense even be erroneously considered to be the data itself) whereas it is in fact just one of various possible visualizations of the data.\n", "link": "https://en.wikipedia.org/wiki/Line_chart"}, {"text": "A true best-fit layer should depict a continuous mathematical function whose parameters are determined by using a suitable error-minimization scheme, which appropriately weights the error in the data values. Such curve fitting functionality is often found in graphing software or spreadsheets. Best-fit curves may vary from simple linear equations to more complex quadratic, polynomial, exponential, and periodic curves.\n", "link": "https://en.wikipedia.org/wiki/Line_chart"}, {"text": "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally.  A vertical bar chart is sometimes called a column chart and has been identified as the prototype of charts.\n", "link": "https://en.wikipedia.org/wiki/Bar_chart"}, {"text": "Many sources consider William Playfair (1759-1824) to have invented the bar chart and the Exports and Imports of Scotland to and from different parts for one Year from Christmas 1780 to Christmas 1781 graph from his The Commercial and Political Atlas to be the first bar chart in history.  Diagrams of the velocity of a constantly accelerating object against time published in The Latitude of Forms (attributed to Jacobus de Sancto Martino or, perhaps, to Nicole Oresme) about 300 years before can be interpreted as \"proto bar charts\".\n", "link": "https://en.wikipedia.org/wiki/Bar_chart"}, {"text": "Bar graphs/charts provide a visual presentation of categorical data. Categorical data is a grouping of data into discrete groups, such as months of the year, age group, shoe sizes, and animals. These categories are usually qualitative. In a column (vertical) bar chart, categories appear along the horizontal axis and the height of the bar corresponds to the value of each category.\n", "link": "https://en.wikipedia.org/wiki/Bar_chart"}, {"text": "Bar charts have a discrete domain of categories, and are usually scaled so that all the data can fit on the chart. When there is no natural ordering of the categories being compared, bars on the chart may be arranged in any order. Bar charts arranged from highest to lowest incidence are called Pareto charts.\n", "link": "https://en.wikipedia.org/wiki/Bar_chart"}, {"text": "In grouped (clustered) bar charts, for each categorical group there are two or more bars color-coded to represent a particular grouping. For example, a business owner with two stores might make a grouped bar chart with different colored bars to represent each store: the horizontal axis would show the months of the year and the vertical axis would show revenue.\n", "link": "https://en.wikipedia.org/wiki/Bar_chart"}, {"text": "A histogram is a visual representation of the distribution of quantitative data. To construct a histogram, the first step is to \"bin\" (or \"bucket\") the range of values\u2014 divide the entire range of values into a series of intervals\u2014and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) are adjacent and are typically (but not required to be) of equal size.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "Histograms are sometimes confused with bar charts. In a histogram, each bin is for a different range of values, so altogether the histogram illustrates the distribution of values. But in a bar chart, each bar is for a different category of observations (e.g., each bar might be for a different population), so altogether the bar chart can be used to compare different categories. Some authors recommend that bar charts always have gaps between the bars to clarify that they are not histograms.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "Pearson himself noted in 1895 that although the term \"histogram\" was new, the type of graph it designates was \"a common form of graphical representation\".\nIn fact the technique of using a bar graph to represent statistical measurements was devised by the Scottish economist, William Playfair, in his Commercial and political atlas (1786).\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "The U.S. Census Bureau found that there were 124 million people who work outside of their homes.  Using their data on the time occupied by travel to work, the table below shows the absolute number of people who responded with travel times \"at least 30 but less than 35 minutes\" is higher than the numbers for the categories above and below it. This is likely due to people rounding their reported journey time. The problem of reporting values as somewhat arbitrarily rounded numbers is a common phenomenon when collecting data from people.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "This histogram shows the number of cases per unit interval as the height of each block, so that the area of each block is equal to the number of people in the survey who fall into its category. The area under the curve represents the total number of cases (124 million). This type of histogram shows absolute numbers, with Q in thousands.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "This histogram differs from the first only in the vertical scale.  The area of each block is the fraction of the total that each category represents, and the total area of all the bars is equal to 1 (the fraction meaning \"all\"). The curve displayed is a simple density estimate. This version shows proportions, and is also known as a unit area histogram.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "In other words, a histogram represents a frequency distribution by means of rectangles whose widths represent class intervals and whose areas are proportional to the corresponding frequencies: the height of each is the average frequency density for the interval. The intervals are placed together in order to show that the data represented by the histogram, while exclusive, is also contiguous. (E.g., in a histogram it is possible to have two connecting intervals of 10.5\u201320.5 and 20.5\u201333.5, but not two connecting intervals of 10.5\u201320.5 and 22.5\u201332.5.  Empty intervals are represented as empty and not skipped.)\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "The data used to construct a histogram are generated via a function mi that counts the number of observations that fall into each of the disjoint categories (known as bins). Thus, if we let n be the total number of observations and k be the total number of bins, the histogram data mi meet the following conditions:\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "Using wider bins where the density of the underlying data points is low reduces noise due to sampling randomness; using narrower bins where the density is high (so the signal drowns the noise) gives greater precision to the density estimation.  Thus varying the bin-width within a histogram can be beneficial.  Nonetheless, equal-width bins are widely used.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "Some theoreticians have attempted to determine an optimal number of bins, but these methods generally make strong assumptions about the shape of the distribution.  Depending on the actual data distribution and the goals of the analysis, different bin widths may be appropriate, so experimentation is usually needed to determine an appropriate width. There are, however, various useful guidelines and rules of thumb.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "Sturges's formula implicitly bases bin sizes on the range of the data, and can perform poorly if n\u00a0<\u00a030, because the number of bins will be small\u2014less than seven\u2014and unlikely to show trends in the data well. On the other extreme, Sturges's formula may overestimate bin width for very large datasets, resulting in oversmoothed histograms. It may also perform poorly if the data are not normally distributed.\n", "link": "https://en.wikipedia.org/wiki/Histogram"}, {"text": "A scatter plot, also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram, is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed.\nThe data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "According to Michael Friendly and Daniel Denis, the defining characteristic distinguishing scatter plots from line charts is the representation of specific observations of bivariate data where one variable is plotted on the horizontal axis and the other on the vertical axis. The two variables are often abstracted from a physical representation like the spread of bullets on a target or a geographic or celestial projection.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "While Edmund Halley created a bivariate plot of temperature and pressure in 1686, he omitted the specific data points used to demonstrate the relationship. Friendly and Denis claim his visualization was different from an actual scatter plot. Friendly and Denis attribute the first scatter plot to John Herschel. In 1833, Herschel plotted the angle between the central star in the constellation Virgo and Gamma Virginis over time to find how the angle changes over time, not through calculation but with freehand drawing and human judgment.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "Sir Francis Galton extended and popularized the scatter plot and many other statistical tools to pursue a scientific basis for eugenics. When, in 1886, Galton published a scatter plot and correlation ellipse of the height of parents and children, he extended Herschel's mere plotting of data points by binning and averaging adjacent cells to create a smoother visualization. Karl Pearson, R. A. Fischer, and other statisticians and eugenicists built on Galton's work and formalized correlations and significance testing.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "For example, to display a link between a person's lung capacity, and how long that person could hold their breath, a researcher would choose a group of people to study, then measure each one's lung capacity (first variable) and how long that person could hold their breath (second variable). The researcher would then plot the data in a scatter plot, assigning \"lung capacity\" to the horizontal axis, and \"time holding breath\" to the vertical axis.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "A person with a lung capacity of 400\u00a0cl who held their breath for 21.7\u00a0s would be represented by a single dot on the scatter plot at the point (400, 21.7) in the Cartesian coordinates. The scatter plot of all the people in the study would enable the researcher to obtain a visual comparison of the two variables in the data set and will help to determine what kind of relationship there might be between the two variables.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "For a set of data variables (dimensions) X1, X2, ... , Xk, the scatter plot matrix shows all the pairwise scatter plots of the variables on a single view with multiple scatterplots in a matrix format. For k variables, the scatterplot matrix will contain k rows and k columns. A plot located on the intersection of row and jth column is a plot of variables Xi versus Xj. This means that each row and column is one dimension, and each cell plots a scatter plot of two dimensions.\n", "link": "https://en.wikipedia.org/wiki/Scatter_plot"}, {"text": "The range-bar method was first introduced by Mary Eleanor Spear in her book \"Charting Statistics\" in 1952 and again in her book \"Practical Charting Techniques\" in 1969. The box-and-whisker plot was first introduced in 1970 by John Tukey, who later published on the subject in his book \"Exploratory Data Analysis\" in 1977.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "The whiskers must end at an observed data point, but can be defined in various ways. In the most straightforward method, the boundary of the lower whisker is the minimum value of the data set, and the boundary of the upper whisker is the maximum value of the data set. Because of this variability, it is appropriate to describe the convention that is being used for the whiskers and outliers in the caption of the box-plot.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "The unusual percentiles 2%, 9%, 91%, 98% are sometimes used for whisker cross-hatches and whisker ends to depict the seven-number summary. If the data are normally distributed, the locations of the seven marks on the box plot will be equally spaced. On some box plots, a cross-hatch is placed before the end of each whisker.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "The first quartile value (Q1 or 25th percentile) is the number that marks one quarter of the ordered data set. In other words, there are exactly 25% of the elements that are less than the first quartile and exactly 75% of the elements that are greater than it. The first quartile value can be easily determined by finding the \"middle\" number between the minimum and the median. For the hourly temperatures, the \"middle\" number found between 57\u00b0F and 70\u00b0F is 66\u00b0F.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "The third quartile value (Q3 or 75th percentile) is the number that marks three quarters of the ordered data set. In other words, there are exactly 75% of the elements that are less than the third quartile and 25% of the elements that are greater than it. The third quartile value can be easily obtained by finding the \"middle\" number between the median and the maximum. For the hourly temperatures, the \"middle\" number between 70\u00b0F and 81\u00b0F is 75\u00b0F.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "The upper whisker boundary of the box-plot is the largest data value that is within 1.5 IQR above the third quartile. Here, 1.5 IQR above the third quartile is 88.5\u00b0F and the maximum is 81\u00b0F. Therefore, the upper whisker is drawn at the value of the maximum, which is 81\u00b0F.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "Similarly, the lower whisker boundary of the box plot is the smallest data value that is within 1.5 IQR below the first quartile. Here, 1.5 IQR below the first quartile is 52.5\u00b0F and the minimum is 57\u00b0F. Therefore, the lower whisker is drawn at the value of the minimum, which is 57\u00b0F.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "In this case, the maximum value in this data set is 89\u00b0F, and 1.5 IQR above the third quartile is 88.5\u00b0F. The maximum is greater than 1.5 IQR plus the third quartile, so the maximum is an outlier. Therefore, the upper whisker is drawn at the greatest value smaller than 1.5 IQR above the third quartile, which is 79\u00b0F.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "Similarly, the minimum value in this data set is 52\u00b0F, and 1.5 IQR below the first quartile is 52.5\u00b0F. The minimum is smaller than 1.5 IQR minus the first quartile, so the minimum is also an outlier. Therefore, the lower whisker is drawn at the smallest value greater than 1.5 IQR below the first quartile, which is 57\u00b0F.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "Although box plots may seem more primitive than histograms or kernel density estimates, they do have a number of advantages. First, the box plot enables statisticians to do a quick graphical examination on one or more data sets. Box-plots also take up less space and are therefore particularly useful for comparing distributions between several groups or sets of data in parallel (see Figure 1 for an example). Lastly, the overall structure of histograms and kernel density estimate can be strongly influenced by the choice of number and width of bins techniques and the choice of bandwidth, respectively.\n", "link": "https://en.wikipedia.org/wiki/Box_plot"}, {"text": "A Pareto chart is a type of chart that contains both bars and a line graph, where individual values are represented in descending order by bars, and the cumulative total is represented by the line.  The chart is named for the Pareto principle, which, in turn, derives its name from Vilfredo Pareto, a noted Italian economist.\n", "link": "https://en.wikipedia.org/wiki/Pareto_chart"}, {"text": "The left vertical axis is the frequency of occurrence, but it can alternatively represent cost or another important unit of measure.  The right vertical axis is the cumulative percentage of the total number of occurrences, total cost, or total of the particular unit of measure. Because the values are in decreasing order, the cumulative function is a concave function. To take the example below, in order to lower the amount of late arrivals by 78%, it is sufficient to solve the first three issues.\n", "link": "https://en.wikipedia.org/wiki/Pareto_chart"}, {"text": "The purpose of the Pareto chart is to highlight the most important among a (typically large) set of factors. In quality control, Pareto charts are useful to find the defects to prioritize in order to observe the greatest overall improvement. It often represents the most common sources of defects, the highest occurring type of defect, or the most frequent reasons for customer complaints, and so on. Wilkinson (2006) \ndevised an algorithm for producing statistically based acceptance limits (similar to confidence intervals) for each bar in the Pareto chart.\n", "link": "https://en.wikipedia.org/wiki/Pareto_chart"}, {"text": "A pie chart (or a circle chart) is a circular statistical graphic which is divided into slices to illustrate numerical proportion. In a pie chart, the arc length of each slice (and consequently its central angle and area) is proportional to the quantity it represents. While it is named for its resemblance to a pie which has been sliced, there are variations on the way it can be presented. The earliest known pie chart is generally credited to William Playfair's Statistical Breviary of 1801.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "Pie charts are very widely used in the business world and the mass media. However, they have been criticized, and many experts recommend avoiding them, as research has shown it is difficult to compare different sections of a given pie chart, or to compare data across different pie charts. Pie charts can be replaced in most cases by other plots such as the bar chart, box plot, dot plot, etc.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "The earliest known pie chart is generally credited to William Playfair's Statistical Breviary of 1801, in which two such graphs are used. Playfair presented an illustration, which contained a series of pie charts. One of those charts depicted the proportions of the Turkish Empire located in Asia, Europe and Africa before 1789. This invention was not widely used at first.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "A 3D pie chart, or perspective pie chart, is used to give the chart a 3D look. Often used for aesthetic reasons, the third dimension does not improve the reading of the data; on the contrary, these plots are difficult to interpret because of the distorted effect of perspective associated with the third dimension. The use of superfluous dimensions not used to display the data of interest is discouraged for charts in general, not only for pie charts.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "A doughnut chart (also spelled donut) is a variant of the pie chart, with a blank center allowing for additional information about the data as a whole to be included.\n Doughnut charts are similar to pie charts in that their aim is to illustrate proportions. This type of circular graph can support multiple statistics at once and it provides a better data intensity ratio to standard pie charts. It does not have to contain information in the center.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "A ring chart, also known as a sunburst chart or a multilevel pie chart, is used to visualize hierarchical data, depicted by concentric circles. The circle in the center represents the root node, with the hierarchy moving outward from the center. A segment of the inner circle bears a hierarchical relationship to those segments of the outer circle which lie within the angular sweep of the parent segment.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "Square charts, also called waffle charts, are a form of pie charts that use squares instead of circles to represent percentages. Similar to basic circular pie charts, square pie charts take each percentage out of a total 100%. They are often 10 by 10 grids, where each cell represents 1%. Despite the name, circles, pictograms (such as of people), and other shapes may be used instead of squares. One major benefit to square charts is that smaller percentages, difficult to see on traditional pie charts, can be easily depicted.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "The following example chart is based on preliminary results of the election for the European Parliament in 2004. The table lists the number of seats allocated to each party group, along with the derived percentage of the total that they each make up. The values in the last column, the derived central angle of each sector, is found by taking that percentage of 360.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "The size of each central angle is proportional to the size of the corresponding quantity, here the number of seats. Since the sum of the central angles has to be 360\u00b0, the central angle for a quantity that is a fraction Q of the total is 360Q degrees.\nIn the example, the central angle for the largest group (European People's Party (EPP)) is 135.7\u00b0 because 0.377 times 360, rounded to one decimal place, equals 135.7.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "Statisticians generally regard pie charts as a poor method of displaying information, and they are uncommon in scientific literature. One reason is that it is more difficult for comparisons to be made between the size of items in a chart when area is used instead of length and when different items are shown as different shapes.\n", "link": "https://en.wikipedia.org/wiki/Pie_chart"}, {"text": "William Playfair is usually credited with inventing the area charts as well as the line, bar, and pie charts. His book The Commercial and Political Atlas, published in 1786, contained a number of time-series graphs, including Interest of the National Debt from the Revolution and Chart of all the Imports and Exports to and from England from the Year 1700 to 1782 that are often described as the first area charts in history.\n", "link": "https://en.wikipedia.org/wiki/Area_chart"}, {"text": "Area charts are used to represent cumulated totals using numbers or percentages (stacked area charts in this case) over time.\nUse the area chart for showing trends over time among related attributes. The area chart is like the plot chart except that the area below the plotted line is filled in with color to indicate volume.\n", "link": "https://en.wikipedia.org/wiki/Area_chart"}, {"text": "Treemaps display hierarchical (tree-structured) data as a set of nested rectangles. Each branch of the tree is given a rectangle, which is then tiled with smaller rectangles representing sub-branches. A leaf node's rectangle has an area proportional to a specified dimension of the data. Often the leaf nodes are colored to show a separate dimension of the data.\n", "link": "https://en.wikipedia.org/wiki/Treemapping"}, {"text": "When the color and size dimensions are correlated in some way with the tree structure, one can often easily see patterns that would be difficult to spot in other ways, such as whether a certain color is particularly relevant. A second advantage of treemaps is that, by construction, they make efficient use of space. As a result, they can legibly display thousands of items on the screen simultaneously.\n", "link": "https://en.wikipedia.org/wiki/Treemapping"}, {"text": "In convex treemaps, the aspect ratio cannot be constant - it grows with the depth of the tree. \nTo attain a constant aspect-ratio,  Orthoconvex treemaps can be used. There, all regions are orthoconvex rectilinear polygons with aspect ratio at most 64; and the leaves are either rectangles with aspect ratio at most 8, or L-shapes or S-shapes with aspect ratio at most 32.\n", "link": "https://en.wikipedia.org/wiki/Treemapping"}, {"text": "A third wave of treemap innovation came around 2004, after Marcos Weskamp created the Newsmap, a treemap that displayed news headlines. This example of a non-analytical treemap inspired many imitators, and introduced treemaps to a new, broad audience. In recent years, treemaps have made their way into the mainstream media, including usage by the New York Times. \nThe Treemap Art Project produced 12 framed images for the National Academies (United States), shown at the Every AlgoRiThm has ART in It exhibit in Washington, DC and another set for the collection of Museum of Modern Art in New York.\n", "link": "https://en.wikipedia.org/wiki/Treemapping"}, {"text": "A bubble chart is a type of chart that displays three dimensions of data. Each entity with its triplet (v1, v2, v3) of associated data is plotted as a disk that expresses two of the vi values through the disk's xy location and the third through its size. Bubble charts can facilitate the understanding of social, economical, medical, and other scientific relationships.\n", "link": "https://en.wikipedia.org/wiki/Bubble_chart"}, {"text": "Bubble charts can be considered a variation of the scatter plot, in which the data points are replaced with bubbles. As the documentation for Microsoft Office explains, \"You can use a bubble chart instead of a scatter chart if your data has three data series that each contain a set of values. The sizes of the bubbles are determined by the values in the third data series.\".\n", "link": "https://en.wikipedia.org/wiki/Bubble_chart"}, {"text": "Using bubbles to represent scalar (one-dimensional) values can be misleading. The human visual system most naturally experiences a disk's size in terms of its diameter, rather than area. This is why most charting software requests the radius or diameter of the bubble as the third data value (after horizontal and vertical axis data). Scaling the size of bubbles based on area can be misleading [ibid]. \n", "link": "https://en.wikipedia.org/wiki/Bubble_chart"}, {"text": "This scaling issue can lead to extreme misinterpretations, especially where the range of the data has a large spread. And because many people are unfamiliar with\u2014or do not stop to consider\u2014the issue and its impact on perception, those who are aware of it often have to hesitate in interpreting a bubble chart because they cannot assume that the scaling correction was indeed made. It is therefore important that bubble charts not only be scaled correctly, but also be clearly labeled to document that it is area, rather than radius or diameter, that conveys the data.\n", "link": "https://en.wikipedia.org/wiki/Bubble_chart"}, {"text": "Judgments based on bubble sizes can be problematic regardless of whether area or diameter is used. For example, bubble charts can lead to misinterpretations such as the weighted average illusion, where the sizes of bubbles are taken into account when estimating the mean x- and y-values of the scatterplot. The range of bubble sizes used is often arbitrary. For example, the maximum bubble size is often set to some fraction of the total width of the chart, and therefore will not equal the true measurement value.\n", "link": "https://en.wikipedia.org/wiki/Bubble_chart"}, {"text": "Additional information about the entities beyond their three primary values can often be incorporated by rendering their disks in colors and patterns that are chosen in a systematic way. And, of course, supplemental information can be added by annotating disks with textual information, sometimes as simple as unique identifying labels for cross-referencing to explanatory keys and the like.\n", "link": "https://en.wikipedia.org/wiki/Bubble_chart"}, {"text": "Warming stripes (sometimes referred to as climate stripes, climate timelines or stripe graphics) are data visualization graphics that use a series of coloured stripes chronologically ordered to visually portray long-term temperature trends. Warming stripes reflect a \"minimalist\" style, conceived to use colour alone to avoid technical distractions to intuitively convey global warming trends to non-scientists.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "\u00a0\u00a0\u00a0\u00a0\"I wanted to communicate temperature changes in a way that was simple and intuitive, removing all the distractions of standard climate graphics so that the long-term trends and variations in temperature are crystal clear. Our visual system will do the interpretation of the stripes without us even thinking about it.\"\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "In May 2016, to make visualizing climate change easier for the general public, University of Reading climate scientist Ed Hawkins created an animated spiral graphic of global temperature change as a function of time, a representation said to have gone viral. Jason Samenow wrote in The Washington Post that the spiral graph was \"the most compelling global warming visualization ever made\", before it was featured in the opening ceremony of the 2016 Summer Olympics.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "On 22 May 2018, Hawkins published graphics constituting a chronologically ordered series of blue and red vertical stripes that he called warming stripes. Hawkins, a lead author for the IPCC 6th Assessment Report, received the Royal Society's 2018 Kavli Medal, in part \"for actively communicating climate science and its various implications with broad audiences\".\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "As described in a BBC article, in the month the big meteorological agencies release their annual climate assessments, Hawkins experimented with different ways of rendering the global data and \"chanced upon the coloured stripes idea\". When he tried out a banner at the Hay Festival, according to the article, Hawkins \"knew he'd struck a chord\".  The National Centre for Atmospheric Science (UK), with which Hawkins is affiliated, states that the stripes \"paint a picture of our changing climate in a compelling way. Hawkins swapped out numerical data points for colours which we intuitively react to\".\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Warming stripe graphics are reminiscent of colour field painting, a style prominent in the mid 20th century, which strips out all distractions and uses only colour to convey meaning. Colour field pioneer artist Barnett Newman said he was \"creating images whose reality is self-evident\", an ethos that Hawkins is said to have applied to the problem of climate change.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Collaborating with Berkeley Earth scientist Robert Rohde, on 17 June 2019 Hawkins published for public use, a large set of warming stripes on ShowYourStripes.info. Individualized warming stripe graphics were published for the globe, for most countries, as well as for certain smaller regions such as states in the US or parts of the UK, since different parts of the world are warming more quickly than others.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Hawkins' original graphics use the eight most saturated blues and reds from the ColorBrewer 9-class single hue palettes, which optimize colour palettes for maps and are noted for their colourblind-friendliness. Hawkins said the specific colour choice was an aesthetic decision (\"I think they look just right\"), also selecting baseline periods to ensure equally dark shades of blue and red for aesthetic balance. Hawkins chose the 1971-2000 average as a boundary between reds and blues because the average global temperature in that reference period represented the mid-point in the warming to date.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "A Republik analysis said that \"this graphic explains everything in the blink of an eye\", attributing its effect mainly to the chosen colors, which \"have a magical effect on our brain, (letting) us recognize connections before we have even actively thought about them\". The analysis concluded that colors other than blue and red \"don't convey the same urgency as (Hawkins') original graphic, in which the colors were used in the classic way: blue=cold, red=warm.\"\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "For each country-level #ShowYourStripes graphic (Hawkins, June 2019), the average temperature in the 1971\u20132000 reference period is set as the boundary between blue (cooler) and red (warmer) colours, the colour scale varying +/- 2.6 standard deviations of the annual average temperatures between 1901 and 2000. Hawkins noted in 2019 that the graphic for the Arctic \"broke the colour scale\" since it is warming more than twice as fast as the global average, and reported that the 2023 global average was so extreme that a new, darker shade of red was required.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "After Hawkins' first publication of warming stripe graphics in May 2018, broadcast meteorologists in multiple countries began to show stripe-decorated neckties, necklaces, pins and coffee mugs on-air, reflecting a growing acceptance of climate science among meteorologists and a willingness to communicate it to audiences. In 2019, the United States House Select Committee on the Climate Crisis used warming stripes in its committee logo, showing horizontally oriented stripes behind a silhouette of the United States Capitol, and three US Senators wore warming stripe lapel pins at the 2020 State of the Union Address.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Through a campaign led by nonprofit Climate Central using hashtag #MetsUnite, more than 100 TV meteorologists\u2014the scientists most laymen interact with more than any other\u2014featured warming stripes and used the graphics to focus audience attention during broadcasts on summer solstices beginning in 2018 with the \"Stripes for the Solstice\" effort.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "In March 2019, German engineer Alexander Radtke extended Hawkins' historical graphics to show predictions of future warming through the year 2200, a graphic that one commentator described as making the future \"a lot more visceral\". Radtke bifurcated the graphic to show diverging predictions for different degrees of human action in reducing greenhouse gas emissions.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "In 2023, University of Derby professor Miles Richardson created sequenced stripes to illustrate biodiversity loss, and the German Meteorological Service represented soil moisture deviations using sequenced green and brown stripes. In August 2024, the website airqualitystripes.info published shareable \"air quality stripes\" graphics for world cities, using blue, yellow, orange, red and black stripes to represent fine particulate matter (PM2.5) concentrations over time.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Some warned that warming stripes of individual countries or states, taken out of context, could advance the idea that global temperatures are not rising, though research meteorologist J. Marshall Shepherd said that \"geographic variations in the graphics offer an outstanding science communication opportunity\". Meteorologist and #MetsUnite coordinator Jeff Berardelli said that \"local stripe visuals help us tell a nuanced story\u2014the climate is not changing uniformly everywhere\".\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Others say the charts should include axes or legends, though the website FAQ page explains the graphics were \"specifically designed to be as simple as possible, and to start conversations... (to) fill a gap and enable communication with minimal scientific knowledge required to understand their meaning\". J. Marshall Shepherd, former president of the American Meteorological Society, lauded Hawkins' approach, writing that \"it is important not to miss the bigger picture. Science communication to the public has to be different\" and commending Hawkins for his \"innovative\" approach and \"outstanding science communication\" effort.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "In The Washington Post, Matthew Cappucci wrote that the \"simple graphics ... leave a striking visual impression\" and are \"an easily accessible way to convey an alarming trend\", adding that \"warming tendencies are plain as day\". Greenpeace spokesman Graham Thompson remarked that the graphics are \"like a really well-designed logo while still being an accurate representation of very important data\".\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "A September 2019 editorial in The Economist hypothesized that \"to represent this span of human history (1850\u20132018) as a set of simple stripes may seem reductive\"\u2014noting those years \"saw world wars, technological innovation, trade on an unprecedented scale and a staggering creation of wealth\"\u2014but concluded that \"those complex histories and the simplifying stripes share a common cause,\" namely, fossil fuel combustion.\n", "link": "https://en.wikipedia.org/wiki/Warming_stripes"}, {"text": "Control charts are graphical plots used in production control to determine whether quality and manufacturing processes are being controlled under stable conditions. (ISO 7870-1) \nThe hourly status is arranged on the graph, and the occurrence of abnormalities is judged based on the presence of data that differs from the conventional trend or deviates from the control limit line.\nControl charts are classified into Shewhart individuals control chart (ISO 7870-2) and CUSUM(CUsUM)(or cumulative sum control chart)(ISO 7870-4).\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "The control chart is one of the seven basic tools of quality control. Typically control charts are used for time-series data, also known as continuous data or variable data. Although they can also be used for data that has logical comparability (i.e. you want to compare samples that were taken all at the same time, or the performance of different individuals); however the type of chart used to do this requires consideration.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Bonnie Small worked in an Allentown plant in the 1950s after the transistor was made. Used Shewhart's methods to improve plant performance in quality control and made up to 5000 control charts. In 1958, The Western Electric Statistical Quality Control Handbook had appeared from her writings and led to use at AT&T.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "If the process is in control (and the process statistic is normal), 99.7300% of all the points will fall between the control limits. Any observations outside the limits, or systematic patterns within, suggest the introduction of a new (and likely unanticipated) source of variation, known as a special-cause variation. Since increased variation means increased quality costs, a control chart \"signaling\" the presence of a special-cause requires immediate investigation.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "This makes the control limits very important decision aids. The control limits provide information about the process behavior and have no intrinsic relationship to any specification targets or engineering tolerance. In practice, the process mean (and hence the centre line) may not coincide with the specified value (or target) of the quality characteristic because the process design simply cannot deliver the process characteristic at the desired level.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Control charts limit specification limits or targets because of the tendency of those involved with the process (e.g., machine operators) to focus on performing to specification when in fact the least-cost course of action is to keep process variation as low as possible. Attempting to make a process whose natural centre is not the same as the target perform to target specification increases process variability and increases costs significantly and is the cause of much inefficiency in operations. Process capability studies do examine the relationship between the natural process limits (the control limits) and specifications, however.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "The purpose of control charts is to allow simple detection of events that are indicative of an increase in process variability. This simple decision can be difficult where the process characteristic is continuously varying; the control chart provides statistically objective criteria of change. When change is detected and considered good its cause should be identified and possibly become the new way of working, where the change is bad then its cause should be identified and eliminated.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Some of the earliest attempts to characterize a state of statistical control were inspired by the belief that there existed a special form of frequency function f and it was early argued that the normal law characterized such a state. When the normal law was found to be inadequate, then generalized functional forms were tried. Today, however, all hopes of finding a unique functional form f are blasted.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "The control chart is intended as a heuristic. Deming insisted that it is not a hypothesis test and is not motivated by the Neyman\u2013Pearson lemma. He contended that the disjoint nature of population and sampling frame in most industrial situations compromised the use of conventional statistical techniques. Deming's intention was to seek insights into the cause system of a process ...under a wide range of unknowable circumstances, future and past.... He claimed that, under such conditions, 3-sigma limits provided ...\u00a0a rational and economic guide to minimum economic loss... from the two errors:\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "In 1935, the British Standards Institution, under the influence of Egon Pearson and against Shewhart's spirit, adopted control charts, replacing 3-sigma limits with limits based on percentiles of the normal distribution. This move continues to be represented by John Oakland and others but has been widely deprecated by writers in the Shewhart\u2013Deming tradition.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "When a point falls outside the limits established for a given control chart, those responsible for the underlying process are expected to determine whether a special cause has occurred. If one has, it is appropriate to determine if the results with the special cause are better than or worse than results from common causes alone. If worse, then that cause should be eliminated if possible. If better, it may be appropriate to intentionally retain the special cause within the system producing the results.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Even when a process is in control (that is, no special causes are present in the system), there is approximately a 0.27% probability of a point exceeding 3-sigma control limits. So, even an in control process plotted on a properly constructed control chart will eventually signal the possible presence of a special cause, even though one may not have actually occurred. For a Shewhart control chart using 3-sigma limits, this false alarm occurs on average once every 1/0.0027 or 370.4 observations. Therefore, the in-control average run length (or in-control ARL) of a Shewhart chart is 370.4.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Meanwhile, if a special cause does occur, it may not be of sufficient magnitude for the chart to produce an immediate alarm condition. If a special cause occurs, one can describe that cause by measuring the change in the mean and/or variance of the process in question. When those changes are quantified, it is possible to determine the out-of-control ARL for the chart.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "It turns out that Shewhart charts are quite good at detecting large changes in the process mean or variance, as their out-of-control ARLs are fairly short in these cases. However, for smaller changes (such as a 1- or 2-sigma change in the mean), the Shewhart chart does not detect these changes efficiently. Other types of control charts have been developed, such as the EWMA chart, the CUSUM chart and the real-time contrasts chart, which detect smaller changes more efficiently by making use of information from observations collected prior to the most recent data point.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Several authors have criticised the control chart on the grounds that it violates the likelihood principle. However, the principle is itself controversial and supporters of control charts further argue that, in general, it is impossible to specify a likelihood function for a process not in statistical control, especially where knowledge about the cause system of the process is weak.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Critics of this approach argue that control charts should not be used when their underlying assumptions are violated, such as when process data is neither normally distributed nor binomially (or Poisson) distributed. Such processes are not in control and should be improved before the application of control charts. Additionally, application of the charts in the presence of such deviations increases the type I and type II error rates of the control charts, and may make the chart of little practical use.\n", "link": "https://en.wikipedia.org/wiki/Control_chart"}, {"text": "Examples could include measurements of the fill level of bottles filled at a bottling plant or the water temperature of a dishwashing machine each time it is run.  Time is generally represented on the horizontal (x) axis and the property under observation on the vertical (y) axis.  Often, some measure of central tendency (mean or median) of the data is indicated by a horizontal reference line.\n", "link": "https://en.wikipedia.org/wiki/Run_chart"}, {"text": "Run charts are analyzed to find anomalies in data that suggest shifts in a process over time or special factors that may be influencing the variability of a process.  Typical factors considered include unusually long \"runs\" of data points above or below the average line, the total number of such runs in the data set, and unusually long series of consecutive increases or decreases.\n", "link": "https://en.wikipedia.org/wiki/Run_chart"}, {"text": "A stem-and-leaf plot is also called a stemplot, but the latter term often refers to another chart type.  A simple stem plot may refer to plotting a matrix of y values onto a common x axis, and identifying the common x value with a vertical line, and the individual y values with symbols on the line.\n", "link": "https://en.wikipedia.org/wiki/Stem-and-leaf_display"}, {"text": "To construct a stem-and-leaf display, the observations must first be sorted in ascending order: this can be done most easily if working by hand by constructing a draft of the stem-and-leaf display with the leaves unsorted, then sorting the leaves to produce the final stem-and-leaf display. Here is the sorted set of data values that will be used in the following example:\n", "link": "https://en.wikipedia.org/wiki/Stem-and-leaf_display"}, {"text": "Next, it must be determined what the stems will represent and what the leaves will represent. Typically, the leaf contains the last digit of the number and the stem contains all of the other digits. In the case of very large numbers, the data values may be rounded to a particular place value (such as the hundreds place) that will be used for the leaves. The remaining digits to the left of the rounded place value are used as the stem.\n", "link": "https://en.wikipedia.org/wiki/Stem-and-leaf_display"}, {"text": "The stem-and-leaf display is drawn with two columns separated by a vertical line. The stems are listed to the left of the vertical line. It is important that each stem is listed only once and that no numbers are skipped, even if it means that some stems have no leaves. The leaves are listed in increasing order in a row to the right of each stem.\n", "link": "https://en.wikipedia.org/wiki/Stem-and-leaf_display"}, {"text": "Stem-and-leaf displays can also be used to convey non-numerical information. In this example of valid two-letter words in Collins Scrabble Words (the word list used in Scrabble tournaments outside the US) with their initials as stems, it can be easily seen that the three most common initials are o, a and e.\n", "link": "https://en.wikipedia.org/wiki/Stem-and-leaf_display"}, {"text": "The continuous cartogram emerged soon after in the United States, where a variety appeared in the popular media after 1911. Most were rather crudely drawn compared to Haack and Weichel, with the exception of the \"rectangular statistical cartograms\" by the American master cartographer Erwin Raisz, who claimed to have invented the technique.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "When Haack and Weichel referred to their map as a kartogramm, this term was commonly being used to refer to all thematic maps, especially in Europe. It was not until Raisz and other academic cartographers stated their preference for a restricted use of the term in their textbooks (Raisz initially espousing value-area cartogram) that the current meaning was gradually adopted.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "The primary challenge of cartograms has always been the drafting of the distorted shapes, making them a prime target for computer automation. Waldo R. Tobler developed one of the first algorithms in 1963, based on a strategy of warping space itself rather than the distinct districts. Since then, a wide variety of algorithms have been developed (see below), although it is still common to craft cartograms manually.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "This is perhaps the simplest method for constructing a cartogram, in which each district is simply reduced or enlarged in size according to the variable without altering its shape at all. In most cases, a second step adjusts the location of each shape to reduce gaps and overlaps between the shapes, but their boundaries are not actually adjacent. While the preservation of shape is a prime advantage of this approach, the results often have a haphazard appearance because the individual districts do not fit together well.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "In this approach, each district is replaced with a simple geometric shape of proportional size. Thus, the original shape is completely eliminated, and contiguity may be retained in a limited form or not at all. Although they are usually referred to as Dorling cartograms after Daniel Dorling's 1996 algorithm first facilitated their construction, these are actually the original form of cartogram, dating back to Levasseur (1876) and Raisz (1934). Several options are available for the geometric shapes:\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "Because the districts are not at all recognizable, this approach is most useful and popular for situations in which the shapes would not be familiar to map readers anyway (e.g., U.K. parliamentary constituencies) or where the districts are so familiar to map readers that their general distribution is sufficient information to recognize them (e.g., countries of the world). Typically, this method is used when it is more important for readers to ascertain the overall geographic pattern than to identify particular districts; if identification is needed, the individual geometric shapes are often labeled.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "This method works best with variables that are already measured as a relatively low-valued integer, enabling a one-to-one match with the cells. This has made them very popular for visualizing the United States Electoral College that determines the election of the president, appearing on television coverage and numerous vote-tracking websites. Several examples of block cartograms were published during the 2016 U.S. presidential election season by The Washington Post, the FiveThirtyEight blog, and the Wall Street Journal, among others.  This is a cartogram for the 2024 and 2028 elections, based on the 2020 Census apportionment:\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "While an area cartogram manipulates the area of a polygon feature, a linear cartogram manipulates linear distance on a line feature. The spatial distortion allows the map reader to easily visualize intangible concepts such as travel time and connectivity on a network. Distance cartograms are also useful for comparing such concepts among different geographic features. A distance cartogram may also be called a central-point cartogram.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "A common use of distance cartograms is to show the relative travel times and directions from vertices in a network. For example, on a distance cartogram showing travel time between cities, the less time required to get from one city to another, the shorter the distance on the cartogram will be. When it takes a longer time to travel between two cities, they will be shown as further apart in the cartogram, even if they are physically close together.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "Distance cartograms are also used to show connectivity. This is common on subway and metro maps, where stations and stops are shown as being the same distance apart on the map even though the true distance varies.  Though the exact time and distance from one location to another is distorted, these cartograms are still useful for travel and analysis.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "Another option for diagrammatic cartograms is to subdivide the shapes as charts (commonly a pie chart), in the same fashion often done with proportional symbol maps. This can be very effective for showing complex variables such as population composition, but can be overwhelming if there are a large number of symbols or if the individual symbols are very small.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "One of the first cartographers to generate cartograms with the aid of computer visualization was Waldo Tobler of UC Santa Barbara in the 1960s. Prior to Tobler's work, cartograms were created by hand (as they occasionally still are). The National Center for Geographic Information and Analysis located on the UCSB campus maintains an online Cartogram Central Archived 2016-10-05 at the Wayback Machine with resources regarding cartograms.\n", "link": "https://en.wikipedia.org/wiki/Cartogram"}, {"text": "A small multiple (sometimes called trellis chart, lattice chart, grid chart, or panel chart) is a series of similar graphs or charts using the same scale and axes, allowing them to be easily compared. It uses multiple views to show different partitions of a dataset. The term was popularized by Edward Tufte.\n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "At the heart of quantitative reasoning is a single question: Compared to what? Small multiple designs, multivariate and data bountiful, answer directly by visually enforcing comparisons of changes, of the differences among objects, of the scope of alternatives. For a wide range of problems in data presentation, small multiples are the best design solution.", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "In the example, the departmental salary expense is charted by month with a dashed line indicating the average for each department.  The scales on each panel are different to emphasize the relative change over time compared to the range. Standardizing the scales could provide insight into comparisons in magnitude between the different departments. Two independent Y axes may be utilized when presenting data with different numeric scales in each panel.\n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "Muybridge's work not only proved for the first time that all four of a horse's hooves left the ground during gallop (see upper central plates), but it also broke new ground in terms of artistic expression and became foundational to the development of the motion picture.  Muybridge went on to produce many more examples of small multiples showing animal locomotion through the medium of stop-motion photography, including boys playing leapfrog and a bison cantering.\n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "Sketched graphic examples can be found in Francis Amasa Walker's charts appearing in the 1870 Statistical Atlas of the United States.  Superintendent of the US Census at the time of its creation, Walker was determined to modernize the Census collection and analysis methods and used the Atlas to present the final data set using unprecedented visual forms, including many beautiful examples of small multiples.\n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "Adjacent is a chart showing the population broken down by occupation, including a count of those attending school, according to the 1870 Census.  This graphic is innovative in its use of both a treemap display and a latticed layout of small multiples.  Additional examples appearing in the Atlas include side-by-side geographic maps showing the changes in population over time, as well as tiled mosaic charts showing population demographic breakdowns, and diverging bar graphs showing deaths broken down by age and gender, tiled by state.\n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "Small multiples are a popular technique in cartographic design for  multivariate mapping. As with the small multiple chart, each panel uses the same underlying two-dimensional space, but in this case that is a geographic space. Typically, the variables being mapped are of a similar type, such as types of agricultural products, so that the same strategy of map symbol can be used on each panel, enabling rapid comparison between the maps. \n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "Another common use of small multiples is to show change in spatial patterns over time, as an alternative to an animated map. Several tests of the effectiveness of each method have generally concluded that they have distinct advantages, with animation being better for seeing trends, especially movement, and small multiples being better for making comparisons between times.\n", "link": "https://en.wikipedia.org/wiki/Small_multiple"}, {"text": "A sparkline is a very small line chart, typically drawn without axes or coordinates. It presents the general shape of a variation (typically over time) in some measurement, such as temperature or stock market price, in a simple and highly condensed way. Whereas a typical chart is designed to professionally show as much data as possible, and is set off from the flow of text, sparklines are intended to be succinct, memorable, and located where they are discussed. Sparklines are small enough to be embedded in text, or several sparklines may be grouped together as elements of a small multiple.  \n", "link": "https://en.wikipedia.org/wiki/Sparkline"}, {"text": "On May 7, 2008, Microsoft employees filed a patent application for the implementation of sparklines in Microsoft Excel 2010. The application was published on November 12, 2009, prompting Tufte to express concerns about patent breadth and non-novelty. On 23 January, 2009, MultiRacio Ltd. published an OpenOffice.org Calc extension named \"EuroOffice Sparkline\". On March 3, 2022, Toma\u017e Vajngerl implemented sparklines in LibreOffice Calc version 7.4, including support for importing sparklines from the OOXML Workbook format.\n", "link": "https://en.wikipedia.org/wiki/Sparkline"}, {"text": "In multi-dimensional tables, each cell in the body of the table (and the value of that cell) relates to the values at the beginnings of the column (i.e. the header), the row, and other structures in more complex tables. This is an injective relation: each combination of the values of the headers row (row 0, for lack of a better term) and the headers column (column 0 for lack of a better term) is related to a unique cell in\nthe table:\n", "link": "https://en.wikipedia.org/wiki/Table_(information)"}, {"text": "The first column often presents information dimension description by which the rest of the table is navigated. This column is called \"stub column\". Tables may contain three or multiple dimensions and can be classified by the number of dimensions. Multi-dimensional tables may have super-rows - rows that describe additional dimensions for the rows that are presented below that row and are usually grouped in a tree-like structure.  This structure is typically visually presented with an appropriate number of white spaces in front of each stub's label.\n", "link": "https://en.wikipedia.org/wiki/Table_(information)"}, {"text": "For example, in the following diagram, two alternate representations of the same information are presented side by side. On the left is the NFPA 704 standard \"fire diamond\" with example values indicated and on the right is a simple table displaying the same values, along with additional information. Both representations convey essentially the same information, but the tabular representation is arguably more comprehensible to someone who is not familiar with the NFPA 704 standard. The tabular representation may not, however, be ideal for every circumstance (for example because of space limitations, or safety reasons).\n", "link": "https://en.wikipedia.org/wiki/Table_(information)"}, {"text": "Tables have uses in software development for both high-level specification and low-level implementation.\nUsage in software specification can encompass ad hoc inclusion of simple decision tables in textual documents through to the use of tabular specification methodologies, examples of which include Software Cost Reduction and Statestep.\nProponents of tabular techniques, among whom David Parnas is prominent, emphasize their understandability, as well as the quality and cost advantages of a format allowing systematic inspection, while corresponding shortcomings experienced with a graphical notation were cited in motivating the development of at least two tabular approaches.\n", "link": "https://en.wikipedia.org/wiki/Table_(information)"}, {"text": "The categorical variables are first put in order. Then, each variable is assigned to an axis. In the table to the right, sequence and classification is presented for this data set. Another ordering will result in a different mosaic plot, i.e., the order of the variables is significant as for all multivariate plots.\n", "link": "https://en.wikipedia.org/wiki/Mosaic_plot"}, {"text": "At the left edge of the first variable we first plot \"Gender,\" meaning that we divide the data vertically in two blocks: the bottom blocks corresponds to females, while the upper (much larger) one to males. One immediately sees that roughly a quarter of the passengers were female and the remaining three quarters male.\n", "link": "https://en.wikipedia.org/wiki/Mosaic_plot"}, {"text": "One then applies the second variable \"Class\" to the top edge. The four vertical columns therefore mark the four values of that variable (1st, 2nd, 3rd, and crew). These columns are of variable thickness, because column width indicates the relative proportion of the corresponding value on the population. Crew plainly represents the largest male group, whereas third-class passengers are the largest female group. The number of female crew members is also seen to have been marginal.\n", "link": "https://en.wikipedia.org/wiki/Mosaic_plot"}, {"text": "The last variable (\"Survived\") is finally applied, this time along the left edge with the result highlighted by shade: dark grey rectangles represent people that did not survive the disaster, light grey ones people that did. Women in the first class are immediately seen to have had the highest survival probability. The survival probability for females is seen to have been higher than that for men (marginalised over all classes). Similarly, a marginalization over gender identifies first-class passengers as most probable to survive. Overall, about 1/3 of all people survived (proportion of light gray areas).\n", "link": "https://en.wikipedia.org/wiki/Mosaic_plot"}, {"text": "Data are collected using techniques such as measurement, observation, query, or analysis, and are typically represented as numbers or characters that may be further processed. Field data are data that are collected in an uncontrolled, in-situ environment. Experimental data are data that are generated in the course of a controlled scientific experiment.  Data are analyzed using techniques such as calculation, reasoning, discussion, presentation, visualization, or other forms of post-analysis. Prior to analysis, raw data (or unprocessed data) is typically cleaned: Outliers are removed, and obvious instrument or data entry errors are corrected.\n", "link": "https://en.wikipedia.org/wiki/Data"}, {"text": "Advances in computing technologies have led to the advent of big data, which usually refers to very large quantities of data, usually at the petabyte scale. Using traditional data analysis methods and computing, working with such large (and growing) datasets is difficult, even impossible. (Theoretically speaking, infinite data would yield infinite information, which would render extracting insights or intelligence impossible.) In response, the relatively new field of data science uses machine learning (and other artificial intelligence) methods that allow for efficient applications of analytic methods to big data.\n", "link": "https://en.wikipedia.org/wiki/Data"}, {"text": "The Latin word data is the plural of datum, \"(thing) given,\" and the neuter past participle of dare, \"to give\".\nThe first English use of the word \"data\" is from the 1640s. The word \"data\" was first used to mean \"transmissible and storable computer information\" in 1946. The expression \"data processing\" was first used in 1954.\n", "link": "https://en.wikipedia.org/wiki/Data"}, {"text": "Data, information, knowledge, and wisdom are closely related concepts, but each has its role concerning the other, and each term has its meaning. According to a common view, data is collected and analyzed; data only becomes information suitable for making decisions once it has been analyzed in some fashion. One can say that the extent to which a set of data is informative to someone depends on the extent to which it is unexpected by that person. The amount of information contained in a data stream may be characterized by its Shannon entropy.\n", "link": "https://en.wikipedia.org/wiki/Data"}, {"text": "Knowledge is the awareness of its environment that some entity possesses, whereas data merely communicates that knowledge. For example, the entry in a database specifying the height of Mount Everest is a datum that communicates a precisely-measured value. This measurement may be included in a book along with other data on Mount Everest to describe the mountain in a manner useful for those who wish to decide on the best method to climb it. Awareness of the characteristics represented by this data is knowledge.\n", "link": "https://en.wikipedia.org/wiki/Data"}, {"text": "The information available through a collection of data may be derived by analysis.  For example, a restaurant collects data from every customer order. That information may be analyzed to produce knowledge that is put to use when the business subsequently wants to identify the most popular or least popular dish.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Information can be transmitted in time, via data storage, and space, via communication and telecommunication. Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, all information is always conveyed as the content of a message.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Information theory is the scientific study of the quantification, storage, and communication of information. The field itself was fundamentally established by the work of Claude Shannon in the 1940s, with earlier contributions by Harry Nyquist and Ralph Hartley in the 1920s. The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory, and information-theoretic security.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference, cryptography, neurobiology, perception, linguistics, the evolution and function of molecular codes (bioinformatics), thermal physics, quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection, pattern recognition, anomaly detection and even art creation.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Systems theory at times seems to refer to information in this sense, assuming information does not necessarily involve any conscious mind, and patterns circulating (due to feedback) in the system can be called information. In other words, it can be said that information in this sense is something potentially perceived as representation, though not created or presented for that purpose. For example, Gregory Bateson defines \"information\" as a \"difference that makes a difference\".\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "In a biological framework, Mizraji  has described information as an entity emerging from the interaction of patterns with receptor systems (eg: in molecular or neural receptors capable of interacting with specific patterns, information emerges from those interactions). In addition, he has incorporated the idea of \"information catalysts\", structures where emerging information promotes the transition from pattern recognition to goal-directed action (for example, the specific transformation of a substrate into a product by an enzyme, or auditory reception of words and the production of an oral response)\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "It is estimated that the world's technological capacity to store information grew from 2.6 (optimally compressed) exabytes in 1986 \u2013 which is the informational equivalent to less than one 730-MB CD-ROM per person (539 MB per person) \u2013 to 295 (optimally compressed) exabytes in 2007. This is the informational equivalent of almost 61 CD-ROM per person in 2007.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Records are specialized forms of information. Essentially, records are information produced consciously or as by-products of business activities or transactions and retained because of their value. Primarily, their value is as evidence of the activities of the organization but they may also be retained for their informational value. Sound records management ensures that the integrity of records is preserved for as long as they are required.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "The international standard on records management, ISO 15489, defines records as \"information created, received, and maintained as evidence and information by an organization or person, in pursuance of legal obligations or in the transaction of business\". The International Committee on Archives (ICA) Committee on electronic records defined a record as, \"recorded information produced or received in the initiation, conduct or completion of an institutional or individual activity and that comprises content, context and structure sufficient to provide evidence of the activity\".\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Records may be maintained to retain corporate memory of the organization or to meet legal, fiscal or accountability requirements imposed on the organization. Willis expressed the view that sound management of business records and information delivered \"...six key requirements for good corporate governance...transparency; accountability; due process; compliance; meeting statutory and common law requirements; and security of personal and corporate information.\"\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Beynon-Davies explains the multi-faceted concept of information in terms of signs and signal-sign systems. Signs themselves can be considered in terms of four inter-dependent levels, layers or branches of semiotics: pragmatics, semantics, syntax, and empirics. These four layers serve to connect the social world on the one hand with the physical or technical world on the other.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Semantics is concerned with the meaning of a message conveyed in a communicative act. Semantics considers the content of communication. Semantics is the study of the meaning of signs \u2013 the association between signs and behaviour. Semantics can be considered as the study of the link between symbols and their referents or concepts \u2013 particularly the way that signs relate to human behavior.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "The existence of information about a closed system is a major concept in both classical physics and quantum mechanics, encompassing the ability, real or theoretical, of an agent to predict the future state of a system based on knowledge gathered during its past and present. Determinism is a philosophical theory holding that causal determination can predict all future events, positing a fully predictable universe described by classical physicist Pierre-Simon Laplace as \"the effect of its past and the cause of its future\".\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Quantum physics instead encodes information as a wave function, which prevents observers from directly identifying all of its possible measurements. Prior to the publication of Bell's theorem, determinists reconciled with this behavior using hidden variable theories, which argued that the information necessary to predict the future of a function must exist, even if it is not accessible for humans; A view surmised by Albert Einstein with the assertion that \"God does not play dice\".\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Modern astronomy cites the mechanical sense of information in the black hole information paradox, positing that, because the complete evaporation of a black hole into Hawking radiation leaves nothing except an expanding cloud of homogeneous particles, this results in the irrecoverability of any information about the matter to have originally crossed the event horizon, violating both classical and quantum assertions against the ability to destroy information.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "The information cycle (addressed as a whole or in its distinct components) is of great concern to information technology, information systems, as well as information science. These fields deal with those processes and techniques pertaining to information capture (through sensors) and generation (through computation, formulation or composition), processing (including encoding, encryption, compression, packaging), transmission (including all telecommunication methods), presentation (including visualization / display methods), storage (such as magnetic or optical, including holographic methods), etc.\n", "link": "https://en.wikipedia.org/wiki/Information"}, {"text": "Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "\"Volume\", \"variety\", \"velocity\", and various other \"Vs\" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the \"three Vs\", \"four Vs\", and \"five Vs\". They represented the qualities of big data in volume, variety, velocity, veracity, and value. Variability is often included as an additional quality of big data.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on the intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data. Since then, Teradata has added semi structured data types including XML, JSON, and Avro.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled \"Big Data Solution Offering\". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Multidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.\nAdditional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures\u2014storage area network (SAN) and network-attached storage (NAS)\u2014 is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good\u2014data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15\u00a0billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100\u00a0billion and was growing at almost 10\u00a0percent a year, about twice as fast as the software business as a whole.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "The use and adoption of big data within governmental processes allows efficiencies in terms of cost, productivity, and innovation, but comes with flaws. Data analysis often requires multiple parts of government (central and local) to work in collaboration and create new and innovative processes to deliver the desired outcome. A common government organization that makes use of big data is the National Security Administration (NSA), which monitors the activities of the Internet constantly in search for potential patterns of suspicious or illegal activities their system may pick up.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "A major practical application of big data for development has been \"fighting poverty with data\". In 2015, Blumenstock and colleagues estimated predicted poverty and wealth from mobile phone metadata and in 2016 Jean and colleagues combined satellite imagery and machine learning to predict poverty. Using digital trace data to study the labor market and the digital economy in Latin America, Hilbert and colleagues  argue that digital trace data has several benefits such as:\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "A related application sub-area, that heavily relies on big data, within the healthcare field is that of computer-aided diagnosis in medicine.  For instance, for epilepsy monitoring it is customary to create 5 to 10 GB of data daily. Similarly, a single uncompressed image of breast tomosynthesis averages 450 MB of data.  These are just a few of the many examples where computer-aided diagnosis uses big data. For this reason, big data has been recognized as one of the seven key challenges that computer-aided diagnosis systems need to overcome in order to reach the next level of performance.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "A McKinsey Global Institute study found a shortage of 1.5 million highly trained data professionals and managers and a number of universities including University of Tennessee and UC Berkeley, have created masters programs to meet this demand. Private boot camps have also developed programs to meet that demand, including paid programs like The Data Incubator or General Assembly. In the specific field of marketing, one of the problems stressed by Wedel and Kannan is that marketing has several sub domains (e.g., advertising, promotions,\nproduct development, branding) that all use different types of data.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Health insurance providers are collecting data on social \"determinants of health\" such as food and TV consumption, marital status, clothing size, and purchasing habits, from which they make predictions on health costs, in order to spot health issues in their clients. It is controversial whether these predictions are currently being used for pricing.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Big data and the IoT work in conjunction. Data extracted from IoT devices provides a mapping of device inter-connectivity. Such mappings have been used by the media industry, companies, and governments to more accurately target their audience and increase media efficiency. The IoT is also increasingly adopted as a means of gathering sensory data, and this sensory data has been used in medical, manufacturing and transportation contexts.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Kevin Ashton, the digital innovation expert who is credited with coining the term, defines the Internet of things in this quote: \"If we had computers that knew everything there was to know about things\u2014using data they gathered without any help from us\u2014we would be able to track and count everything, and greatly reduce waste, loss, and cost. We would know when things needed replacing, repairing, or recalling, and whether they were fresh or past their best.\"\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Big data can be used to improve training and understanding competitors, using sport sensors. It is also possible to predict winners in a match using big data analytics.\nFuture performance of players could be predicted as well. Thus, players' value and salary is determined by data collected throughout the season.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "In Formula One races, race cars with hundreds of sensors generate terabytes of data. These sensors collect data points from tire pressure to fuel burn efficiency.\nBased on the data, engineers and data analysts decide whether adjustments should be made in order to win a race. Besides, using big data, race teams try to predict the time they will finish the race beforehand, based on simulations using data collected over the season.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "The initiative included a National Science Foundation \"Expeditions in Computing\" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Privacy advocates are concerned about the threat to privacy represented by increasing storage and integration of personally identifiable information; expert panels have released various policy recommendations to conform practice to expectations of privacy. The misuse of big data in several cases by media, companies, and even the government has allowed for abolition of trust in almost every fundamental institution holding up society.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Large data sets have been analyzed by computing machines for well over a century, including the US census analytics performed by IBM's punch-card machines which computed statistics including means and variances of populations across the whole continent. In more recent decades, science experiments such as CERN have produced data on similar scales to current commercial \"big data\". However, science experiments have tended to analyze their data using specialized custom-built high-performance computing (super-computing) clusters and grids, rather than clouds of cheap commodity computers as in the current commercial wave, implying a difference in both culture and technology stack.\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Big data has been used in policing and surveillance by institutions like law enforcement and corporations. Due to the less visible nature of data-based surveillance as compared to traditional methods of policing, objections to big data policing are less likely to arise. According to Sarah Brayne's Big Data Surveillance: The Case of Policing, big data policing can reproduce existing societal inequalities in three ways:\n", "link": "https://en.wikipedia.org/wiki/Big_data"}, {"text": "Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Formally, a \"database\" refers to a set of related data accessed through the use of a \"database management system\" (DBMS), which is an integrated set of computer software that allows users to interact with one or more databases and provides access to all of the data contained in the database (although restrictions may exist that limit access to particular data). The DBMS provides various functions that allow entry, storage and retrieval of large quantities of information and provides ways to manage how that information is organized.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Physically, database servers are dedicated computers that hold the actual databases and run only the DBMS and related software. Database servers are usually multiprocessor computers, with generous memory and RAID disk arrays used for stable storage. Hardware database accelerators, connected to one or more servers via a high-speed channel, are also used in large-volume transaction processing environments. DBMSs are found at the heart of most database applications. DBMSs may be built around a custom multitasking kernel with built-in networking support, but modern DBMSs typically rely on a standard operating system to provide these functions.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Databases and DBMSs can be categorized according to the database model(s) that they support (such as relational or XML), the type(s) of computer they run on (from a server cluster to a mobile phone), the query language(s) used to access the database (such as SQL or XQuery), and their internal engineering, which affects performance, scalability, resilience, and security.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "The sizes, capabilities, and performance of databases and their respective DBMSs have grown in orders of magnitude. These performance increases were enabled by the technology progress in the areas of processors, computer memory, computer storage, and computer networks. The concept of a database was made possible by the emergence of direct access storage media such as magnetic disks, which became widely available in the mid-1960s; earlier systems relied on sequential storage of data on magnetic tape. The subsequent development of database technology can be divided into three eras based on data model or structure: navigational, SQL/relational, and post-relational.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "The next generation of post-relational databases in the late 2000s became known as NoSQL databases, introducing fast key\u2013value stores and document-oriented databases. A competing \"next generation\" known as NewSQL databases attempted new implementations that retained the relational/SQL model while aiming to match the high performance of NoSQL compared to commercially available relational DBMSs.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "The introduction of the term database coincided with the availability of direct-access storage (disks and drums) from the mid-1960s onwards. The term represented a contrast with the tape-based systems of the past, allowing shared interactive use rather than daily batch processing. The Oxford English Dictionary cites a 1962 report by the System Development Corporation of California as the first to use the term \"data-base\" in a specific technical sense.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "As computers grew in speed and capability, a number of general-purpose database systems emerged; by the mid-1960s a number of such systems had come into commercial use. Interest in a standard began to grow, and Charles Bachman, author of one such product, the Integrated Data Store (IDS), founded the Database Task Group within CODASYL, the group responsible for the creation and standardization of COBOL. In 1971, the Database Task Group delivered their standard, which generally became known as the CODASYL approach, and soon a number of commercial products based on this approach entered the market.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Edgar F. Codd worked at IBM in San Jose, California, in one of their offshoot offices that were primarily involved in the development of hard disk systems. He was unhappy with the navigational model of the CODASYL approach, notably the lack of a \"search\" facility. In 1970, he wrote a number of papers that outlined a new approach to database construction that eventually culminated in the groundbreaking A Relational Model of Data for Large Shared Data Banks.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "In the hierarchic and network models, records were allowed to have a complex internal structure. For example, the salary history of an employee might be represented as a \"repeating group\" within the employee record. In the relational model, the process of normalization led to such internal structures being replaced by data held in multiple tables, connected only by logical keys.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "For instance, a common use of a database system is to track information about users, their name, login information, various addresses and phone numbers. In the navigational approach, all of this data would be placed in a single variable-length record. In the relational approach, the data would be normalized into a user table, an address table and a phone number table (for instance). Records would be created in these optional tables only if the address or phone numbers were actually provided.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Codd's paper was picked up by two people at Berkeley, Eugene Wong and Michael Stonebraker. They started a project known as INGRES using funding that had already been allocated for a geographical database project and student programmers to produce code. Beginning in 1973, INGRES delivered its first test products which were generally ready for widespread use in 1979. INGRES was similar to System R in a number of ways, including the use of a \"language\" for data access, known as QUEL. Over time, INGRES moved to the emerging SQL standard.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "In 1970, the University of Michigan began development of the MICRO Information Management System based on D.L. Childs' Set-Theoretic Data model. MICRO was used to manage very large data sets by the US Department of Labor, the U.S. Environmental Protection Agency, and researchers from the University of Alberta, the University of Michigan, and Wayne State University. It ran on IBM mainframe computers using the Michigan Terminal System. The system remained in production until 1998.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Another approach to hardware support for database management was ICL's CAFS accelerator, a hardware disk controller with programmable search capabilities. In the long term, these efforts were generally unsuccessful because specialized database machines could not keep pace with the rapid development and progress of general-purpose computers. Thus most database systems nowadays are software systems running on general-purpose hardware, using general-purpose computer data storage. However, this idea is still pursued in certain applications by some companies like Netezza and Oracle (Exadata).\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Stonebraker went on to apply the lessons from INGRES to develop a new database, Postgres, which is now known as PostgreSQL. PostgreSQL is often used for global mission-critical applications (the .org and .info domain name registries use it as their primary data store, as do many large companies and financial institutions).\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Another data model, the entity\u2013relationship model, emerged in 1976 and gained popularity for database design as it emphasized a more familiar description than the earlier relational model. Later on, entity\u2013relationship constructs were retrofitted as a data modeling construct for the relational model, and the difference between the two has become irrelevant.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "XML databases are a type of structured document-oriented database that allows querying based on XML document attributes. XML databases are mostly used in applications where the data is conveniently viewed as a collection of documents, with a structure that can vary from the very flexible to the highly rigid: examples include scientific articles, patents, tax filings, and personnel records.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "In recent years, there has been a strong demand for massively distributed databases with high partition tolerance, but according to the CAP theorem, it is impossible for a distributed system to simultaneously provide consistency, availability, and partition tolerance guarantees. A distributed system can satisfy any two of these guarantees at the same time, but not all three. For that reason, many NoSQL databases are using what is called eventual consistency to provide both availability and partition tolerance guarantees with a reduced level of data consistency.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "One way to classify databases involves the type of their contents, for example: bibliographic, document-text, statistical, or multimedia objects. Another way is by their application area, for example: accounting, music compositions, movies, banking, manufacturing, or insurance. A third way is by some technical aspect, such as the database structure or interface type. This section lists a few of the adjectives used to characterize different kinds of databases.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "It is also generally to be expected the DBMS will provide a set of utilities for such purposes as may be necessary to administer the database effectively, including import, export, monitoring, defragmentation and analysis utilities. The core part of the DBMS interacting between the database and the application interface sometimes referred to as the database engine.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Often DBMSs will have configuration parameters that can be statically and dynamically tuned, for example the maximum amount of main memory on a server the database can use. The trend is to minimize the amount of manual configuration, and for cases such as embedded databases the need to target zero-administration is paramount.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Early multi-user DBMS typically only allowed for the application to reside on the same computer with access via terminals or terminal emulation software. The client\u2013server architecture was a development where the application resided on a client desktop and the database on a server allowing the processing to be distributed. This evolved into a multitier architecture incorporating application servers and web servers with the end user interface via a web browser with the database only directly connected to the adjacent tier.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "A general-purpose DBMS will provide public application programming interfaces (API) and optionally a processor for database languages such as SQL to allow applications to be written to interact with and manipulate the database. A special purpose DBMS may use a private API and be specifically customized and linked to a single application. For example, an email system performs many of the functions of a general-purpose DBMS such as message insertion, message deletion, attachment handling, blocklist lookup, associating messages an email address and so forth however these functions are limited to what is required to handle email.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "A programmer will code interactions to the database (sometimes referred to as a datasource) via an application program interface (API) or via a database language. The particular API or language chosen will need to be supported by DBMS, possibly indirectly via a preprocessor or a bridging API. Some API's aim to be database independent, ODBC being a commonly known example. Other common API's include JDBC and ADO.NET.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Often storage redundancy is employed to increase performance. A common example is storing materialized views, which consist of frequently needed external views or query results. Storing such views saves the expensive computing them each time they are needed. The downsides of materialized views are the overhead incurred when updating them to keep them synchronized with their original updated database data, and the cost of storage redundancy.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Occasionally a database employs storage redundancy by database objects replication (with one or more copies) to increase data availability (both to improve performance of simultaneous multiple end-user accesses to the same database object, and to provide resiliency in a case of partial failure of a distributed database). Updates of a replicated object need to be synchronized across the object copies. In many cases, the entire database is replicated.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Database access control deals with controlling who (a person or a certain computer program) are allowed to access what information in the database. The information may comprise specific database objects (e.g., record types, specific records, data structures), certain computations over certain objects (e.g., query types, or specific queries), or using specific access paths to the former (e.g., using specific indexes or other data structures to access information). Database access controls are set by special authorized (by the database owner) personnel that uses dedicated protected security DBMS interfaces.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Data security in general deals with protecting specific chunks of data, both physically (i.e., from corruption, or destruction, or removal; e.g., see physical security), or the interpretation of them, or parts of them to meaningful information (e.g., by looking at the strings of bits that they comprise, concluding specific valid credit-card numbers; e.g., see data encryption).\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Database transactions can be used to introduce some level of fault tolerance and data integrity after recovery from a crash. A database transaction is a unit of work, typically encapsulating a number of operations over a database (e.g., reading a database object, writing, acquiring or releasing a lock, etc.), an abstraction supported in database and also other systems. Each transaction has well defined boundaries in terms of which program/code executions are included in that transaction (determined by the transaction's programmer via special transaction commands).\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "After designing a database for an application, the next stage is building the database. Typically, an appropriate general-purpose DBMS can be selected to be used for this purpose. A DBMS provides the needed user interfaces to be used by database administrators to define the needed application's data structures within the DBMS's respective data model. Other user interfaces are used to select needed DBMS parameters (like security related, storage allocation parameters, etc.).\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "When the database is ready (all its data structures and other needed components are defined), it is typically populated with initial application's data (database initialization, which is typically a distinct project; in many cases using specialized DBMS interfaces that support bulk insertion) before making it operational. In some cases, the database becomes operational while empty of application data, and data are accumulated during its operation.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "After the database is created, initialized and populated it needs to be maintained. Various database parameters may need changing and the database may need to be tuned (tuning) for better performance; application's data structures may be changed or added, new related application programs may be written to add to the application's functionality, etc.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Static analysis techniques for software verification can be applied also in the scenario of query languages. In particular, the *Abstract interpretation framework has been extended to the field of query languages for relational databases as a way to support sound approximation techniques. The semantics of query languages can be tuned according to suitable abstractions of the concrete domain of data. The abstraction of relational database systems has many interesting applications, in particular, for security purposes, such as fine-grained access control, watermarking, etc.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Producing the conceptual data model sometimes involves input from business processes, or the analysis of workflow in the organization. This can help to establish what information is needed in the database, and what can be left out. For example, it can help when deciding whether the database needs to hold historic data as well as current data.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "The most popular database model for general-purpose databases is the relational model, or more precisely, the relational model as represented by the SQL language. The process of creating a logical database design using this model uses a methodical approach known as normalization. The goal of normalization is to ensure that each elementary \"fact\" is only recorded in one place, so that insertions, updates, and deletions automatically maintain consistency.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "A database model is a type of data model that determines the logical structure of a database and fundamentally determines in which manner data can be stored, organized, and manipulated. The most popular example of a database model is the relational model (or the SQL approximation of relational), which uses a table-based format.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "While there is typically only one conceptual and internal view of the data, there can be any number of different external views. This allows users to see database information in a more business-related way rather than from a technical, processing viewpoint. For example, a financial department of a company needs the payment details of all employees as part of the company's expenses, but does not need details about employees that are in the interest of the human resources department. Thus different departments need different views of the company's database.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "The three-level database architecture relates to the concept of data independence which was one of the major initial driving forces of the relational model. The idea is that changes made at a certain level do not affect the view at a higher level. For example, changes in the internal level do not affect application programs written using conceptual level interfaces, which reduces the impact of making physical changes to improve performance.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Database technology has been an active research topic since the 1960s, both in academia and in the research and development groups of companies (for example IBM Research). Research activity includes theory and development of prototypes. Notable research topics have included models, the atomic transaction concept, related concurrency control techniques, query languages and query optimization methods, RAID, and more.\n", "link": "https://en.wikipedia.org/wiki/Database"}, {"text": "Markings and visual elements can be called chartjunk if they are not part of the minimum set of visuals necessary to communicate the information understandably.  Examples of unnecessary elements that might be called chartjunk include heavy or dark grid lines, unnecessary text, inappropriately complex or gimmicky font faces, ornamented chart axes, and display frames, pictures, backgrounds or icons within data graphs, ornamental shading and unnecessary dimensions.\n", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "Another kind of chartjunk skews the depiction and makes it difficult to understand the real data being displayed.  Examples of this type include items depicted out of scale to one another, noisy backgrounds making comparison between elements difficult in a chart or graph, and 3-D simulations in line and bar charts.\n", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "The interior decoration of graphics generates a lot of ink that does not tell the viewer anything new. The purpose of decoration varies\u2014to make the graphic appear more scientific and precise, to enliven the display, to give the designer an opportunity to exercise artistic skills. Regardless of its cause, it is all non-data-ink or redundant data-ink, and it is often chartjunk.", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "Nearly all those who produce graphics for mass publication are trained exclusively in the fine arts and have had little experience with the analysis of data. Such experiences are essential for achieving precision and grace in the presence of statistics... Those who get ahead are those who beautified data, never mind statistical integrity.\"", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "\"Time's Nigel Holmes, creator of the diamonds graph, was understandably irked when Tufte criticized it. Holmes admits his work has sometimes been exaggerated, but feels that Tufte, in his insistence on absolute mathematical fidelity, remains trapped in \u2019the world of academia\u2019 and insensitive to \u2019the world of commerce,\u2019 with its need to grab an audience\"", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "In a recent study by Parsons and Shukla, they interviewed data visualization designers and found that there is both a \"corrective movement\" in the design community to move away from minimalist design principles, but also, that designers had different definitions for what constitutes chartjunk. The authors felt that \"better definitions are needed so that everyone has a shared understanding [about chartjunk].\"\n", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "The information visualization research community has researched the effects of chartjunk on how viewers interpret visualizations. There have been studies that found that chartjunk increases long-term memorability of the chart. A recent study found that chartjunk, in the form of semantically meaningful icons, increased accessibility of charts for people with Intellectual and Developmental Disabilities (IDD).\n", "link": "https://en.wikipedia.org/wiki/Chartjunk"}, {"text": "Visual perception is the ability to interpret the surrounding environment through photopic vision (daytime vision), color vision, scotopic vision (night vision), and mesopic vision (twilight vision), using light in the visible spectrum reflected by objects in the environment. This is different from visual acuity, which refers to how clearly a person sees (for example \"20/20 vision\"). A person can have problems with visual perceptual processing even if they have 20/20 vision.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The resulting perception is also known as vision, sight, or eyesight (adjectives visual, optical, and ocular, respectively). The various physiological components involved in vision are referred to collectively as the visual system, and are the focus of much research in linguistics, psychology, cognitive science, neuroscience, and molecular biology, collectively referred to as vision science.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The lateral geniculate nucleus sends signals to the primary visual cortex, also called striate cortex. Extrastriate cortex, also called visual association cortex is a set of cortical structures, that receive information from striate cortex, as well as each other. Recent descriptions of visual association cortex describe a division into two functional pathways, a ventral and a dorsal pathway. This conjecture is known as the two streams hypothesis.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The human visual system is generally believed to be sensitive to visible light in the range of wavelengths between 370 and 730 nanometers of the electromagnetic spectrum. However, some research suggests that humans can perceive light in wavelengths down to 340 nanometers (UV-A), especially the young. Under optimal conditions these limits of human perception can extend to 310\u2009nm (UV) to 1100\u2009nm (NIR).\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The major problem in visual perception is that what people see is not simply a translation of retinal stimuli (i.e., the image on the retina), with the brain altering the basic information taken in. Thus people interested in perception have long struggled to explain what visual processing does to create what is actually seen.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The second school advocated the so-called 'intromission' approach which sees vision as coming from something entering the eyes representative of the object. With its main propagator Aristotle (De Sensu), and his followers, this theory seems to have some contact with modern theories of what vision really is, but it remained only a speculation lacking any experimental foundation. (In eighteenth-century England, Isaac Newton, John Locke, and others, carried the intromission theory of vision forward by insisting that vision involved a process in which rays\u2014composed of actual corporeal matter\u2014emanated from seen objects and entered the seer's mind/sensorium through the eye's aperture.)\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Both schools of thought relied upon the principle that \"like is only known by like\", and thus upon the notion that the eye was composed of some \"internal fire\" that interacted with the \"external fire\" of visible light and made vision possible. Plato makes this assertion in his dialogue Timaeus (45b and 46b), as does Empedocles (as reported by Aristotle in his De Sensu, DK frag. B17).\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Alhazen (965 \u2013 c. 1040) carried out many investigations and experiments on visual perception, extended the work of Ptolemy on binocular vision, and commented on the anatomical works of Galen. He was the first person to explain that vision occurs when light bounces on an object and then is directed to one's eyes.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Leonardo da Vinci (1452\u20131519) is believed to be the first to recognize the special optical qualities of the eye. He wrote \"The function of the human eye ... was described by a large number of authors in a certain way. But I found it to be completely different.\" His main experimental finding was that there is only a distinct and clear vision at the line of sight\u2014the optical line that ends at the fovea. Although he did not use these words literally he actually is the father of the modern distinction between foveal and peripheral vision.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Isaac Newton (1642\u20131726/27) was the first to discover through experimentation, by isolating individual colors of the spectrum of light passing through a prism, that the visually perceived color of objects appeared due to the character of light the objects reflected, and that these divided colors could not be changed into any other color, which was contrary to scientific expectation of the day.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Hermann von Helmholtz is often credited with the first modern study of visual perception. Helmholtz examined the human eye and concluded that it was incapable of producing a high-quality image. Insufficient information seemed to make vision impossible. He, therefore, concluded that vision could only be the result of some form of \"unconscious inference\", coining that term in 1867. He proposed the brain was making assumptions and conclusions from incomplete data, based on previous experiences.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The Gestalt Laws of Organization have guided the study of how people perceive visual components as organized patterns or wholes, instead of many different parts. \"Gestalt\" is a German word that partially translates to \"configuration or pattern\" along with \"whole or emergent structure\". According to this theory, there are eight main factors that determine how the visual system automatically groups elements into patterns: Proximity, Similarity, Closure, Symmetry, Common Fate (i.e. common motion), Continuity as well as Good Gestalt (pattern that is regular, simple, and orderly) and Past Experience.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The picture to the right shows what may happen during the first two seconds of visual inspection. While the background is out of focus, representing the peripheral vision, the first eye movement goes to the boots of the man (just because they are very near the starting fixation and have a reasonable contrast).  Eye movements serve the function of attentional selection, i.e.,  to select a fraction of all visual inputs for deeper processing by the brain.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Some studies tend to show that rather than the uniform global image, some particular features and regions of interest of the objects are key elements when the brain needs to recognise an object in an image. In this way, the human vision is vulnerable to small particular changes to the image, such as disrupting the edges of the object,\u00a0modifying texture or any small change in a crucial region of the image.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "Studies of people whose sight has been restored after a long blindness reveal that they cannot necessarily recognize objects and faces (as opposed to color, motion, and simple geometric shapes). Some hypothesize that being blind during childhood prevents some part of the visual system necessary for these higher-level tasks from developing properly. The general belief that a critical period lasts until age 5 or 6 was challenged by a 2007 study that found that older patients could improve these abilities with years of exposure.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "In the 1970s, David Marr developed a multi-level theory of vision, which analyzed the process of vision at different levels of abstraction. In order to focus on the understanding of specific problems in vision, he identified three levels of analysis: the computational, algorithmic and implementational levels. Many vision scientists, including Tomaso Poggio, have embraced these levels of analysis and employed them to further characterize vision from a computational perspective.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The computational level addresses, at a high level of abstraction, the problems that the visual system must overcome. The algorithmic level attempts to identify the strategy that may be used to solve these problems. Finally, the implementational level attempts to explain how solutions to these problems are realized in neural circuitry.\n", "link": "https://en.wikipedia.org/wiki/Visual_perception"}, {"text": "The method of least squares was published by Legendre in 1805, and by Gauss in 1809. Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets). Gauss published a further development of the theory of least squares in 1821, including a version of the Gauss\u2013Markov theorem.\n", "link": "https://en.wikipedia.org/wiki/Regression_analysis"}, {"text": "Once a regression model has been constructed, it may be important to confirm the goodness of fit of the model and the statistical significance of the estimated parameters. Commonly used checks of goodness of fit include the R-squared, analyses of the pattern of residuals and hypothesis testing. Statistical significance can be checked by an F-test of the overall fit, followed by t-tests of individual parameters.\n", "link": "https://en.wikipedia.org/wiki/Regression_analysis"}, {"text": "Interpretations of these diagnostic tests rest heavily on the model's assumptions. Although examination of the residuals can be used to invalidate a model, the results of a t-test or F-test are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a central limit theorem can be invoked such that hypothesis testing may proceed using asymptotic approximations.\n", "link": "https://en.wikipedia.org/wiki/Regression_analysis"}, {"text": "Regression models predict a value of the Y variable given known values of the X variables. Prediction within the range of values in the dataset used for model-fitting is known informally as interpolation. Prediction outside this range of the data is known as extrapolation. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.\n", "link": "https://en.wikipedia.org/wiki/Regression_analysis"}, {"text": "All major statistical software packages perform least squares regression analysis and inference. Simple linear regression and multiple regression using least squares can be done in some spreadsheet applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized. Different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.\n", "link": "https://en.wikipedia.org/wiki/Regression_analysis"}, {"text": "Informally, a statistical model can be thought of as a statistical assumption (or set of statistical assumptions) with a certain property: that the assumption allows us to calculate the probability of any event. As an example, consider a pair of ordinary six-sided dice. We will study two different statistical assumptions about the dice.\n", "link": "https://en.wikipedia.org/wiki/Statistical_model"}, {"text": "In this example, the dimension, k, equals 2. As another example, suppose that the data consists of points (x, y) that we assume are distributed according to a straight line with i.i.d. Gaussian residuals (with zero mean): this leads to the same statistical model as was used in the example with children's heights.  The dimension of the statistical model is 3: the intercept of the line, the slope of the line, and the variance of the distribution of the residuals. (Note the set of all possible lines has dimension 2, even though geometrically, a line has dimension 1.)\n", "link": "https://en.wikipedia.org/wiki/Statistical_model"}, {"text": "Two statistical models are nested if the first model can be transformed into the second model by imposing constraints on the parameters of the first model. As an example, the set of all Gaussian distributions has, nested within it, the set of zero-mean Gaussian distributions: we constrain the mean in the set of all Gaussian distributions to get the zero-mean distributions. As a second example, the quadratic model \n", "link": "https://en.wikipedia.org/wiki/Statistical_model"}, {"text": "In both those examples, the first model has a higher dimension than the second model (for the first example, the zero-mean model has dimension\u00a01).  Such is often, but not always, the case.  As an example where they have the same dimension, the set of positive-mean Gaussian distributions is nested within the set of all Gaussian distributions; they both have dimension 2.\n", "link": "https://en.wikipedia.org/wiki/Statistical_model"}, {"text": "Comparing statistical models is fundamental for much of statistical inference. Konishi & Kitagawa (2008, p.\u00a075) state: \"The majority of the problems in statistical inference can be considered to be problems related to statistical modeling. They are typically formulated as comparisons of several statistical models.\" Common criteria for comparing models include the following: R, Bayes factor, Akaike information criterion, and the likelihood-ratio test together with its generalization, the relative likelihood.\n", "link": "https://en.wikipedia.org/wiki/Statistical_model"}, {"text": "Misleading graphs may be created intentionally to hinder the proper interpretation of data or accidentally due to unfamiliarity with graphing software, misinterpretation of data, or because data cannot be accurately conveyed. Misleading graphs are often used in false advertising. One of the first authors to write about misleading graphs was Darrell Huff, publisher of the 1954 book How to Lie with Statistics.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "Tables are preferable to graphics for many small data sets. A table is nearly always better than a dumb pie chart; the only thing worse than a pie chart is several of them, for then the viewer is asked to compare quantities located in spatial disarray both within and between pies \u2013 Given their low data-density and failure to order numbers along a visual dimension, pie charts should never be used.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "Misuse of log scales may also cause relationships between quantities to appear linear whilst those relationships are exponentials or power laws that rise very rapidly towards higher values. It has been stated, although mainly in a humorous way, that \"anything looks linear on a log-log plot with thick marker pen\" .\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "Both graphs show an identical exponential function of f(x) = 2. The graph on the left uses a linear scale, showing clearly an exponential trend. The graph on the right, however uses a logarithmic scale, which generates a straight line. If the graph viewer were not aware of this, the graph would appear to show a linear trend.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "While truncated graphs can be used to overdraw differences or to save space, their use is often discouraged. Commercial software such as MS Excel will tend to truncate graphs by default if the values are all within a narrow range, as in this example. To show relative differences in values over time, an index chart can be used. Truncated diagrams will always distort the underlying numbers visually. Several studies found that even if people were correctly informed that the y-axis was truncated, they still overestimated the actual differences, often substantially.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "Though all three graphs share the same data, and hence the actual slope of the (x, y) data is the same, the way that the data is plotted can change the visual appearance of the angle made by the line on the graph. This is because each plot has a different scale on its vertical axis. Because the scale is not shown, these graphs can be misleading.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "The graph discrepancy index, also known as the graph distortion index (GDI), was originally proposed by Paul John Steinbart in 1998. GDI is calculated as a percentage ranging from \u2212100% to positive infinity, with zero percent indicating that the graph has been properly constructed and anything outside the \u00b15% margin is considered to be distorted. Research into the usage of GDI as a measure of graphics distortion has found it to be inconsistent and discontinuous, making the usage of GDI as a measurement for comparisons difficult.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "Several published studies have looked at the usage of graphs in corporate reports for different corporations in different countries and have found frequent usage of improper design, selectivity, and measurement distortion within these reports. The presence of misleading graphs in annual reports has led to requests for standards to be set.\n", "link": "https://en.wikipedia.org/wiki/Misleading_graph"}, {"text": "Computational physics is the study and implementation of numerical analysis to solve problems in physics. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics \u2014 an area of study which supplements both theory and experiment.\n", "link": "https://en.wikipedia.org/wiki/Computational_physics"}, {"text": "There is a debate about the status of computation within the scientific method. Sometimes it is regarded as more akin to theoretical physics; some others regard computer simulation as \"computer experiments\", yet still others consider it an intermediate or different branch between theoretical and experimental physics, a third way that supplements theory and experiment. While computers can be used in experiments for the measurement and recording (and storage) of data, this clearly does not constitute a computational approach.\n", "link": "https://en.wikipedia.org/wiki/Computational_physics"}, {"text": "Due to the broad class of problems computational physics deals, it is an essential component of modern research in different areas of physics, namely: accelerator physics, astrophysics, general theory of relativity (through numerical relativity), fluid mechanics (computational fluid dynamics), lattice field theory/lattice gauge theory (especially lattice quantum chromodynamics), plasma physics (see plasma modeling), simulating physical systems (using e.g. molecular dynamics), nuclear engineering computer codes, protein structure prediction, weather prediction, solid state physics, soft condensed matter physics, hypervelocity impact physics etc.\n", "link": "https://en.wikipedia.org/wiki/Computational_physics"}, {"text": "Computational solid state physics, for example, uses density functional theory to calculate properties of solids, a method similar to that used by chemists to study molecules.  Other quantities of interest in solid state physics, such as the electronic band structure, magnetic properties and charge densities can be calculated by this and several methods, including the Luttinger-Kohn/k.p method and ab-initio methods.\n", "link": "https://en.wikipedia.org/wiki/Computational_physics"}, {"text": "Computational mechanics is the discipline concerned with the use of computational methods to study phenomena governed by the principles of mechanics. Before the emergence of computational science (also called scientific computing) as a \"third way\" besides theoretical and experimental sciences, computational mechanics was widely considered to be a sub-discipline of applied mechanics. It is now considered to be a sub-discipline within computational science.\n", "link": "https://en.wikipedia.org/wiki/Computational_mechanics"}, {"text": "The areas of mathematics most related to computational mechanics are partial differential equations, linear algebra and numerical analysis. The most popular numerical methods used are the finite element, finite difference, and boundary element methods in order of dominance. In solid mechanics finite element methods are far more prevalent than finite difference methods, whereas in fluid mechanics, thermodynamics, and electromagnetism, finite difference methods are almost equally applicable. The boundary element technique is in general less popular, but has a niche in certain areas including acoustics engineering, for example.\n", "link": "https://en.wikipedia.org/wiki/Computational_mechanics"}, {"text": "With regard to computing, computer programming, algorithms, and parallel computing play a major role in CM. The most widely used programming language in the scientific community, including computational mechanics, is Fortran. Recently, C++ has increased in popularity. The scientific computing community has been slow in adopting C++ as the lingua franca. Because of its very natural way of expressing mathematical computations, and its built-in visualization capacities, the proprietary language/environment MATLAB is also widely used, especially for rapid application development and model verification.\n", "link": "https://en.wikipedia.org/wiki/Computational_mechanics"}, {"text": "It typically involves using computer programs to compute approximate solutions to Maxwell's equations to calculate antenna performance, electromagnetic compatibility, radar cross section and electromagnetic wave propagation when not in free space.  A large subfield is antenna modeling computer programs, which calculate the radiation pattern and electrical properties of radio antennas, and are widely used to design antennas for specific applications.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "Several real-world electromagnetic problems like electromagnetic scattering, electromagnetic radiation, modeling of waveguides etc., are not analytically calculable, for the multitude of irregular geometries found in actual devices. Computational numerical techniques can overcome the inability to derive closed form solutions of Maxwell's equations under various constitutive relations of media, and boundary conditions. This makes computational electromagnetics (CEM) important to the design, and modeling of antenna, radar, satellite and other communication systems, nanophotonic devices and high speed silicon electronics, medical imaging, cell-phone antenna design, among other applications.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "CEM typically solves the problem of computing the E (electric) and H (magnetic) fields across the problem domain (e.g., to calculate antenna radiation pattern for an arbitrarily shaped antenna structure). Also calculating power flow direction (Poynting vector), a waveguide's normal modes, media-generated wave dispersion, and scattering can be computed from the E and H fields. CEM models may or may not assume symmetry, simplifying real world structures to idealized cylinders, spheres, and other regular geometrical objects. CEM models extensively make use of symmetry, and solve for reduced dimensionality from 3 spatial dimensions to 2D and even 1D.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "An eigenvalue problem formulation of CEM allows us to calculate steady state normal modes in a structure. Transient response and impulse field effects are more accurately modeled by CEM in time domain, by FDTD. Curved geometrical objects are treated more accurately as finite elements FEM, or non-orthogonal grids. Beam propagation method (BPM) can solve for the power flow in waveguides. CEM is application specific, even if different techniques converge to the same field and power distributions in the modeled domain.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "Choosing the right technique for solving a problem is important, as choosing the wrong one can either result in incorrect results, or results which take excessively long to compute. However, the name of a technique does not always tell one how it is implemented, especially for commercial tools, which will often have more than one solver.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "The method of moments (MoM) or boundary element method (BEM) is a numerical computational method of solving linear partial differential equations which have been formulated as integral equations (i.e. in boundary integral form). It can be applied in many areas of engineering and science including fluid mechanics, acoustics, electromagnetics, fracture mechanics, and plasticity.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "MoM is applicable to problems for which Green's functions can be calculated. These usually involve fields in linear homogeneous media. This places considerable restrictions on the range and generality of problems suitable for boundary elements. Nonlinearities can be included in the formulation, although they generally introduce volume integrals which require the volume to be discretized before solution, removing an oft-cited advantage of MoM.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "The fast multipole method (FMM) is an alternative to MoM or Ewald summation. It is an accurate simulation technique and requires less memory and processor power than MoM. The FMM was first introduced by Greengard and Rokhlin and is based on the multipole expansion technique. The first application of the FMM in computational electromagnetics was by Engheta et al.(1992). The FMM has also applications in computational bioelectromagnetics in the Charge based boundary element fast multipole method.   FMM can also be used to accelerate MoM.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "While the fast multipole method is useful for accelerating MoM solutions of integral equations with static or frequency-domain oscillatory kernels, the plane wave time-domain (PWTD) algorithm employs similar ideas to accelerate the MoM solution of time-domain integral equations involving the retarded potential. The PWTD algorithm was introduced in 1998 by Ergin, Shanker, and Michielssen.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "Finite-difference frequency-domain (FDFD) provides a rigorous solution to Maxwell\u2019s equations in the frequency-domain using the finite-difference method. FDFD is arguably the simplest numerical method that still provides a rigorous solution. It is incredibly versatile and able to solve virtually any problem in electromagnetics. The primary drawback of FDFD is poor efficiency compared to other methods. On modern computers, however, a huge array of problems are easily handled such as calculated guided modes in waveguides, calculating scattering from an object, calculating transmission and reflection from photonic crystals, calculate photonic band diagrams, simulating metamaterials, and much more.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "FDFD may be the best \"first\" method to learn in computational electromagnetics (CEM). It involves almost all the concepts encountered with other methods, but in a much simpler framework. Concepts include boundary conditions, linear algebra, injecting sources, representing devices numerically, and post-processing field data to calculate meaningful things. This will help a person learn other techniques as well as provide a way to test and benchmark those other techniques.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "FDFD is very similar to finite-difference time-domain (FDTD). Both methods represent space as an array of points and enforces Maxwell\u2019s equations at each point. FDFD puts this large set of equations into a matrix and solves all the equations simultaneously using linear algebra techniques. In contrast, FDTD continually iterates over these equations to evolve a solution over time. Numerically, FDFD and FDTD are very similar, but their implementations are very different.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "FDTD belongs in the general class of grid-based differential time-domain numerical modeling methods. Maxwell's equations (in partial differential form) are modified to central-difference equations, discretized, and implemented in software. The equations are solved in a cyclic manner:  the electric field is solved at a given instant in time, then the magnetic field is solved at the next instant in time, and the process is repeated over and over again.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "The finite element method (FEM) is used to find approximate solution of partial differential equations (PDE) and integral equations.  The solution approach is based either on eliminating the time derivatives completely (steady state problems), or rendering the PDE into an equivalent ordinary differential equation, which is then solved using standard techniques such as finite differences, etc.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "In solving partial differential equations, the primary challenge is to create an equation which approximates the equation to be studied, but which is numerically stable, meaning that errors in the input data and intermediate calculations do not accumulate and destroy the meaning of the resulting output. There are many ways of doing this, with various advantages and disadvantages. The finite element method is a good choice for solving partial differential equations over complex domains or when the desired precision varies over the entire domain.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "The finite integration technique (FIT) is a spatial discretization scheme to numerically solve electromagnetic field problems in time and frequency domain. It preserves basic topological properties of the continuous equations such as conservation of charge and energy. FIT was proposed in 1977 by Thomas Weiland and has been enhanced continually over the years. This method covers the full range of electromagnetics (from static up to high frequency) and optic applications and is the basis for commercial simulation tools: CST Studio Suite developed by Computer Simulation Technology (CST AG) and \nElectromagnetic Simulation solutions developed by Nimbic.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "The basic idea of this approach is to apply the Maxwell equations in integral form to a set of staggered grids. This method stands out due to high flexibility in geometric modeling and boundary handling as well as incorporation of arbitrary material distributions and material properties such as anisotropy, non-linearity and dispersion. Furthermore, the use of a consistent dual orthogonal grid (e.g. Cartesian grid) in conjunction with an explicit time integration scheme (e.g. leap-frog-scheme) leads to compute and memory-efficient algorithms, which are especially adapted for transient field analysis in radio frequency (RF) applications.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "This class of marching-in-time computational techniques for Maxwell's equations uses either discrete Fourier or discrete Chebyshev transforms to calculate the spatial derivatives of the electric and magnetic field vector components that are arranged in either a 2-D grid or 3-D lattice of unit cells. PSTD causes negligible numerical phase velocity anisotropy errors relative to FDTD, and therefore allows problems of much greater electrical size to be modeled.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "PSSD solves Maxwell's equations by propagating them forward in a chosen spatial direction. The fields are therefore held as a function of time, and (possibly) any transverse spatial dimensions. The method is pseudo-spectral because temporal derivatives are calculated in the frequency domain with the aid of FFTs. Because the fields are held as functions of time, this enables arbitrary dispersion in the propagation medium to be rapidly and accurately modelled with minimal effort. However, the choice to propagate forward in space (rather than in time) brings with it some subtleties, particularly if reflections are important.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "Transmission line matrix (TLM) can be formulated in several means as a direct set of lumped elements solvable directly by a circuit solver (ala SPICE, HSPICE, et al.), as a custom network of elements or via a scattering matrix approach.  TLM is a very flexible analysis strategy akin to FDTD in capabilities, though more codes tend to be available with FDTD engines.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "Eigenmode expansion (EME) is a rigorous bi-directional technique to simulate electromagnetic propagation which relies on the decomposition of the electromagnetic fields into a basis set of local eigenmodes. The eigenmodes are found by solving Maxwell's equations in each local cross-section. Eigenmode expansion can solve Maxwell's equations in 2D and 3D and can provide a fully vectorial solution provided that the mode solvers are vectorial. It offers very strong benefits compared with the FDTD method for the modelling of optical waveguides, and it is a popular tool for the modelling of fiber optics and silicon photonics devices.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "Physical optics (PO) is the name of a high frequency approximation (short-wavelength approximation) commonly used in optics, electrical engineering and applied physics.  It is an intermediate method between geometric optics, which ignores wave effects, and full wave electromagnetism, which is a precise theory. The word \"physical\" means that it is more physical than geometrical optics and not that it is an exact physical theory.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "The uniform theory of diffraction approximates near field electromagnetic fields as quasi optical and uses ray diffraction to determine diffraction coefficients for each diffracting object-source combination. These coefficients are then used to calculate the field strength and phase for each direction away from the diffracting point. These fields are then added to the incident fields and reflected fields to obtain a total solution.\n", "link": "https://en.wikipedia.org/wiki/Computational_electromagnetics"}, {"text": "In computational modelling, multiphysics simulation (often shortened to simply \"multiphysics\") is defined as the simultaneous simulation of different aspects of a physical system or systems and the interactions among them. For example, simultaneous simulation of the physical stress on an object, the temperature distribution of the object and the thermal expansion which leads to the variation of the stress and temperature distributions would be considered a multiphysics simulation. Multiphysics simulation is related to multiscale simulation, which is the simultaneous simulation of a single process on either multiple time or distance scales.\n", "link": "https://en.wikipedia.org/wiki/Multiphysics_simulation"}, {"text": "Mathematical models used in multiphysics simulations are generally a set of coupled equations. The equations can be divided into three categories according to the nature and intended role: governing equation, auxiliary equations and boundary/initial conditions. A governing equation describes a major physical mechanism or process. Multiphysics simulations are numerically implemented with discretization methods such as the finite element method, finite difference method, or finite volume method.\n", "link": "https://en.wikipedia.org/wiki/Multiphysics_simulation"}, {"text": "Generally speaking, multiphysics simulation is much harder than that for individual aspects of the physical processes.\nThe main extra issue is how to integrate the multiple aspects of the processes with proper handling of the interactions among them.\nSuch issue becomes quite difficult when different types numerical methods are used for the simulations of individual physical aspects. \nFor example, when simulating a fluid-structure interaction problem with typical Eulerian finite volume method for flow \nand Lagrangian finite element method for structure dynamics.\n", "link": "https://en.wikipedia.org/wiki/Multiphysics_simulation"}, {"text": "Computational particle physics refers to the methods and computing tools developed in and used by  particle physics research. Like computational chemistry or computational biology, it is, for particle physics both a specific branch and an interdisciplinary field relying on computer science, theoretical and experimental particle physics and mathematics.\nThe main fields of computational particle physics are: lattice field theory (numerical computations), automatic calculation of particle interaction or decay (computer algebra) and event generators (stochastic methods).\n", "link": "https://en.wikipedia.org/wiki/Computational_particle_physics"}, {"text": "Since the early 1980s, LQCD researchers have pioneered the use of massively parallel computers in large scientific applications, using virtually all available computing systems including traditional main-frames, large PC clusters, and high-performance systems. In addition, it has also been used as a benchmark for high-performance computing, starting with the IBM Blue Gene supercomputer.\n", "link": "https://en.wikipedia.org/wiki/Computational_particle_physics"}, {"text": "Several open and commercial programs exist to perform these operations. The concept of the technique is minimization of Gibbs free energy of the system; the success of this method is due not only to properly measuring thermodynamic properties, such as those in the list of thermodynamic properties, but also due to the extrapolation of the properties of metastable allotropes of the chemical elements.\n", "link": "https://en.wikipedia.org/wiki/Computational_thermodynamics"}, {"text": "The computational modeling of metal-based phase diagrams, which dates back to the beginning of the previous century mainly by Johannes van Laar and to the modeling of regular solutions, has evolved in more recent years to the CALPHAD (CALculation of PHAse Diagrams). This has been pioneered by American metallurgist Larry Kaufman since the 1970s.\n", "link": "https://en.wikipedia.org/wiki/Computational_thermodynamics"}, {"text": "A model consists of the equations used to capture the behavior of a system. By contrast, computer simulation is the actual running of the program that perform algorithms which solve those equations, often in an approximate manner. Simulation, therefore, is the process of running a model. Thus one would not \"build a simulation\"; instead, one would \"build a model (or a simulator)\", and then either \"run the model\" or equivalently \"run a simulation\".\n", "link": "https://en.wikipedia.org/wiki/Computer_simulation"}, {"text": "In social sciences, computer simulation is an integral component of the five angles of analysis fostered by the data percolation methodology, which also includes qualitative and quantitative methods, reviews of the literature (including scholarly), and interviews with experts, and which forms an extension of data triangulation. Of course, similar to any other scientific method, replication is an important part of computational modeling \n", "link": "https://en.wikipedia.org/wiki/Computer_simulation"}, {"text": "Vehicle manufacturers make use of computer simulation to test safety features in new designs. By building a copy of the car in a physics simulation environment, they can save the hundreds of thousands of dollars that would otherwise be required to build and test a unique prototype. Engineers can step through the simulation milliseconds at a time to determine the exact stresses being put upon each section of the prototype.\n", "link": "https://en.wikipedia.org/wiki/Computer_simulation"}, {"text": "Computer graphics can be used to display the results of a computer simulation. Animations can be used to experience a simulation in real-time, e.g., in training simulations. In some cases animations may also be useful in faster than real-time or even slower than real-time modes. For example, faster than real-time animations can be useful in visualizing the buildup of queues in the simulation of humans evacuating a building. Furthermore, simulation results are often aggregated into static images using various ways of scientific visualization.\n", "link": "https://en.wikipedia.org/wiki/Computer_simulation"}, {"text": "In debugging, simulating a program execution under test (rather than executing natively) can detect far more errors than the hardware itself can detect and, at the same time, log useful debugging information such as instruction trace, memory alterations and instruction counts. This technique can also detect buffer overflow and similar \"hard to detect\" errors as well as produce performance information and tuning data.\n", "link": "https://en.wikipedia.org/wiki/Computer_simulation"}, {"text": "The MLR potential is based on the classic Morse potential which was first introduced in 1929 by Philip M. Morse. A primitive version of the MLR potential was first introduced in 2006 by Robert J. Le Roy and colleagues for a study on N2. This primitive form was used on Ca2, KLi and MgH, before the more modern version was introduced in 2009. A further extension of the MLR potential referred to as the MLR3 potential was introduced in a 2010 study of Cs2, and this potential has since been used on HF, HCl, HBr and HI.\n", "link": "https://en.wikipedia.org/wiki/Morse/Long-range_potential"}, {"text": "The MLR potential has successfully summarized all experimental spectroscopic data (and/or virial data) for a number of diatomic molecules, including: N2, Ca2, KLi, MgH, several electronic states of Li2, Cs2, Sr2, ArXe, LiCa, LiNa, Br2, Mg2, HF, HCl, HBr, HI, MgD, Be2, BeH, and NaH. More sophisticated versions are used for polyatomic molecules.\n", "link": "https://en.wikipedia.org/wiki/Morse/Long-range_potential"}, {"text": "In computational chemistry, molecular physics, and physical chemistry, the Lennard-Jones potential (also termed the LJ potential or 12-6 potential; named for John Lennard-Jones) is an intermolecular pair potential. Out of all the intermolecular potentials, the Lennard-Jones potential is probably the one that has been the most extensively studied. It is considered an archetype model for simple yet realistic intermolecular interactions. The Lennard-Jones potential is often used as a building block in molecular models (a.k.a. force fields) for more complex substances. Many studies of the idealized \"Lennard-Jones substance\" use the potential to understand the physical nature of matter.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "The Lennard-Jones potential is a simple model that still manages to describe the essential features of interactions between simple atoms and molecules: Two interacting particles repel each other at very close distance, attract each other at moderate distance, and eventually stop interacting at infinite distance, as shown in the Figure. The Lennard-Jones potential is a pair potential, i.e. no three- or multi-body interactions are covered by the potential.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "Numerous intermolecular potentials have been proposed in the past for the modeling of simple soft repulsive and attractive interactions between spherically symmetric particles, i.e. the general shape shown in the Figure. Examples for other potentials are the Morse potential, the Mie potential, the Buckingham potential and the Tang-T\u00f6nnies potential. While some of these may be more suited to modelling real fluids, the simplicity of the Lennard-Jones potential, as well as its often surprising ability to accurately capture real fluid behavior, has historically made it the pair-potential of greatest general importance.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "Furthermore, the quality of the long-range correction scheme depends on the cut-off radius. The assumptions made with the correction schemes are usually not justified at (very) short cut-off radii. This is illustrated in the example shown in Figure on the right. The long-range correction scheme is said to be converged, if the remaining error of the correction scheme is sufficiently small at a given cut-off distance, cf. Figure.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "The Lennard-Jones potential \u2013 as an archetype for intermolecular potentials \u2013 has been used numerous times as starting point for the development of more elaborate or more generalized intermolecular potentials. Various extensions and modifications of the Lennard-Jones potential have been proposed in the literature; a more extensive list is given in the 'interatomic potential' article. The following list refers only to several example potentials that are directly related to the Lennard-Jones potential and are of both historic importance and still relevant for present research.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "The LJTS potential is computationally significantly cheaper than the 'full' Lennard-Jones potential, but still covers the essential physical features of matter (the presence of a critical and a triple point, soft repulsive and attractive interactions, phase equilibria etc.). Therefore, the LJTS potential is used for the testing of new algorithms, simulation methods, and new physical theories.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "The Lennard-Jones potential is not only of fundamental importance in computational chemistry and soft-matter physics, but also for the modeling of real substances. The Lennard-Jones potential is used for fundamental studies on the behavior of matter and for elucidating atomistic phenomena. It is also often used for somewhat special use cases, e.g. for studying thermophysical properties of two- or four-dimensional substances (instead of the classical three spatial directions of our universe).\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "Overall, due to the large timespan the Lennard-Jones potential has been studied and thermophysical property data has been reported in the literature and computational resources were insufficient for accurate simulations (to modern standards), a noticeable amount of data is known to be dubious. Nevertheless, in many studies such data is used as reference. The lack of data repositories and data assessment is a crucial element for future work in the long-going field of Lennard-Jones potential research.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "The uncertainties represent the scattering of data from different authors. The critical point of the Lennard-Jones substance has been studied far more often than the triple point. For both the critical point and the vapor\u2013liquid\u2013solid triple point, several studies reported results out of the above stated ranges. The above stated data is the presently assumed correct and reliable data. Nevertheless, the determinateness of the critical temperature and the triple point temperature is still unsatisfactory.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "Properties of the Lennard-Jones fluid have been studied extensively in the literature due to the outstanding importance of the Lennard-Jones potential in soft-matter physics and related fields. About 50 datasets of computer experiment data for the vapor\u2013liquid equilibrium have been published to date. Furthermore, more than 35,000 data points at homogeneous fluid states have been published over the years and recently been compiled and assessed for outliers in an open access database.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "Mixtures of Lennard-Jones particles are mostly used as a prototype for the development of theories and methods of solutions, but also to study properties of solutions in general. This dates back to the fundamental work of conformal solution theory of Longuet-Higgins and Leland and Rowlinson and co-workers. Those are today the basis of most theories for mixtures.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "A large number of equations of state (EOS) for the Lennard-Jones potential/ substance have been proposed since its characterization and evaluation became available with the first computer simulations. Due to the fundamental importance of the Lennard-Jones potential, most currently available molecular-based EOS are built around the Lennard-Jones fluid. They have been comprehensively reviewed by Stephan et al.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "Equations of state for the Lennard-Jones fluid are of particular importance in soft-matter physics and physical chemistry, used as starting point for the development of EOS for complex fluids, e.g. polymers and associating fluids. The monomer units of these models are usually directly adapted from Lennard-Jones EOS as a building block, e.g. the PHC EOS, the BACKONE EOS, and SAFT type EOS.\n", "link": "https://en.wikipedia.org/wiki/Lennard-Jones_potential"}, {"text": "on account of its short-range. However, there were many issues with his theory. For one, it is impossible for an electron of spin \u20601/2\u2060 and a proton of spin \u20601/2\u2060 to add up to the neutron spin of \u20601/2\u2060. The way Heisenberg treated this issue would go on to form the ideas of isospin.\n", "link": "https://en.wikipedia.org/wiki/Yukawa_potential"}, {"text": "A comparison of the long range potential strength for Yukawa and Coulomb is shown in Figure\u00a02. It can be seen that the Coulomb potential has effect over a greater distance whereas the Yukawa potential approaches zero rather quickly. However, any Yukawa potential or Coulomb potential is non-zero for any large r.\n", "link": "https://en.wikipedia.org/wiki/Yukawa_potential"}, {"text": "We can calculate the differential cross section between a proton or neutron and the pion by making use of the Yukawa potential. We use the Born approximation, which tells us that, in a spherically symmetrical potential, we can approximate the outgoing scattered wave function as the sum of incoming plane wave function and a small perturbation:\n", "link": "https://en.wikipedia.org/wiki/Yukawa_potential"}, {"text": "An extension of the Morse potential that made the Morse form useful for modern (high-resolution) spectroscopy is the MLR (Morse/Long-range) potential. The MLR potential is used as a standard for representing spectroscopic and/or virial data of diatomic molecules by a potential energy curve. It has been used on N2, Ca2, KLi, MgH, several electronic states of Li2, Cs2, Sr2, ArXe, LiCa, LiNa, Br2, Mg2, HF, HCl, HBr, HI, MgD, Be2, BeH, and NaH.  More sophisticated versions are used for polyatomic molecules.\n", "link": "https://en.wikipedia.org/wiki/Morse_potential"}, {"text": "CFD is applied to a wide range of research and engineering problems in many fields of study and industries, including aerodynamics and aerospace analysis, hypersonics, weather simulation, natural science and environmental engineering, industrial system design and analysis, biological engineering, fluid flows and heat transfer, engine and combustion analysis, and visual effects for film and games.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "The fundamental basis of almost all CFD problems is the Navier\u2013Stokes equations, which define many single-phase (gas or liquid, but not both) fluid flows.  These equations can be simplified by removing terms describing viscous actions to yield the Euler equations.  Further simplification, by removing terms describing vorticity yields the full potential equations. Finally, for small perturbations in subsonic and supersonic flows (not transonic or hypersonic) these equations can be linearized to yield the linearized potential equations.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "One of the earliest type of calculations resembling modern CFD are those by Lewis Fry Richardson, in the sense that these calculations used finite differences and divided the physical space in cells. Although they failed dramatically, these calculations, together with Richardson's book Weather Prediction by Numerical Process, set the basis for modern CFD and numerical meteorology. In fact, early CFD calculations during the 1940s using ENIAC used methods close to those in Richardson's 1922 book.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "The next step was the Euler equations, which promised to provide more accurate solutions of transonic flows.  The methodology used by Jameson in his three-dimensional FLO57 code (1981) was used by others to produce such programs as Lockheed's TEAM program and IAI/Analytical Methods' MGAERO program.  MGAERO is unique in being a structured cartesian mesh code, while most other such codes use structured body-fitted grids (with the exception of NASA's highly successful CART3D code, Lockheed's SPLITFLOW code and Georgia Tech's NASCART-GT). Antony Jameson also developed the three-dimensional AIRPLANE code which made use of unstructured tetrahedral grids.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "In the two-dimensional realm, Mark Drela and Michael Giles, then graduate students at MIT, developed the ISES Euler program (actually a suite of programs) for airfoil design and analysis.  This code first became available in 1986 and has been further developed to design, analyze and optimize single or multi-element airfoils, as the MSES program.  MSES sees wide use throughout the world.  A derivative of MSES, for the design and analysis of airfoils in a cascade, is MISES, developed by Harold Youngren while he was a graduate student at MIT.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "In the finite volume method, the governing partial differential equations (typically the Navier-Stokes equations, the mass and energy conservation equations, and the turbulence equations) are recast in a conservative form, and then solved over discrete control volumes. This discretization guarantees the conservation of fluxes through a particular control volume. The finite volume equation yields governing equations in the form,\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "The lattice Boltzmann method (LBM) with its simplified kinetic picture on a lattice provides a computationally efficient description of hydrodynamics.\nUnlike the traditional CFD methods, which solve the conservation equations of macroscopic properties (i.e., mass, momentum, and energy) numerically, LBM models the fluid consisting of fictive particles, and such particles perform consecutive propagation and collision processes over a discrete lattice mesh. In this method, one works with the discrete in space and time version of the kinetic evolution equation in the Boltzmann Bhatnagar-Gross-Krook (BGK) form.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "In computational modeling of turbulent flows, one common objective is to obtain a model that can predict quantities of interest, such as fluid velocity, for use in engineering designs of the system being modeled.  For turbulent flows, the range of length scales and complexity of phenomena involved in turbulence make most modeling approaches prohibitively expensive; the resolution required to resolve all scales involved in turbulence is beyond what is computationally possible.  The primary approach in such cases is to create numerical models to approximate unresolved phenomena.  This section lists some commonly used computational models for turbulent flows.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "Turbulence models can be classified based on computational expense, which corresponds to the range of scales that are modeled versus resolved (the more turbulent scales that are resolved, the finer the resolution of the simulation, and therefore the higher the computational cost). If a majority or all of the turbulent scales are not modeled, the computational cost is very low, but the tradeoff comes in the form of decreased accuracy.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "Large eddy simulation (LES) is a technique in which the smallest scales of the flow are removed through a filtering operation, and their effect modeled using subgrid scale models.  This allows the largest and most important scales of the turbulence to be resolved, while greatly reducing the computational cost incurred by the smallest scales. This method requires greater computational resources than RANS methods, but is far cheaper than DNS.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "The vorticity confinement (VC) method is an Eulerian technique used in the simulation of turbulent wakes. It uses a solitary-wave like approach to produce a stable solution with no numerical spreading. VC can capture the small-scale features to within as few as 2 grid cells. Within these features, a nonlinear difference equation is solved as opposed to the finite difference equation. VC is similar to shock capturing methods, where conservation laws are satisfied, so that the essential integral quantities are accurately computed.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "The Linear eddy model is a technique used to simulate the convective mixing that takes place in turbulent flow. Specifically, it provides a mathematical way to describe the interactions of a scalar variable within the vector flow field. It is primarily used in one-dimensional representations of turbulent flow, since it can be applied across a wide range of length scales and Reynolds numbers. This model is generally used as a building block for more complicated flow representations, as it provides high resolution predictions that hold across a large range of flow conditions.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "The modeling of two-phase flow is still under development. Different methods have been proposed, including the Volume of fluid method, the level-set method and front tracking.  These methods often involve a tradeoff between maintaining a sharp interface or conserving mass .  This is crucial since the evaluation of the density, viscosity and surface tension is based on the values averaged over the interface.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "Multigrid has the advantage of asymptotically optimal performance on many problems. Traditional solvers and preconditioners are effective at reducing high-frequency components of the residual, but low-frequency components typically require many iterations to reduce. By operating on multiple scales, multigrid reduces all components of the residual by similar factors, leading to a mesh-independent number of iterations.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "For indefinite systems, preconditioners such as incomplete LU factorization, additive Schwarz, and multigrid perform poorly or fail entirely, so the problem structure must be used for effective preconditioning. Methods commonly used in CFD are the SIMPLE and Uzawa algorithms which exhibit mesh-dependent convergence rates, but recent advances based on block LU factorization combined with multigrid for the resulting definite systems have led to preconditioners that deliver mesh-independent convergence rates.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "CFD made a major break through in late 70s with the introduction of LTRAN2, a 2-D code to model oscillating airfoils based on transonic small perturbation theory by Ballhaus and associates. It uses a Murman-Cole switch algorithm for modeling the moving shock-waves. Later it was extended to 3-D with use of a rotated difference scheme by AFWAL/Boeing that resulted in LTRAN3.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "CFD investigations are used to clarify the characteristics of aortic flow in details that are beyond the capabilities of experimental measurements. To analyze these conditions, CAD models of the human vascular system are extracted employing modern imaging techniques such as MRI or Computed Tomography. A 3D model is reconstructed from this data and the fluid flow can be computed. Blood properties such as density and viscosity, and realistic boundary conditions (e.g.  systemic pressure) have to be taken into consideration. Therefore, making it possible to analyze and optimize the flow in the cardiovascular system for different applications.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "In a more recent trend, simulations are also performed on GPUs. These typically contain slower but more processors. For CFD algorithms that feature good parallelism performance (i.e. good speed-up by adding more cores) this can greatly reduce simulation times. Fluid-implicit particle and lattice-Boltzmann methods are typical examples of codes that scale well on GPUs.\n", "link": "https://en.wikipedia.org/wiki/Computational_fluid_dynamics"}, {"text": "In numerical analysis, finite-difference methods (FDM) are a class of numerical techniques for solving differential equations by approximating derivatives with finite differences. Both the spatial domain and time domain (if applicable) are discretized, or broken into a finite number of intervals, and the values of the solution at the end points of the intervals are approximated by solving algebraic equations containing finite differences and values from nearby points.\n", "link": "https://en.wikipedia.org/wiki/Finite_difference_method"}, {"text": "Finite difference methods convert ordinary differential equations (ODE) or partial differential equations (PDE), which may be nonlinear, into a system of linear equations that can be solved by matrix algebra techniques. Modern computers can perform these linear algebra computations efficiently, and this, along with their relative ease of implementation, has led to the widespread use of FDM in modern numerical analysis.\nToday, FDMs are one of the most common approaches to the numerical solution of PDE, along with finite element methods.\n", "link": "https://en.wikipedia.org/wiki/Finite_difference_method"}, {"text": "The error in a method's solution is defined as the difference between the approximation and the exact analytical solution. The two sources of error in finite difference methods are round-off error, the loss of precision due to computer rounding of decimal quantities, and truncation error or discretization error, the difference between the exact solution of the original differential equation and the exact quantity assuming perfect arithmetic (no round-off).\n", "link": "https://en.wikipedia.org/wiki/Finite_difference_method"}, {"text": "To use a finite difference method to approximate the solution to a problem, one must first discretize the problem's domain. This is usually done by dividing the domain into a uniform grid (see image). This means that finite-difference methods produce sets of discrete numerical approximations to the derivative, often in a \"time-stepping\" manner.\n", "link": "https://en.wikipedia.org/wiki/Finite_difference_method"}, {"text": "In this case, the local truncation error is proportional to the step sizes. The quality and duration of simulated FDM solution depends on the discretization equation selection and the step sizes (time and space steps). The data quality and simulation duration increase significantly with smaller step size. Therefore, a reasonable balance between data quality and simulation duration is necessary for practical usage. Large time steps are useful for increasing simulation speed in practice. However, time steps which are too large may create instabilities and affect the data quality.\n", "link": "https://en.wikipedia.org/wiki/Finite_difference_method"}, {"text": "To summarize, usually the Crank\u2013Nicolson scheme is the most accurate scheme for small time steps. For larger time steps, the implicit scheme works better since it is less computationally demanding. The explicit scheme is the least accurate and can be unstable, but is also the easiest to implement and the least numerically intensive. \n", "link": "https://en.wikipedia.org/wiki/Finite_difference_method"}]
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e44b67e-f87a-4ffd-9cbd-530617d6bcfd",
   "metadata": {},
   "source": [
    "# Ανάκτησης Πληροφορίας - Δημιουργία Μηχανής Αναζήτησης\n",
    "- Μπηλιώνη Παρασκευή <br> Α.Μ. ice20390286\n",
    "  \n",
    "- Πλάγου Αικατερίνη  <br> Α.Μ. ice20390191"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f16e44-86fe-4f42-b1fb-b9a4ef6aea66",
   "metadata": {},
   "source": [
    "# 1. Συλλογή δεδομένων"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce382ce-5a5d-4e64-b889-3db892d56fbf",
   "metadata": {},
   "source": [
    "Εισαγωγή και αρχικοποίηση των απαραίτητων βιβλιοθηκών."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecc6cc-f715-4e2f-a491-63fe559c791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from bs4 import BeautifulSoup  # Web scraping and parsing \n",
    "import requests                # Makes HTTP request\n",
    "import json                    # Handles JSON data\n",
    "import nltk                    # Text processing\n",
    "import string                  # Handles strings\n",
    "import sys                     # System-specific parameters and functions\n",
    "from nltk.corpus import stopwords       # Handles stopwords in text processing\n",
    "from nltk.stem import WordNetLemmatizer # Word lemmatization\n",
    "from collections import defaultdict     # Creates dictionaries \n",
    "import ipywidgets as widgets            # Creates widgets\n",
    "import numpy as np                      # Does computations\n",
    "from IPython.display import display, Markdown                # Displays widgets and text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF vectorization\n",
    "from sklearn.metrics.pairwise import cosine_similarity       # Calculates vector similarity (used in VSM)\n",
    "from rank_bm25 import BM25Okapi                              # Rankins documents (used in okapi BM25)\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score  # Calculates engine evaluation metrics\n",
    "from IPython.display import display, Markdown               \n",
    "import pandas as pd   \n",
    "\n",
    "# Download NLTK datasets\n",
    "# Tokenizer models\n",
    "nltk.download('punkt')  \n",
    "# List of stopwords\n",
    "nltk.download('stopwords')  \n",
    "# For lemmatization\n",
    "nltk.download('wordnet')    \n",
    "\n",
    "# Initialize stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words('english'))  # Set of stopwords\n",
    "lemmatizer = WordNetLemmatizer()              # Lemmatizer for reducing words to base forms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b283a0-d7aa-4e3c-9b8c-b1937d8d982f",
   "metadata": {},
   "source": [
    "Επιλογή πηγής από την οποία θα ξεκινήσει η συλλογή εγγράφων. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef38166-29e8-4966-b24a-68a38b476147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting wikipedia link to scrape\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Data_analysis'\n",
    "\n",
    "try:\n",
    "    # Send request to starting link\n",
    "    response = requests.get(wiki_url)\n",
    "    # Raise an exception for errors\n",
    "    response.raise_for_status()\n",
    "        \n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "# Handle any exceptions during the request\n",
    "except requests.RequestException as e:\n",
    "    print(f\"____Request failed____\\n{e}\\n\")\n",
    "\n",
    "display(Markdown(f\"Data will be scraped from this starting page: [{wiki_url}]({wiki_url})<br><br>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d551f31d-061d-4fe0-ae56-56cc5bb607a0",
   "metadata": {},
   "source": [
    "Αποθήκευση δεδομένων σε JSON αρχείο.\n",
    "Σε περίπτωση σφάλματος κατά την διάρκεια της διαδικασίας, εμφανίζεται κατάλληλο μήνυμα λάθους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84adcf0-9753-4895-81aa-ae954f00445a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_things(things, filename):\n",
    "    try:\n",
    "        # Open the specified file in write mode\n",
    "        with open(filename, 'w') as file:\n",
    "            # Convert data to a JSON string and write it to the file\n",
    "            json.dump(things, file, indent = 4)\n",
    "\n",
    "    # If the process fails print error message\n",
    "    except Exception as e:\n",
    "        print(f\"____Error saving____\\n{e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2a824-8e28-4db4-81e5-27bd903d42a1",
   "metadata": {},
   "source": [
    "Απόκτηση εγγράφων/συνδέσμων από την κύρια σελίδα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976e336-5f00-41b0-b034-9434e2d3dc76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_links(soup):\n",
    "    # Base wikipedia url\n",
    "    https = 'https://en.wikipedia.org'  \n",
    "    # For storing valid links\n",
    "    links = []  \n",
    "\n",
    "    display(Markdown(\"Links saved: <br>\"))\n",
    "    # Find all anchor tags with 'href' attribute\n",
    "    for link in soup.find_all('a', href = True):\n",
    "        # Extract link reference\n",
    "        url = link.get('href') \n",
    "\n",
    "        # Check if the link is a wikipedia aticle and filter out irrelevant links\n",
    "        if url.startswith('/wiki/') and not any(\n",
    "            url.startswith(f'/wiki/{keyword}')\n",
    "            for keyword in ['Wikipedia', 'Help', 'Special', 'Portal', 'Talk', 'Category', 'File', 'Main_Page']):\n",
    "            # Construct full wikipedia url\n",
    "            full_url = f\"{https}{url}\"\n",
    "\n",
    "            # Skip links that appear if they are already saved\n",
    "            if full_url not in links:\n",
    "                links.append(full_url)\n",
    "                #display(Markdown(f\"[{full_url}]({full_url})<br>\"))\n",
    "                print(f\"{full_url}\\n\")\n",
    "                \n",
    "        # Collect 70 first valid links\n",
    "        if len(links) >= 70:\n",
    "            break \n",
    "            \n",
    "    # Return links\n",
    "    return links  \n",
    "\n",
    "\n",
    "# Collect and store links from the main page\n",
    "links = get_links(soup)\n",
    "store_things(links, 'wikipedia_collected_urls.json')\n",
    "\n",
    "# For tracking visited pages\n",
    "visited_links = set()\n",
    "\n",
    "# Mark the starting link as visited\n",
    "visited_links.add(wiki_url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925bea3-bbbd-448e-8568-3adac1c97b46",
   "metadata": {},
   "source": [
    "Συλλογή παραγράφων από κάθε σελίδα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf1a21c-496e-4ea1-8c9b-78f12d095586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_paragraphs(soup, visited_paragraphs, link):\n",
    "    # Stores paragraphs\n",
    "    paragraphs = []  \n",
    "\n",
    "    # Remove the content from superscripts and references\n",
    "    for sup in soup.find_all(['sup', 'reflist']):\n",
    "        sup.decompose()  \n",
    "\n",
    "    # Find all paragraph tags \n",
    "    for p in soup.find_all('p'):\n",
    "        # Extract text from each paragraph\n",
    "        text = p.get_text()  \n",
    "        \n",
    "        # Remove empty paragraphs and those containing the term 'displaystyle' to avoid mathematical functions\n",
    "        if text and 'displaystyle' not in text.lower():\n",
    "            # Calculate the number of words in the paragraph\n",
    "            word_count = len(text.split())  \n",
    "            \n",
    "            # Include paragraphs with word count between 50 and 100 and avoid duplicates\n",
    "            if 50 <= word_count <= 100 and text not in visited_paragraphs:\n",
    "                # Store paragraph with source link\n",
    "                paragraphs.append({'text': text, 'link': link}) \n",
    "                # Mark the paragraph as visited to avoid repetition\n",
    "                visited_paragraphs.add(text)  \n",
    "                        \n",
    "    # Return filtered paragraphs         \n",
    "    return paragraphs  \n",
    "\n",
    "\n",
    "# For tracking visited paragraphs\n",
    "visited_paragraphs = set()\n",
    "\n",
    "# Collect paragraphs from starting page\n",
    "original_paragraphs = get_paragraphs(soup, visited_paragraphs, wiki_url)\n",
    "\n",
    "display(Markdown(\"Paragraphs saved from starting page: <br>\"))\n",
    "for paragraph in original_paragraphs:\n",
    "    print(f\"{paragraph['text']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021ba030-174f-4a75-8510-f9f8391109db",
   "metadata": {},
   "source": [
    "# 2. Προεπεξεργασία κειμένου (Text Procissing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb7b9ad-490e-4ed1-8e61-194977a3656c",
   "metadata": {},
   "source": [
    "Διαμόρφωση κειμένου μετά από αφαίρεση διακόπτουσων λέξεων, ειδικών χαρακτήρων και λημματοποίση όρων. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00cbde-eb35-40a8-8276-2e2ee890422d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Tokenize text into individual words and convert to lowercase for better search results\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # List to store cleaned tokens\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    # Remove punctuation and non alphanumeric tokens\n",
    "    for token in tokens:\n",
    "        if token not in string.punctuation and token.isalnum():\n",
    "            cleaned_tokens.append(token)\n",
    "\n",
    "    # List to store tokens after stopword removal\n",
    "    filtered_tokens = []\n",
    "\n",
    "    # Remove stopwords from the tokenized text\n",
    "    for token in cleaned_tokens:\n",
    "        if token not in stop_words:\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    # Initialize a list to store lemmatized tokens\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    for token in filtered_tokens:\n",
    "        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n",
    "\n",
    "    # Return processed tokens\n",
    "    return lemmatized_tokens\n",
    "\n",
    "cleaned_paragraphs = []\n",
    "\n",
    "# Clean the collected paragraphs using text preprocessing\n",
    "for paragraph in original_paragraphs:\n",
    "    #clean_paragraph = ' '.join(clean_text(paragraph))\n",
    "    clean_paragraph = clean_text(paragraph['text'])\n",
    "    cleaned_paragraphs.append({'tokens': clean_paragraph, 'link': paragraph['link']})\n",
    "\n",
    "display(Markdown(\"Processed paragraphs from starting page: <br>\"))\n",
    "for paragraph in cleaned_paragraphs:\n",
    "    print(f\"{paragraph['tokens']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd0fcd5-5937-4b52-8588-1c8e65ee8f2f",
   "metadata": {},
   "source": [
    "Ληψη και εξαγωγή παραγράφων από κάθε σύνδεσμο. \n",
    "Λαμβάνεται το περιεχόμενο από έναν σύνδεσμο και εξάγονται οι παράγραφοι, σε περίπτωση που δεν έχουν ήδη καταχωρηθεί."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8110dc8-7ea2-4676-824b-3ad28e6a00d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paragraphs_within_links(link, visited_links, visited_paragraphs):\n",
    "    # Skip the link and return an empty list if it has already been processed\n",
    "    if link in visited_links:\n",
    "        return []\n",
    "\n",
    "    try:\n",
    "        # Send get request to link and raise error if the request was unsuccessful\n",
    "        response = requests.get(link)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the pages content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Mark current link as visited\n",
    "        visited_links.add(link)\n",
    "        \n",
    "        # Extract and return paragraphs from the page\n",
    "        return get_paragraphs(soup, visited_paragraphs, link)\n",
    "        \n",
    "    # Exception for request errors\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving links: {e}\")\n",
    "        # Return an empty list if there is an error\n",
    "        return []  \n",
    "\n",
    "display(Markdown(\"Scraping paragraphs from each link <br>\"))\n",
    "progress = widgets.IntProgress(\n",
    "    value = 0,\n",
    "    min = 0,\n",
    "    max = len(links)\n",
    ")\n",
    "\n",
    "display(progress)\n",
    "\n",
    "# Get paragraphs from each link and avoid re visiting links\n",
    "for i, link in enumerate(links):\n",
    "    # Get paragraphs from current link\n",
    "    link_paragraphs = paragraphs_within_links(link, visited_links, visited_paragraphs)\n",
    "\n",
    "    # Extend list of original paragraphs adding the new ones\n",
    "    original_paragraphs.extend(link_paragraphs)\n",
    "\n",
    "    # Clean and store paragraphs from current link\n",
    "    for paragraph in link_paragraphs:\n",
    "        #clean_paragraph = ' '.join(clean_text(paragraph))\n",
    "        clean_paragraph = clean_text(paragraph['text'])\n",
    "        cleaned_paragraphs.append({'tokens': clean_paragraph, 'link': paragraph['link']})\n",
    "\n",
    "    # Update progress bar\n",
    "    progress.value = i + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27794df3-965b-4c06-93e7-569cf7a90cbb",
   "metadata": {},
   "source": [
    "Αποθήκευση επιλεγμένων παραγράφων στην αρχική και επεξεργασμένη μορφή τους."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a0e48-8cac-49d7-8cfe-9801585e438a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "store_things(original_paragraphs, 'wikipedia_paragraphs.json')\n",
    "store_things(cleaned_paragraphs, 'wikipedia_paragraphs_cleaned.json')\n",
    "\n",
    "display(Markdown(\"How some of the paragraphs are saved: <br>\"))\n",
    "for paragraph in original_paragraphs[:10]:\n",
    "    text = paragraph['text']\n",
    "    link = paragraph['link']\n",
    "\n",
    "    print(f\"text: {text}\")\n",
    "    print(f\"link: {link}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da15bc76-a952-4880-b64b-2700f1faa693",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "display(Markdown(\"How some of the processed and tokenised paragraphs are saved: <br>\"))\n",
    "for paragraph in cleaned_paragraphs[:10]:\n",
    "    tokens = paragraph['tokens']\n",
    "    link = paragraph['link']\n",
    "\n",
    "    print(f\"tokens: {tokens}\\n\")\n",
    "    print(f\"link: {link}\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c66a7f-8e48-4b8e-a5f5-ef32da94d613",
   "metadata": {},
   "source": [
    "# 3. Ευρετήριο (Indexing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a505bb83-33e1-44b5-b1d1-92cfd0dc153d",
   "metadata": {},
   "source": [
    "Δημιουργία ανεστραμμένου ευρετήριου από τις επιλεγμένες και επεξεργασμένες παραγράφους και αποθήκευσή του σε αρχείο JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91790f45-59fc-4315-acd1-c2e8c24f8dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build an inverted index from the collected paragraphs\n",
    "def build_inverted_index(cleaned_paragraphs):\n",
    "    \"\"\"\n",
    "    Builds an inverted index from a list of paragraphs.\n",
    "\n",
    "    Parameters:\n",
    "    - paragraphs: A list of dictionaries, each containing a 'text' key with the paragraph content.\n",
    "\n",
    "    Returns:\n",
    "    - inverted_index: A defaultdict where each token (word) maps to a set of paragraph IDs where the token appears.\n",
    "    \"\"\"\n",
    "    # Defaultdict where keys are tokens and values are sets of paragraph IDs\n",
    "    inverted_index = defaultdict(set)\n",
    "    \n",
    "    # Look through each tokenized paragraph and assign a unique ID\n",
    "    for paragraph_id, paragraph in enumerate(cleaned_paragraphs):\n",
    "        # Get the tokens from each paragraph\n",
    "        tokens = paragraph['tokens']\n",
    "        \n",
    "        # Add each token to the inverted index with its paragraph ID\n",
    "        for token in tokens:\n",
    "            inverted_index[token].add(paragraph_id)\n",
    "             \n",
    "    return inverted_index\n",
    "\n",
    "\n",
    "# Function to store an inverted index in a JSON file\n",
    "def store_inverted_index(inverted_index):\n",
    "    \"\"\"\n",
    "    Stores an inverted index into a JSON file named 'inverted_index.json'.\n",
    "\n",
    "    Parameters:\n",
    "    - inverted_index: A dictionary where each key is a term and the value is a set of paragraph IDs.\n",
    "\n",
    "    Data Conversion:\n",
    "    - Since sets are not JSON serializable, the sets are converted to lists before storage.\n",
    "\n",
    "    Exception Handling:\n",
    "    - Catches any exceptions that may occur during file writing and prints an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert sets to lists to save in JSON file\n",
    "        serializable_index = {}\n",
    "        for term, paragraph_ids in inverted_index.items():\n",
    "            serializable_index[term] = list(paragraph_ids)\n",
    "\n",
    "        # Save the converted index to the file\n",
    "        with open('inverted_index.json', 'w') as file:\n",
    "            json.dump(serializable_index, file, indent = 4)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"____Error saving inverted index____\\n{e}\")\n",
    "\n",
    "# Build inverted index from cleaned paragraphs\n",
    "inverted_index = build_inverted_index(cleaned_paragraphs)\n",
    "\n",
    "# Store generated inverted index\n",
    "store_inverted_index(inverted_index)\n",
    "\n",
    "\n",
    "# Convert the inverted index into a list of token and paragraph IDs\n",
    "index = list(inverted_index.items())\n",
    "# Create a data frame from the inverted index\n",
    "inverted_df = pd.DataFrame(index, columns = ['Token', 'Paragraph ID'])\n",
    "# Display the inverted index\n",
    "display(Markdown(\"Inverted Index <br>\"))\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(inverted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b861c544-9ba8-4163-82e5-3dbdb13b64f7",
   "metadata": {},
   "source": [
    "# 4. Μηχανή αναζήτησης (Search Engine) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334124ca-2280-4e33-b913-df9c7ea0c99a",
   "metadata": {},
   "source": [
    "α) Επεξεργασία ερωτήματος (Query Processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9990651-605c-46be-817e-5b2906320007",
   "metadata": {},
   "source": [
    "Μετατροπή ερωτήματος από infix σε postfix μορφή και ό,τι δεν συμπεριλαμβάνεται στην άλγεβρα Boole, γίνεται lemmatized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4cbe77-abda-4b29-b49c-03b85640d1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infix_to_postfix(query):\n",
    "    # Define operator precedence (higher value means higher precedence)\n",
    "    precedence = {'NOT': 3, 'AND': 2, 'OR': 1}\n",
    "    \n",
    "    # Output and operator stack\n",
    "    output = []  \n",
    "    operators = []\n",
    "\n",
    "    # Split the query into tokens\n",
    "    tokens = query.split()\n",
    "    \n",
    "    # Process each token\n",
    "    for token in tokens:\n",
    "        # If the token is an operator handle based on precedence\n",
    "        if token in precedence:\n",
    "            # Pop operators with higher or equal precedence from the stack\n",
    "            while operators and precedence.get(operators[-1], 0) >= precedence[token]:\n",
    "                output.append(operators.pop())\n",
    "            # Push the current operator to the stack\n",
    "            operators.append(token)  \n",
    "\n",
    "        # If the token is left parenthesis push it onto the stack\n",
    "        elif token == '(':\n",
    "            operators.append(token)\n",
    "\n",
    "        # If the token is right parenthesis pop until the matching left parenthesis\n",
    "        elif token == ')':\n",
    "            while operators and operators[-1] != '(':\n",
    "                output.append(operators.pop())\n",
    "            # Remove left parenthesis from stack\n",
    "            operators.pop()  \n",
    "\n",
    "        # If the token is a word or other charachters \n",
    "        else:\n",
    "            # Lemmatize and turn into lowecase\n",
    "            token = lemmatizer.lemmatize(token.lower())\n",
    "            # Rremove word or number charachters\n",
    "            if token.isalnum():\n",
    "                output.append(token)\n",
    "\n",
    "    # Pop any remaining operators from the stack and append them to output\n",
    "    while operators:\n",
    "        output.append(operators.pop())\n",
    "\n",
    "    # Return the query in postfix notation\n",
    "    return output\n",
    "\n",
    "\n",
    "display(Markdown(\"Example of how queries are processed <br>\"))\n",
    "test_query = \"( data AND analysis ) OR NOT science\"\n",
    "# Convert the query from infix to postfix notation\n",
    "test_postfix_query = infix_to_postfix(test_query)\n",
    "\n",
    "# Print the original query and its postfix conversion\n",
    "print(f\"Original queryn (Infix):   {test_query}\")\n",
    "print(f\"Converted query (Postfix): {(test_postfix_query)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d084d88c-7164-40a6-be17-1e40ef8f12d7",
   "metadata": {},
   "source": [
    "Συλλογή παραγράφων με βάση το ήδη επεξεργασμένο ερώτημα. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c1eae0-0af5-4e9a-85c2-d8fc4895716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_postfix(postfix_tokens, inverted_index, num_paragraphs):\n",
    "    # Evaluating the postfix expression\n",
    "    stack = []  \n",
    "    \n",
    "    # Handling NOT operations\n",
    "    all_paragraphs = set(range(num_paragraphs))  \n",
    "\n",
    "    # Look through each token in the expression and preform nessesary operations\n",
    "    for token in postfix_tokens:\n",
    "        if token == 'AND':\n",
    "            # Pop the top two sets\n",
    "            right = stack.pop()  \n",
    "            left = stack.pop()\n",
    "            # Push the result of addition to stack\n",
    "            stack.append(left & right)  \n",
    "\n",
    "        elif token == 'OR':\n",
    "            right = stack.pop()\n",
    "            left = stack.pop()\n",
    "            # Push the result of union to stack\n",
    "            stack.append(left | right)  \n",
    "\n",
    "        # If the token is NOT operator calculate the difference \n",
    "        elif token == 'NOT':\n",
    "            operand = stack.pop()\n",
    "            # Push ducuments that are not in list to stack\n",
    "            stack.append(all_paragraphs - operand)  \n",
    "\n",
    "        # If the token is search term retrieve the matching paragraph IDs from inverted index\n",
    "        else:\n",
    "            # Push matching paragraph IDs to stack\n",
    "            stack.append(inverted_index.get(token, set())) \n",
    "\n",
    "    # Return the matching paragraph IDs if there are any or empty set\n",
    "    if stack:\n",
    "        return stack.pop()\n",
    "    else:\n",
    "        return set()\n",
    "\n",
    "\n",
    "test_matching_paragraphs = evaluate_postfix(test_postfix_query, inverted_index, len(cleaned_paragraphs))\n",
    "display(Markdown(\"Matching paragraph IDs for example query:\"))\n",
    "print(f\"{test_matching_paragraphs}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9949db3-9917-4c6a-9fc2-acf027ad0539",
   "metadata": {},
   "source": [
    "<br>β) Κατάταξη αποτελεσμάτων (Ranking)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd2b54-f6ab-49c4-8e3e-3e63f86eb681",
   "metadata": {},
   "source": [
    "Δημιουργία πίνακα TF-IDF με βάση τα αποτελέσματα του ερωτήματος. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739e1233-c684-4bbc-809e-34eaaa13ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(results, cleaned_paragraphs):\n",
    "    # Return nothing if there are no results\n",
    "    if not results:\n",
    "        return None, [], [], None\n",
    "\n",
    "    # Store the filtered paragraphs and their IDs\n",
    "    filtered_paragraphs = []\n",
    "    filtered_ids = []\n",
    "\n",
    "    # Extract the text and IDs of paragraphs that match the query results\n",
    "    for paragraph_id in results:\n",
    "        # Join the tokens of each paragraph into a single string for processing\n",
    "        filtered_paragraphs.append(' '.join(cleaned_paragraphs[paragraph_id]['tokens']))\n",
    "        filtered_ids.append(paragraph_id)\n",
    "\n",
    "    # Initialize and compute the TF-IDF matrix of the paragraphs\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(filtered_paragraphs)\n",
    "\n",
    "    # Return the TF-IDF matrix, paragraphs, their IDs and TF-IDF vectorizer\n",
    "    return tfidf_matrix, filtered_paragraphs, filtered_ids, vectorizer\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd2cfc2-bfad-4af0-b7ae-6e4ee47fa8cf",
   "metadata": {},
   "source": [
    "Κατάταξη αποτελεσμάτων με τον αλγόριθμο Vector Space Model και επιστροφή της παραγράφου με μεγαλύτερη τη βαθμολόγηση από κάθε σχετικό σύνδεσμο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a827cf-0075-47e9-ac56-3d25b9162856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_space_model(cleaned_query, tfidf_matrix, original_paragraphs, filtered_ids, vectorizer):\n",
    "    # Join the query tokens into a string\n",
    "    cleaned_query = ' '.join(cleaned_query)\n",
    "\n",
    "    # Transform the query into a TF-IDF vector using the TF-IDF vectorizer\n",
    "    query_vector = vectorizer.transform([cleaned_query])\n",
    "\n",
    "    # Compute the cosine similarity between the query and the TF-IDF matrix\n",
    "    cosine_similarities = cosine_similarity(query_vector, tfidf_matrix)[0]\n",
    "\n",
    "    # Store the paragraph with the biggest score for each link\n",
    "    link_top_scores = {}\n",
    "\n",
    "    # Look through each paragraph and its similarity score\n",
    "    for paragraph_id, score in zip(filtered_ids, cosine_similarities):\n",
    "        # Retrieve the original paragraph data to print\n",
    "        original = original_paragraphs[paragraph_id]\n",
    "        # Extract the link of current paragraph\n",
    "        link = original['link']  \n",
    "\n",
    "        # If the link is new or the score is higher than the existing one update the record\n",
    "        if link not in link_top_scores or score > link_top_scores[link]['score']:\n",
    "            link_top_scores[link] = {\n",
    "                'paragraph_id': paragraph_id,\n",
    "                'link': link,\n",
    "                'text': original['text'],\n",
    "                'score': score\n",
    "            }\n",
    "\n",
    "    # Sort the results in descending order\n",
    "    sorted_scores = sorted(\n",
    "        link_top_scores.values(),\n",
    "        key = lambda x: x['score'],\n",
    "        reverse = True\n",
    "    )\n",
    "\n",
    "    # Return highest ranked paragraphs \n",
    "    return sorted_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2448c57e-4e92-4652-b3ba-6488b0504c51",
   "metadata": {},
   "source": [
    "Κατάταξη αποτελεσμάτων με τον αλγόριθμο Okapi BM25 και επιστροφή της παραγράφου με τη μεγαλύτερη βαθμολόγηση από κάθε σχετικό σύνδεσμο."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce26782-a24e-4b86-84f5-6c00183a2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def okapi_bm25(cleaned_query, filtered_paragraphs, filtered_ids, original_paragraphs):\n",
    "    # Tokenize the filtered paragraphs\n",
    "    tokenized_paragraphs = []\n",
    "\n",
    "    # For every paragraph\n",
    "    for paragraph in filtered_paragraphs:\n",
    "        # Split into tokens\n",
    "        tokens = paragraph.split(' ')\n",
    "        tokenized_paragraphs.append(tokens)\n",
    "\n",
    "    # Initialize BM25 Okapi and fit it on the tokenized paragraphs\n",
    "    bm25 = BM25Okapi(tokenized_paragraphs)\n",
    "\n",
    "    # Compute the BM25 scores for the query\n",
    "    bm25_scores = bm25.get_scores(cleaned_query)\n",
    "\n",
    "    # Keep track of the highest ranked paragraph for each link\n",
    "    link_top_scores = {}\n",
    "\n",
    "    # Look through the filtered paragraph IDs and their BM25 scores\n",
    "    for paragraph_id, score in zip(filtered_ids, bm25_scores):\n",
    "        # Retrieve original paragraph data for printing\n",
    "        original = original_paragraphs[paragraph_id]\n",
    "        link = original['link']\n",
    "\n",
    "        # Store the paragraph only if it has the highest score for this link\n",
    "        if link not in link_top_scores or score > link_top_scores[link]['score']:\n",
    "            link_top_scores[link] = {\n",
    "                'paragraph_id': paragraph_id,\n",
    "                'link': link,\n",
    "                'text': original['text'],\n",
    "                'score': score\n",
    "            }\n",
    "\n",
    "    # Sort the results descending order\n",
    "    sorted_scores = sorted(\n",
    "        link_top_scores.values(),\n",
    "        key = lambda x: x['score'],\n",
    "        reverse = True\n",
    "    )\n",
    "\n",
    "    # Return highest ranked paragraphs\n",
    "    return sorted_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22096737-f0fc-44ec-b4a0-75d1f3d58262",
   "metadata": {},
   "source": [
    "Δημιουργία μηχανής αναζήτησης (Search Engine) με τη δυνατότητα επιλογής αλγόριθμου αναζήτησης και εισαγωγή ερωτήματος από τον χρήστη. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81d521a-3b49-460c-9496-01da8376f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine(inverted_index, original_paragraphs, cleaned_paragraphs):\n",
    "    # Select the ranking algorithm\n",
    "    toggle_buttons = widgets.ToggleButtons(\n",
    "        options = ['Boolean retrieval', 'Vector Space Model', 'Okapi BM25'],\n",
    "        description = 'Select ranking algorithm'\n",
    "    )\n",
    "    \n",
    "    space = widgets.HTML(value = '<br>')\n",
    "\n",
    "    # Enter search queries\n",
    "    input_text = widgets.Text(\n",
    "        placeholder = 'Input search query here',\n",
    "        layout = widgets.Layout(width = '70%')\n",
    "    )\n",
    "    \n",
    "    # Button to activate searching\n",
    "    search_button = widgets.Button(\n",
    "        description = 'Search',\n",
    "        button_style = 'primary'\n",
    "    )\n",
    "    \n",
    "    # Display search results\n",
    "    output = widgets.Output()\n",
    "\n",
    "    # Handles the search when the button is clicked\n",
    "    def search(b):\n",
    "        # Clear previous results\n",
    "        output.clear_output()  \n",
    "        # Get the selected ranking algorithm\n",
    "        algorithm = toggle_buttons.value  \n",
    "        # Get query that user entered\n",
    "        query = input_text.value  \n",
    "\n",
    "        # Convert the query to postfix and evaluate using the inverted index\n",
    "        postfix_query = infix_to_postfix(query)\n",
    "        results = evaluate_postfix(postfix_query, inverted_index, len(cleaned_paragraphs))\n",
    "        \n",
    "        # Apply TF-IDF transformation on the resulting paragraphs\n",
    "        tfidf_matrix, filtered_paragraphs, filtered_ids, vectorizer = tf_idf(results, cleaned_paragraphs)\n",
    "        # Process query for ranking\n",
    "        cleaned_query = clean_text(query)  \n",
    "\n",
    "        # Display search results based on the selected ranking algorithm\n",
    "        with output:\n",
    "            if filtered_paragraphs:\n",
    "                print(f\"Total matching paragraphs: {len(filtered_ids)}\\n\")\n",
    "\n",
    "                # Display results with Boolean retrieval\n",
    "                if algorithm == 'Boolean retrieval':\n",
    "                    displayed_links = set()\n",
    "                    for i, paragraph_id in enumerate(filtered_ids):\n",
    "                        original = original_paragraphs[paragraph_id]\n",
    "                        if original['link'] not in displayed_links:\n",
    "                            displayed_links.add(original['link'])\n",
    "                            print(f\"Link: {original['link']}\\n{original['text']}\")\n",
    "                    print(f\"Total links shown: {len(displayed_links)}\\n\")\n",
    "\n",
    "                # Display ranked results with VSM\n",
    "                elif algorithm == 'Vector Space Model':\n",
    "                    ranked_results = vector_space_model(cleaned_query, tfidf_matrix, original_paragraphs, filtered_ids, vectorizer)\n",
    "                    displayed_links = set()\n",
    "                    for result in ranked_results:\n",
    "                        displayed_links.add(result['link'])\n",
    "                        print(f\"Link: {result['link']}      (Score: {result['score']:.3f})\\n{result['text']}\")\n",
    "                    print(f\"Total links shown: {len(displayed_links)}\\n\")\n",
    "                \n",
    "                # Display ranked results with okapi BM25\n",
    "                elif algorithm == 'Okapi BM25':\n",
    "                    ranked_results = okapi_bm25(cleaned_query, filtered_paragraphs, filtered_ids, original_paragraphs)\n",
    "                    displayed_links = set()\n",
    "                    for result in ranked_results:\n",
    "                        displayed_links.add(result['link'])\n",
    "                        print(f\"Link: {result['link']}      (Score: {result['score']:.3f})\\n{result['text']}\")\n",
    "                    print(f\"Total links shown: {len(displayed_links)}\\n\")\n",
    "                \n",
    "            else:\n",
    "                # If no results match the query\n",
    "                print(f\"No results found for '{query}' using {algorithm}.\")\n",
    "\n",
    "    # Connect the search button to the search function\n",
    "    search_button.on_click(search)\n",
    "\n",
    "    # Display widgets\n",
    "    display(widgets.VBox([toggle_buttons, space, input_text, search_button, output]))\n",
    "\n",
    "search_engine(inverted_index, original_paragraphs, cleaned_paragraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650e50fe-e653-4033-b2f5-4e3b2a4d6ea9",
   "metadata": {},
   "source": [
    "# 5. Αξιολόγηση συστήματος"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79310f9-b3fe-459c-805c-a407ad382b55",
   "metadata": {},
   "source": [
    "Εισαγωγή και επεξεργασία δεδομένων από το CISI dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16e782-8a0b-40d3-9b79-7f3b4d075304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process CISI documents\n",
    "def read_documents():\n",
    "    # File path to the CISI documents\n",
    "    fp = '/Users/vivh/ergasia/cisi/CISI.ALL/CISI.ALL'\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "        # To merge content across lines\n",
    "        merged = ''  \n",
    "\n",
    "        # Read file line by line and merge content while preserving field tags\n",
    "        for a_line in f:\n",
    "            # Identify field tags (.I, .X)\n",
    "            if a_line.startswith('.'):  \n",
    "                merged += '\\n' + a_line.strip()\n",
    "            else:\n",
    "                # Add text to the merged content\n",
    "                merged += ' ' + a_line.strip()  \n",
    "\n",
    "    # Store processed documents\n",
    "    documents = []  \n",
    "    # Temporary storage for document text\n",
    "    content = ''  \n",
    "    # Temporary storage for document ID\n",
    "    doc_id = '' \n",
    "\n",
    "    # Process content line by line\n",
    "    for a_line in merged.split('\\n'):\n",
    "        # New document identifier\n",
    "        if a_line.startswith('.I'):\n",
    "            # Save previous document\n",
    "            if doc_id and content:  \n",
    "                documents.append({'id': doc_id, 'text': content.strip()})\n",
    "\n",
    "            # Extract document ID\n",
    "            doc_id = a_line.split(' ')[1].strip()  \n",
    "            # Reset for the new document\n",
    "            content = ''  \n",
    "        \n",
    "        # End of document identifier    \n",
    "        elif a_line.startswith('.X'): \n",
    "            if doc_id and content:\n",
    "                documents.append({'id': doc_id, 'text': content.strip()})\n",
    "                \n",
    "            doc_id = ''\n",
    "            content = ''\n",
    "            \n",
    "        else:\n",
    "            # Add the content excluding the tags\n",
    "            content += a_line[3:].strip() + ' '\n",
    "\n",
    "    # Last document in the file\n",
    "    if doc_id and content:\n",
    "        documents.append({'id': doc_id, 'text': content.strip()})\n",
    "    \n",
    "    # Save processed documents to a JSON file\n",
    "    store_things(documents, 'cisi_documents.json')\n",
    "    return documents\n",
    "\n",
    "\n",
    "# Read and process CISI queries\n",
    "def read_queries():\n",
    "    fp = '/Users/vivh/ergasia/cisi/CISI.QRY'\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "        merged = '' \n",
    "\n",
    "        for a_line in f:\n",
    "            if a_line.startswith('.'):\n",
    "                merged += '\\n' + a_line.strip()\n",
    "            else:\n",
    "                merged += ' ' + a_line.strip() \n",
    "\n",
    "    queries = [] \n",
    "    content = ''  \n",
    "    qry_id = '' \n",
    "\n",
    "    for a_line in merged.split('\\n'):\n",
    "        if a_line.startswith('.I'):\n",
    "            if qry_id and content: \n",
    "                queries.append({'id': qry_id, 'text': content.strip()})\n",
    "                \n",
    "            qry_id = a_line.split(' ')[1].strip() \n",
    "            content = '' \n",
    "        \n",
    "        elif a_line.startswith('.W') or a_line.startswith('.T'): \n",
    "            content += a_line.strip()[3:] + ' '\n",
    "\n",
    "    if qry_id and content:\n",
    "        queries.append({'id': qry_id, 'text': content.strip()})\n",
    "\n",
    "    store_things(queries, 'cisi_queries.json')\n",
    "    return queries\n",
    "\n",
    "\n",
    "# Function to read and process CISI relevance mappings\n",
    "def read_mappings():\n",
    "    fp = '/Users/vivh/ergasia/cisi/CISI.REL'\n",
    "\n",
    "    with open(fp, 'r') as f:\n",
    "        # Store processed mappings\n",
    "        mappings = []  \n",
    "\n",
    "        # Read file line by line\n",
    "        for a_line in f:\n",
    "            # Split the line into components\n",
    "            voc = a_line.strip().split()  \n",
    "            # Extract query ID\n",
    "            qry_id = voc[0].strip() \n",
    "            # Extract document ID\n",
    "            doc_id = voc[1].strip()  \n",
    "\n",
    "            # Append the mapping as a dictionary\n",
    "            mappings.append({'query_id': qry_id, 'doc_id': doc_id})\n",
    "\n",
    "    store_things(mappings, 'cisi_mappings.json')\n",
    "    return mappings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20336f84-42ec-48d3-bf57-bdde77e85219",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = read_documents()\n",
    "\n",
    "display(Markdown(\"How some of the CISI documents are saved: <br>\"))\n",
    "for paragraph in documents[:10]:\n",
    "    d_id = paragraph['id']\n",
    "    text = paragraph['text']\n",
    "\n",
    "    print(f\"id: {d_id}\")\n",
    "    print(f\"text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb307ac-4cb7-4715-b30b-e0743648a6f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "queries = read_queries()  \n",
    "\n",
    "display(Markdown(\"How some of the CISI queries are saved: <br>\"))\n",
    "for paragraph in queries[:10]:\n",
    "    d_id = paragraph['id']\n",
    "    text = paragraph['text']\n",
    "\n",
    "    print(f\"id: {d_id}\")\n",
    "    print(f\"text: {text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d643a-1826-429d-a6ba-34205d5e4b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = read_mappings()\n",
    "\n",
    "display(Markdown(\"How CISI mappings are saved: <br>\"))\n",
    "# Convert mappings to data frame\n",
    "mappings_df = pd.DataFrame(mappings)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "display(mappings_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7536a64-4317-42ec-8cdb-fd7d7c5c0d66",
   "metadata": {},
   "source": [
    "Υπολογισμός ακρίβειας, ανάκλησης, F1-score και μέσης ακρίβειας με αγλόριθμο αναζήτησης Okapi BM25 για τα έγγραφα και ερωτήματα του CISI dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d95ccb4-284e-4c39-8282-cc0fdce5eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(documents, queries, mappings):\n",
    "    # Stores cleaned documents\n",
    "    cleaned_paragraphs = {}\n",
    "\n",
    "    for doc in documents:\n",
    "        doc_id = doc['id']\n",
    "        # Clean and tokenize the document text, then store it\n",
    "        cleaned_text = clean_text(doc['text'])  \n",
    "        cleaned_paragraphs[doc_id] = {'tokens': cleaned_text}\n",
    "\n",
    "    # Build the inverted index\n",
    "    inverted_index = defaultdict(set)\n",
    "    for doc_id, doc in cleaned_paragraphs.items():\n",
    "        for term in doc['tokens']:  # Use 'tokens' instead of 'text'\n",
    "            inverted_index[term].add(doc_id)\n",
    "\n",
    "    # Store average precision for each query\n",
    "    average_precisions = []\n",
    "\n",
    "    for query in queries:\n",
    "        query_id = query['id']\n",
    "        query_text = query['text']\n",
    "\n",
    "        # Preprocess the query text\n",
    "        cleaned_query = ' '.join(clean_text(query_text))\n",
    "      \n",
    "        # Convert the query to postfix and evaluate using the inverted index\n",
    "        postfix_query = infix_to_postfix(cleaned_query)\n",
    "        results = evaluate_postfix(postfix_query, inverted_index, len(cleaned_paragraphs))\n",
    "\n",
    "        # Apply TF-IDF transformation on the resulting paragraphs\n",
    "        tfidf_matrix, filtered_paragraphs, filtered_ids, vectorizer = tf_idf(results, cleaned_paragraphs)\n",
    "\n",
    "        if not filtered_paragraphs:  \n",
    "            print(f\"\\nQuery ID: {query_id}\")\n",
    "            print(f\"Query: {query_text}\")\n",
    "            print(\"No matches found.\\n\")\n",
    "            average_precisions.append(0)\n",
    "            continue\n",
    "\n",
    "        # Use TF-IDF results as input to BM25\n",
    "        tokenized_docs = []\n",
    "        for doc_id in filtered_ids:\n",
    "            tokenized_docs.append(cleaned_paragraphs[doc_id]['tokens'])\n",
    "    \n",
    "        bm25 = BM25Okapi(tokenized_docs)    \n",
    "        bm25_scores = bm25.get_scores(postfix_query)\n",
    "\n",
    "        # Rank documents based on BM25 scores\n",
    "        ranked_results = sorted(\n",
    "            zip(filtered_ids, bm25_scores),\n",
    "            key = lambda x: x[1],\n",
    "            reverse = True\n",
    "        )\n",
    "\n",
    "        # Extract retrieved document IDs with scores > 0\n",
    "        retrieved_docs = []\n",
    "        for doc_id, score in ranked_results:\n",
    "            if score > 0:\n",
    "                retrieved_docs.append(doc_id)\n",
    "\n",
    "       # Find relevant documents for this query\n",
    "        relevant_docs = set()\n",
    "        for mapping in mappings:\n",
    "            if mapping['query_id'] == query_id:\n",
    "                relevant_docs.add(mapping['doc_id'])\n",
    "        \n",
    "        # Calculate precision at each relevant document's position\n",
    "        true_positives = 0\n",
    "        precisions = []\n",
    "\n",
    "        for i, doc_id in enumerate(retrieved_docs, start = 1):\n",
    "            if doc_id in relevant_docs:\n",
    "                true_positives += 1\n",
    "                precision_at_k = true_positives / i\n",
    "                precisions.append(precision_at_k)\n",
    "\n",
    "        if retrieved_docs:\n",
    "            precision = true_positives / len(retrieved_docs)\n",
    "        else:\n",
    "            precision = 0\n",
    "        \n",
    "        if relevant_docs:\n",
    "            ap = sum(precisions) / len(relevant_docs)\n",
    "            recall = true_positives / len(relevant_docs)\n",
    "        else:\n",
    "            ap = 0\n",
    "            \n",
    "        average_precisions.append(ap)\n",
    "\n",
    "        if precision + recall > 0:\n",
    "            f1_score = (2 * precision * recall) / (precision + recall)\n",
    "        else:\n",
    "            f1_score = 0;\n",
    "\n",
    "        # Print metrics for the current query\n",
    "        display(Markdown(\"Query\"))\n",
    "        print(f\"{query_text}\")\n",
    "        display(Markdown(\"Matching document IDs\"))\n",
    "        print(f\"{retrieved_docs}\")\n",
    "        display(Markdown(f\"Precision: {precision:.3f}\"))\n",
    "        display(Markdown(f\"Recall: {recall:.3f}\"))\n",
    "        display(Markdown(f\"F1-Score: {f1_score:.3f}<br><br>\"))\n",
    "\n",
    "    # Calculate Mean Average Precision \n",
    "    map_score = sum(average_precisions) / len(average_precisions) if average_precisions else 0\n",
    "    display(Markdown(f\"Mean Average Precision: {map_score:.3f}\"))\n",
    "    \n",
    "\n",
    "display(Markdown(\"Using the CISI dataset to evaluate search engine<br>\"))\n",
    "testing(documents, queries, mappings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa391df-e608-45d7-a7b1-9ddc441eb820",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
